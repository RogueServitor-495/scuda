#include <nvml.h>
#include <cuda.h>
#include <cudnn.h>
#include <cublas_v2.h>
#include <cublasLt.h>
#include <cuda_runtime_api.h>

#include <cstring>
#include <string>
#include <unordered_map>

#include "gen_api.h"

#include "manual_client.h"

#include "rpc.h"

extern int rpc_size();
extern conn_t *rpc_client_get_connection(unsigned int index);
int is_unified_pointer(conn_t *conn, void *arg);
int maybe_copy_unified_arg(conn_t *conn, void *arg, enum cudaMemcpyKind kind);
extern void rpc_close(conn_t *conn);

nvmlReturn_t nvmlInit_v2()
{
    conn_t *conn = rpc_client_get_connection(0);
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlInit_v2) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlInitWithFlags(unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlInitWithFlags) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlShutdown()
{
    conn_t *conn = rpc_client_get_connection(0);
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlShutdown) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    rpc_close(conn);
    return return_value;
}

nvmlReturn_t nvmlSystemGetDriverVersion(char* version, unsigned int length)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&length, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)version, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(length) && is_unified_pointer(conn, (void*)version); i++)
      if (maybe_copy_unified_arg(conn, (void*)&version[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlSystemGetDriverVersion) < 0 ||
        rpc_write(conn, &length, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, version, length * sizeof(char)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&length, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)version, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(length) && is_unified_pointer(conn, (void*)version); i++)
      if (maybe_copy_unified_arg(conn, (void*)&version[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlSystemGetNVMLVersion(char* version, unsigned int length)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&length, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)version, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(length) && is_unified_pointer(conn, (void*)version); i++)
      if (maybe_copy_unified_arg(conn, (void*)&version[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlSystemGetNVMLVersion) < 0 ||
        rpc_write(conn, &length, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, version, length * sizeof(char)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&length, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)version, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(length) && is_unified_pointer(conn, (void*)version); i++)
      if (maybe_copy_unified_arg(conn, (void*)&version[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlSystemGetCudaDriverVersion(int* cudaDriverVersion)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)cudaDriverVersion, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlSystemGetCudaDriverVersion) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, cudaDriverVersion, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)cudaDriverVersion, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlSystemGetCudaDriverVersion_v2(int* cudaDriverVersion)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)cudaDriverVersion, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlSystemGetCudaDriverVersion_v2) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, cudaDriverVersion, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)cudaDriverVersion, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlSystemGetProcessName(unsigned int pid, char* name, unsigned int length)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&pid, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&length, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)name, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(length) && is_unified_pointer(conn, (void*)name); i++)
      if (maybe_copy_unified_arg(conn, (void*)&name[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlSystemGetProcessName) < 0 ||
        rpc_write(conn, &pid, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &length, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, name, length * sizeof(char)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&pid, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&length, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)name, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(length) && is_unified_pointer(conn, (void*)name); i++)
      if (maybe_copy_unified_arg(conn, (void*)&name[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlSystemGetHicVersion(unsigned int* hwbcCount, nvmlHwbcEntry_t* hwbcEntries)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)hwbcCount, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)hwbcEntries, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*hwbcCount) && is_unified_pointer(conn, (void*)hwbcEntries); i++)
      if (maybe_copy_unified_arg(conn, (void*)&hwbcEntries[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlSystemGetHicVersion) < 0 ||
        rpc_write(conn, hwbcCount, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, hwbcCount, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, hwbcEntries, *hwbcCount * sizeof(nvmlHwbcEntry_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)hwbcCount, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)hwbcEntries, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*hwbcCount) && is_unified_pointer(conn, (void*)hwbcEntries); i++)
      if (maybe_copy_unified_arg(conn, (void*)&hwbcEntries[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlSystemGetTopologyGpuSet(unsigned int cpuNumber, unsigned int* count, nvmlDevice_t* deviceArray)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&cpuNumber, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)count, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)deviceArray, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*count) && is_unified_pointer(conn, (void*)deviceArray); i++)
      if (maybe_copy_unified_arg(conn, (void*)&deviceArray[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlSystemGetTopologyGpuSet) < 0 ||
        rpc_write(conn, &cpuNumber, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, count, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, count, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, deviceArray, *count * sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&cpuNumber, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)count, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)deviceArray, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*count) && is_unified_pointer(conn, (void*)deviceArray); i++)
      if (maybe_copy_unified_arg(conn, (void*)&deviceArray[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlUnitGetCount(unsigned int* unitCount)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)unitCount, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlUnitGetCount) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, unitCount, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)unitCount, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlUnitGetHandleByIndex(unsigned int index, nvmlUnit_t* unit)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&index, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)unit, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlUnitGetHandleByIndex) < 0 ||
        rpc_write(conn, &index, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, unit, sizeof(nvmlUnit_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&index, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)unit, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlUnitGetUnitInfo(nvmlUnit_t unit, nvmlUnitInfo_t* info)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&unit, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)info, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlUnitGetUnitInfo) < 0 ||
        rpc_write(conn, &unit, sizeof(nvmlUnit_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, info, sizeof(nvmlUnitInfo_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&unit, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)info, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlUnitGetLedState(nvmlUnit_t unit, nvmlLedState_t* state)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&unit, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)state, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlUnitGetLedState) < 0 ||
        rpc_write(conn, &unit, sizeof(nvmlUnit_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, state, sizeof(nvmlLedState_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&unit, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)state, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlUnitGetPsuInfo(nvmlUnit_t unit, nvmlPSUInfo_t* psu)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&unit, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)psu, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlUnitGetPsuInfo) < 0 ||
        rpc_write(conn, &unit, sizeof(nvmlUnit_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, psu, sizeof(nvmlPSUInfo_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&unit, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)psu, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlUnitGetTemperature(nvmlUnit_t unit, unsigned int type, unsigned int* temp)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&unit, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&type, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)temp, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlUnitGetTemperature) < 0 ||
        rpc_write(conn, &unit, sizeof(nvmlUnit_t)) < 0 ||
        rpc_write(conn, &type, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, temp, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&unit, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&type, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)temp, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlUnitGetFanSpeedInfo(nvmlUnit_t unit, nvmlUnitFanSpeeds_t* fanSpeeds)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&unit, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)fanSpeeds, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlUnitGetFanSpeedInfo) < 0 ||
        rpc_write(conn, &unit, sizeof(nvmlUnit_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, fanSpeeds, sizeof(nvmlUnitFanSpeeds_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&unit, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)fanSpeeds, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlUnitGetDevices(nvmlUnit_t unit, unsigned int* deviceCount, nvmlDevice_t* devices)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&unit, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)deviceCount, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)devices, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*deviceCount) && is_unified_pointer(conn, (void*)devices); i++)
      if (maybe_copy_unified_arg(conn, (void*)&devices[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlUnitGetDevices) < 0 ||
        rpc_write(conn, &unit, sizeof(nvmlUnit_t)) < 0 ||
        rpc_write(conn, deviceCount, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, deviceCount, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, devices, *deviceCount * sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&unit, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)deviceCount, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)devices, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*deviceCount) && is_unified_pointer(conn, (void*)devices); i++)
      if (maybe_copy_unified_arg(conn, (void*)&devices[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetCount_v2(unsigned int* deviceCount)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)deviceCount, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetCount_v2) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, deviceCount, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)deviceCount, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetAttributes_v2(nvmlDevice_t device, nvmlDeviceAttributes_t* attributes)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)attributes, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetAttributes_v2) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, attributes, sizeof(nvmlDeviceAttributes_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)attributes, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetHandleByIndex_v2(unsigned int index, nvmlDevice_t* device)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&index, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetHandleByIndex_v2) < 0 ||
        rpc_write(conn, &index, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&index, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetHandleBySerial(const char* serial, nvmlDevice_t* device)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)serial, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    std::size_t serial_len = std::strlen(serial) + 1;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetHandleBySerial) < 0 ||
        rpc_write(conn, &serial_len, sizeof(std::size_t)) < 0 ||
        rpc_write(conn, serial, serial_len) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)serial, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetHandleByUUID(const char* uuid, nvmlDevice_t* device)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)uuid, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    std::size_t uuid_len = std::strlen(uuid) + 1;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetHandleByUUID) < 0 ||
        rpc_write(conn, &uuid_len, sizeof(std::size_t)) < 0 ||
        rpc_write(conn, uuid, uuid_len) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)uuid, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetHandleByPciBusId_v2(const char* pciBusId, nvmlDevice_t* device)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pciBusId, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    std::size_t pciBusId_len = std::strlen(pciBusId) + 1;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetHandleByPciBusId_v2) < 0 ||
        rpc_write(conn, &pciBusId_len, sizeof(std::size_t)) < 0 ||
        rpc_write(conn, pciBusId, pciBusId_len) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pciBusId, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetName(nvmlDevice_t device, char* name, unsigned int length)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&length, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)name, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(length) && is_unified_pointer(conn, (void*)name); i++)
      if (maybe_copy_unified_arg(conn, (void*)&name[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetName) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &length, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, name, length * sizeof(char)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&length, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)name, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(length) && is_unified_pointer(conn, (void*)name); i++)
      if (maybe_copy_unified_arg(conn, (void*)&name[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetBrand(nvmlDevice_t device, nvmlBrandType_t* type)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)type, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetBrand) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, type, sizeof(nvmlBrandType_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)type, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetIndex(nvmlDevice_t device, unsigned int* index)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)index, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetIndex) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, index, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)index, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetSerial(nvmlDevice_t device, char* serial, unsigned int length)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&length, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)serial, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(length) && is_unified_pointer(conn, (void*)serial); i++)
      if (maybe_copy_unified_arg(conn, (void*)&serial[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetSerial) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &length, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, serial, length * sizeof(char)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&length, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)serial, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(length) && is_unified_pointer(conn, (void*)serial); i++)
      if (maybe_copy_unified_arg(conn, (void*)&serial[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetModuleId(nvmlDevice_t device, unsigned int* moduleId)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)moduleId, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetModuleId) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, moduleId, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, moduleId, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)moduleId, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetC2cModeInfoV(nvmlDevice_t device, nvmlC2cModeInfo_v1_t* c2cModeInfo)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)c2cModeInfo, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetC2cModeInfoV) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, c2cModeInfo, sizeof(nvmlC2cModeInfo_v1_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, c2cModeInfo, sizeof(nvmlC2cModeInfo_v1_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)c2cModeInfo, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetMemoryAffinity(nvmlDevice_t device, unsigned int nodeSetSize, unsigned long* nodeSet, nvmlAffinityScope_t scope)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&nodeSetSize, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)nodeSet, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(nodeSetSize) && is_unified_pointer(conn, (void*)nodeSet); i++)
      if (maybe_copy_unified_arg(conn, (void*)&nodeSet[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&scope, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetMemoryAffinity) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &nodeSetSize, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &scope, sizeof(nvmlAffinityScope_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, nodeSet, nodeSetSize * sizeof(unsigned long)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&nodeSetSize, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)nodeSet, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(nodeSetSize) && is_unified_pointer(conn, (void*)nodeSet); i++)
      if (maybe_copy_unified_arg(conn, (void*)&nodeSet[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&scope, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetCpuAffinityWithinScope(nvmlDevice_t device, unsigned int cpuSetSize, unsigned long* cpuSet, nvmlAffinityScope_t scope)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&cpuSetSize, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)cpuSet, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(cpuSetSize) && is_unified_pointer(conn, (void*)cpuSet); i++)
      if (maybe_copy_unified_arg(conn, (void*)&cpuSet[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&scope, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetCpuAffinityWithinScope) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &cpuSetSize, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &scope, sizeof(nvmlAffinityScope_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, cpuSet, cpuSetSize * sizeof(unsigned long)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&cpuSetSize, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)cpuSet, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(cpuSetSize) && is_unified_pointer(conn, (void*)cpuSet); i++)
      if (maybe_copy_unified_arg(conn, (void*)&cpuSet[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&scope, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetCpuAffinity(nvmlDevice_t device, unsigned int cpuSetSize, unsigned long* cpuSet)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&cpuSetSize, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)cpuSet, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(cpuSetSize) && is_unified_pointer(conn, (void*)cpuSet); i++)
      if (maybe_copy_unified_arg(conn, (void*)&cpuSet[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetCpuAffinity) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &cpuSetSize, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, cpuSet, cpuSetSize * sizeof(unsigned long)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&cpuSetSize, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)cpuSet, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(cpuSetSize) && is_unified_pointer(conn, (void*)cpuSet); i++)
      if (maybe_copy_unified_arg(conn, (void*)&cpuSet[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceSetCpuAffinity(nvmlDevice_t device)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceSetCpuAffinity) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceClearCpuAffinity(nvmlDevice_t device)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceClearCpuAffinity) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetNumaNodeId(nvmlDevice_t device, unsigned int* node)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)node, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetNumaNodeId) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, node, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, node, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)node, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetTopologyCommonAncestor(nvmlDevice_t device1, nvmlDevice_t device2, nvmlGpuTopologyLevel_t* pathInfo)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device1, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device2, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pathInfo, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetTopologyCommonAncestor) < 0 ||
        rpc_write(conn, &device1, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &device2, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pathInfo, sizeof(nvmlGpuTopologyLevel_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device1, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device2, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pathInfo, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetTopologyNearestGpus(nvmlDevice_t device, nvmlGpuTopologyLevel_t level, unsigned int* count, nvmlDevice_t* deviceArray)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&level, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)count, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)deviceArray, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*count) && is_unified_pointer(conn, (void*)deviceArray); i++)
      if (maybe_copy_unified_arg(conn, (void*)&deviceArray[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetTopologyNearestGpus) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &level, sizeof(nvmlGpuTopologyLevel_t)) < 0 ||
        rpc_write(conn, count, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, count, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, deviceArray, *count * sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&level, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)count, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)deviceArray, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*count) && is_unified_pointer(conn, (void*)deviceArray); i++)
      if (maybe_copy_unified_arg(conn, (void*)&deviceArray[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetP2PStatus(nvmlDevice_t device1, nvmlDevice_t device2, nvmlGpuP2PCapsIndex_t p2pIndex, nvmlGpuP2PStatus_t* p2pStatus)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device1, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device2, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&p2pIndex, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)p2pStatus, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetP2PStatus) < 0 ||
        rpc_write(conn, &device1, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &device2, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &p2pIndex, sizeof(nvmlGpuP2PCapsIndex_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, p2pStatus, sizeof(nvmlGpuP2PStatus_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device1, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device2, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&p2pIndex, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)p2pStatus, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetUUID(nvmlDevice_t device, char* uuid, unsigned int length)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&length, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)uuid, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(length) && is_unified_pointer(conn, (void*)uuid); i++)
      if (maybe_copy_unified_arg(conn, (void*)&uuid[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetUUID) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &length, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, uuid, length * sizeof(char)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&length, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)uuid, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(length) && is_unified_pointer(conn, (void*)uuid); i++)
      if (maybe_copy_unified_arg(conn, (void*)&uuid[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetMinorNumber(nvmlDevice_t device, unsigned int* minorNumber)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)minorNumber, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetMinorNumber) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, minorNumber, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)minorNumber, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetBoardPartNumber(nvmlDevice_t device, char* partNumber, unsigned int length)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&length, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)partNumber, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(length) && is_unified_pointer(conn, (void*)partNumber); i++)
      if (maybe_copy_unified_arg(conn, (void*)&partNumber[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetBoardPartNumber) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &length, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, partNumber, length * sizeof(char)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&length, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)partNumber, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(length) && is_unified_pointer(conn, (void*)partNumber); i++)
      if (maybe_copy_unified_arg(conn, (void*)&partNumber[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetInforomVersion(nvmlDevice_t device, nvmlInforomObject_t object, char* version, unsigned int length)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&object, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&length, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)version, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(length) && is_unified_pointer(conn, (void*)version); i++)
      if (maybe_copy_unified_arg(conn, (void*)&version[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetInforomVersion) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &object, sizeof(nvmlInforomObject_t)) < 0 ||
        rpc_write(conn, &length, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, version, length * sizeof(char)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&object, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&length, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)version, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(length) && is_unified_pointer(conn, (void*)version); i++)
      if (maybe_copy_unified_arg(conn, (void*)&version[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetInforomImageVersion(nvmlDevice_t device, char* version, unsigned int length)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&length, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)version, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(length) && is_unified_pointer(conn, (void*)version); i++)
      if (maybe_copy_unified_arg(conn, (void*)&version[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetInforomImageVersion) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &length, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, version, length * sizeof(char)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&length, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)version, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(length) && is_unified_pointer(conn, (void*)version); i++)
      if (maybe_copy_unified_arg(conn, (void*)&version[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetInforomConfigurationChecksum(nvmlDevice_t device, unsigned int* checksum)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)checksum, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetInforomConfigurationChecksum) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, checksum, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)checksum, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceValidateInforom(nvmlDevice_t device)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceValidateInforom) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetLastBBXFlushTime(nvmlDevice_t device, unsigned long long* timestamp, unsigned long* durationUs)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)timestamp, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)durationUs, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetLastBBXFlushTime) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, timestamp, sizeof(unsigned long long)) < 0 ||
        rpc_write(conn, durationUs, sizeof(unsigned long)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, timestamp, sizeof(unsigned long long)) < 0 ||
        rpc_read(conn, durationUs, sizeof(unsigned long)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)timestamp, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)durationUs, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetDisplayMode(nvmlDevice_t device, nvmlEnableState_t* display)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)display, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetDisplayMode) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, display, sizeof(nvmlEnableState_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)display, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetDisplayActive(nvmlDevice_t device, nvmlEnableState_t* isActive)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)isActive, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetDisplayActive) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, isActive, sizeof(nvmlEnableState_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)isActive, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetPersistenceMode(nvmlDevice_t device, nvmlEnableState_t* mode)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)mode, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetPersistenceMode) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, mode, sizeof(nvmlEnableState_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)mode, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetPciInfoExt(nvmlDevice_t device, nvmlPciInfoExt_t* pci)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pci, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetPciInfoExt) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, pci, sizeof(nvmlPciInfoExt_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pci, sizeof(nvmlPciInfoExt_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pci, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetPciInfo_v3(nvmlDevice_t device, nvmlPciInfo_t* pci)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pci, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetPciInfo_v3) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pci, sizeof(nvmlPciInfo_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pci, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetMaxPcieLinkGeneration(nvmlDevice_t device, unsigned int* maxLinkGen)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)maxLinkGen, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetMaxPcieLinkGeneration) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, maxLinkGen, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)maxLinkGen, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetGpuMaxPcieLinkGeneration(nvmlDevice_t device, unsigned int* maxLinkGenDevice)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)maxLinkGenDevice, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetGpuMaxPcieLinkGeneration) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, maxLinkGenDevice, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)maxLinkGenDevice, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetMaxPcieLinkWidth(nvmlDevice_t device, unsigned int* maxLinkWidth)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)maxLinkWidth, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetMaxPcieLinkWidth) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, maxLinkWidth, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)maxLinkWidth, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetCurrPcieLinkGeneration(nvmlDevice_t device, unsigned int* currLinkGen)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)currLinkGen, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetCurrPcieLinkGeneration) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, currLinkGen, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)currLinkGen, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetCurrPcieLinkWidth(nvmlDevice_t device, unsigned int* currLinkWidth)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)currLinkWidth, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetCurrPcieLinkWidth) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, currLinkWidth, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)currLinkWidth, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetPcieThroughput(nvmlDevice_t device, nvmlPcieUtilCounter_t counter, unsigned int* value)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&counter, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)value, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetPcieThroughput) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &counter, sizeof(nvmlPcieUtilCounter_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, value, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&counter, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)value, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetPcieReplayCounter(nvmlDevice_t device, unsigned int* value)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)value, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetPcieReplayCounter) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, value, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)value, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetClockInfo(nvmlDevice_t device, nvmlClockType_t type, unsigned int* clock)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&type, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)clock, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetClockInfo) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &type, sizeof(nvmlClockType_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, clock, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&type, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)clock, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetMaxClockInfo(nvmlDevice_t device, nvmlClockType_t type, unsigned int* clock)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&type, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)clock, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetMaxClockInfo) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &type, sizeof(nvmlClockType_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, clock, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&type, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)clock, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetGpcClkVfOffset(nvmlDevice_t device, int* offset)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)offset, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetGpcClkVfOffset) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, offset, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)offset, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetApplicationsClock(nvmlDevice_t device, nvmlClockType_t clockType, unsigned int* clockMHz)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&clockType, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)clockMHz, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetApplicationsClock) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &clockType, sizeof(nvmlClockType_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, clockMHz, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&clockType, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)clockMHz, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetDefaultApplicationsClock(nvmlDevice_t device, nvmlClockType_t clockType, unsigned int* clockMHz)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&clockType, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)clockMHz, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetDefaultApplicationsClock) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &clockType, sizeof(nvmlClockType_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, clockMHz, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&clockType, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)clockMHz, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetClock(nvmlDevice_t device, nvmlClockType_t clockType, nvmlClockId_t clockId, unsigned int* clockMHz)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&clockType, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&clockId, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)clockMHz, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetClock) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &clockType, sizeof(nvmlClockType_t)) < 0 ||
        rpc_write(conn, &clockId, sizeof(nvmlClockId_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, clockMHz, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&clockType, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&clockId, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)clockMHz, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetMaxCustomerBoostClock(nvmlDevice_t device, nvmlClockType_t clockType, unsigned int* clockMHz)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&clockType, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)clockMHz, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetMaxCustomerBoostClock) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &clockType, sizeof(nvmlClockType_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, clockMHz, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&clockType, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)clockMHz, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetSupportedMemoryClocks(nvmlDevice_t device, unsigned int* count, unsigned int* clocksMHz)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)count, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)clocksMHz, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*count) && is_unified_pointer(conn, (void*)clocksMHz); i++)
      if (maybe_copy_unified_arg(conn, (void*)&clocksMHz[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetSupportedMemoryClocks) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, count, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, count, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, clocksMHz, *count * sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)count, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)clocksMHz, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*count) && is_unified_pointer(conn, (void*)clocksMHz); i++)
      if (maybe_copy_unified_arg(conn, (void*)&clocksMHz[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetSupportedGraphicsClocks(nvmlDevice_t device, unsigned int memoryClockMHz, unsigned int* count, unsigned int* clocksMHz)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&memoryClockMHz, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)count, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)clocksMHz, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*count) && is_unified_pointer(conn, (void*)clocksMHz); i++)
      if (maybe_copy_unified_arg(conn, (void*)&clocksMHz[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetSupportedGraphicsClocks) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &memoryClockMHz, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, count, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, count, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, clocksMHz, *count * sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&memoryClockMHz, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)count, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)clocksMHz, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*count) && is_unified_pointer(conn, (void*)clocksMHz); i++)
      if (maybe_copy_unified_arg(conn, (void*)&clocksMHz[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetAutoBoostedClocksEnabled(nvmlDevice_t device, nvmlEnableState_t* isEnabled, nvmlEnableState_t* defaultIsEnabled)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)isEnabled, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)defaultIsEnabled, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetAutoBoostedClocksEnabled) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, isEnabled, sizeof(nvmlEnableState_t)) < 0 ||
        rpc_read(conn, defaultIsEnabled, sizeof(nvmlEnableState_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)isEnabled, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)defaultIsEnabled, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetFanSpeed(nvmlDevice_t device, unsigned int* speed)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)speed, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetFanSpeed) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, speed, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)speed, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetFanSpeed_v2(nvmlDevice_t device, unsigned int fan, unsigned int* speed)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&fan, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)speed, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetFanSpeed_v2) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &fan, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, speed, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&fan, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)speed, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetTargetFanSpeed(nvmlDevice_t device, unsigned int fan, unsigned int* targetSpeed)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&fan, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)targetSpeed, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetTargetFanSpeed) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &fan, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, targetSpeed, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&fan, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)targetSpeed, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetMinMaxFanSpeed(nvmlDevice_t device, unsigned int* minSpeed, unsigned int* maxSpeed)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)minSpeed, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)maxSpeed, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetMinMaxFanSpeed) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, minSpeed, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, maxSpeed, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)minSpeed, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)maxSpeed, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetFanControlPolicy_v2(nvmlDevice_t device, unsigned int fan, nvmlFanControlPolicy_t* policy)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&fan, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)policy, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetFanControlPolicy_v2) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &fan, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, policy, sizeof(nvmlFanControlPolicy_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&fan, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)policy, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetNumFans(nvmlDevice_t device, unsigned int* numFans)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)numFans, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetNumFans) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, numFans, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)numFans, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetTemperature(nvmlDevice_t device, nvmlTemperatureSensors_t sensorType, unsigned int* temp)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&sensorType, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)temp, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetTemperature) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &sensorType, sizeof(nvmlTemperatureSensors_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, temp, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&sensorType, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)temp, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetTemperatureThreshold(nvmlDevice_t device, nvmlTemperatureThresholds_t thresholdType, unsigned int* temp)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&thresholdType, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)temp, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetTemperatureThreshold) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &thresholdType, sizeof(nvmlTemperatureThresholds_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, temp, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&thresholdType, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)temp, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetThermalSettings(nvmlDevice_t device, unsigned int sensorIndex, nvmlGpuThermalSettings_t* pThermalSettings)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&sensorIndex, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pThermalSettings, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetThermalSettings) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &sensorIndex, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pThermalSettings, sizeof(nvmlGpuThermalSettings_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&sensorIndex, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pThermalSettings, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetPerformanceState(nvmlDevice_t device, nvmlPstates_t* pState)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pState, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetPerformanceState) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pState, sizeof(nvmlPstates_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pState, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetCurrentClocksEventReasons(nvmlDevice_t device, unsigned long long* clocksEventReasons)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)clocksEventReasons, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetCurrentClocksEventReasons) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, clocksEventReasons, sizeof(unsigned long long)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, clocksEventReasons, sizeof(unsigned long long)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)clocksEventReasons, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetCurrentClocksThrottleReasons(nvmlDevice_t device, unsigned long long* clocksThrottleReasons)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)clocksThrottleReasons, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetCurrentClocksThrottleReasons) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, clocksThrottleReasons, sizeof(unsigned long long)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)clocksThrottleReasons, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetSupportedClocksEventReasons(nvmlDevice_t device, unsigned long long* supportedClocksEventReasons)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)supportedClocksEventReasons, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetSupportedClocksEventReasons) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, supportedClocksEventReasons, sizeof(unsigned long long)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, supportedClocksEventReasons, sizeof(unsigned long long)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)supportedClocksEventReasons, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetSupportedClocksThrottleReasons(nvmlDevice_t device, unsigned long long* supportedClocksThrottleReasons)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)supportedClocksThrottleReasons, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetSupportedClocksThrottleReasons) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, supportedClocksThrottleReasons, sizeof(unsigned long long)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)supportedClocksThrottleReasons, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetPowerState(nvmlDevice_t device, nvmlPstates_t* pState)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pState, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetPowerState) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pState, sizeof(nvmlPstates_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pState, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetDynamicPstatesInfo(nvmlDevice_t device, nvmlGpuDynamicPstatesInfo_t* pDynamicPstatesInfo)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pDynamicPstatesInfo, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetDynamicPstatesInfo) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pDynamicPstatesInfo, sizeof(nvmlGpuDynamicPstatesInfo_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pDynamicPstatesInfo, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetMemClkVfOffset(nvmlDevice_t device, int* offset)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)offset, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetMemClkVfOffset) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, offset, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)offset, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetMinMaxClockOfPState(nvmlDevice_t device, nvmlClockType_t type, nvmlPstates_t pstate, unsigned int* minClockMHz, unsigned int* maxClockMHz)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&type, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&pstate, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)minClockMHz, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)maxClockMHz, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetMinMaxClockOfPState) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &type, sizeof(nvmlClockType_t)) < 0 ||
        rpc_write(conn, &pstate, sizeof(nvmlPstates_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, minClockMHz, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, maxClockMHz, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&type, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&pstate, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)minClockMHz, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)maxClockMHz, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetSupportedPerformanceStates(nvmlDevice_t device, nvmlPstates_t* pstates, unsigned int size)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&size, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pstates, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(size) && is_unified_pointer(conn, (void*)pstates); i++)
      if (maybe_copy_unified_arg(conn, (void*)&pstates[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetSupportedPerformanceStates) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &size, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pstates, size * sizeof(nvmlPstates_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&size, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pstates, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(size) && is_unified_pointer(conn, (void*)pstates); i++)
      if (maybe_copy_unified_arg(conn, (void*)&pstates[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetGpcClkMinMaxVfOffset(nvmlDevice_t device, int* minOffset, int* maxOffset)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)minOffset, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)maxOffset, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetGpcClkMinMaxVfOffset) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, minOffset, sizeof(int)) < 0 ||
        rpc_read(conn, maxOffset, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)minOffset, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)maxOffset, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetMemClkMinMaxVfOffset(nvmlDevice_t device, int* minOffset, int* maxOffset)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)minOffset, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)maxOffset, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetMemClkMinMaxVfOffset) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, minOffset, sizeof(int)) < 0 ||
        rpc_read(conn, maxOffset, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)minOffset, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)maxOffset, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetPowerManagementMode(nvmlDevice_t device, nvmlEnableState_t* mode)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)mode, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetPowerManagementMode) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, mode, sizeof(nvmlEnableState_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)mode, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetPowerManagementLimit(nvmlDevice_t device, unsigned int* limit)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)limit, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetPowerManagementLimit) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, limit, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)limit, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetPowerManagementLimitConstraints(nvmlDevice_t device, unsigned int* minLimit, unsigned int* maxLimit)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)minLimit, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)maxLimit, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetPowerManagementLimitConstraints) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, minLimit, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, maxLimit, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)minLimit, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)maxLimit, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetPowerManagementDefaultLimit(nvmlDevice_t device, unsigned int* defaultLimit)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)defaultLimit, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetPowerManagementDefaultLimit) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, defaultLimit, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)defaultLimit, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetPowerUsage(nvmlDevice_t device, unsigned int* power)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)power, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetPowerUsage) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, power, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)power, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetTotalEnergyConsumption(nvmlDevice_t device, unsigned long long* energy)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)energy, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetTotalEnergyConsumption) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, energy, sizeof(unsigned long long)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)energy, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetEnforcedPowerLimit(nvmlDevice_t device, unsigned int* limit)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)limit, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetEnforcedPowerLimit) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, limit, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)limit, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetGpuOperationMode(nvmlDevice_t device, nvmlGpuOperationMode_t* current, nvmlGpuOperationMode_t* pending)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)current, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pending, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetGpuOperationMode) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, current, sizeof(nvmlGpuOperationMode_t)) < 0 ||
        rpc_read(conn, pending, sizeof(nvmlGpuOperationMode_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)current, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pending, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetMemoryInfo(nvmlDevice_t device, nvmlMemory_t* memory)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)memory, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetMemoryInfo) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, memory, sizeof(nvmlMemory_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)memory, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetMemoryInfo_v2(nvmlDevice_t device, nvmlMemory_v2_t* memory)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)memory, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetMemoryInfo_v2) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, memory, sizeof(nvmlMemory_v2_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)memory, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetComputeMode(nvmlDevice_t device, nvmlComputeMode_t* mode)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)mode, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetComputeMode) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, mode, sizeof(nvmlComputeMode_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)mode, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetCudaComputeCapability(nvmlDevice_t device, int* major, int* minor)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)major, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)minor, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetCudaComputeCapability) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, major, sizeof(int)) < 0 ||
        rpc_read(conn, minor, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)major, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)minor, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetEccMode(nvmlDevice_t device, nvmlEnableState_t* current, nvmlEnableState_t* pending)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)current, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pending, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetEccMode) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, current, sizeof(nvmlEnableState_t)) < 0 ||
        rpc_read(conn, pending, sizeof(nvmlEnableState_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)current, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pending, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetDefaultEccMode(nvmlDevice_t device, nvmlEnableState_t* defaultMode)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)defaultMode, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetDefaultEccMode) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, defaultMode, sizeof(nvmlEnableState_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)defaultMode, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetBoardId(nvmlDevice_t device, unsigned int* boardId)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)boardId, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetBoardId) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, boardId, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)boardId, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetMultiGpuBoard(nvmlDevice_t device, unsigned int* multiGpuBool)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)multiGpuBool, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetMultiGpuBoard) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, multiGpuBool, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)multiGpuBool, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetTotalEccErrors(nvmlDevice_t device, nvmlMemoryErrorType_t errorType, nvmlEccCounterType_t counterType, unsigned long long* eccCounts)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&errorType, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&counterType, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)eccCounts, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetTotalEccErrors) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &errorType, sizeof(nvmlMemoryErrorType_t)) < 0 ||
        rpc_write(conn, &counterType, sizeof(nvmlEccCounterType_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, eccCounts, sizeof(unsigned long long)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&errorType, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&counterType, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)eccCounts, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetDetailedEccErrors(nvmlDevice_t device, nvmlMemoryErrorType_t errorType, nvmlEccCounterType_t counterType, nvmlEccErrorCounts_t* eccCounts)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&errorType, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&counterType, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)eccCounts, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetDetailedEccErrors) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &errorType, sizeof(nvmlMemoryErrorType_t)) < 0 ||
        rpc_write(conn, &counterType, sizeof(nvmlEccCounterType_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, eccCounts, sizeof(nvmlEccErrorCounts_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&errorType, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&counterType, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)eccCounts, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetMemoryErrorCounter(nvmlDevice_t device, nvmlMemoryErrorType_t errorType, nvmlEccCounterType_t counterType, nvmlMemoryLocation_t locationType, unsigned long long* count)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&errorType, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&counterType, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&locationType, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)count, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetMemoryErrorCounter) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &errorType, sizeof(nvmlMemoryErrorType_t)) < 0 ||
        rpc_write(conn, &counterType, sizeof(nvmlEccCounterType_t)) < 0 ||
        rpc_write(conn, &locationType, sizeof(nvmlMemoryLocation_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, count, sizeof(unsigned long long)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&errorType, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&counterType, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&locationType, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)count, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetUtilizationRates(nvmlDevice_t device, nvmlUtilization_t* utilization)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)utilization, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetUtilizationRates) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, utilization, sizeof(nvmlUtilization_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)utilization, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetEncoderUtilization(nvmlDevice_t device, unsigned int* utilization, unsigned int* samplingPeriodUs)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)utilization, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)samplingPeriodUs, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetEncoderUtilization) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, utilization, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, samplingPeriodUs, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)utilization, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)samplingPeriodUs, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetEncoderCapacity(nvmlDevice_t device, nvmlEncoderType_t encoderQueryType, unsigned int* encoderCapacity)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&encoderQueryType, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)encoderCapacity, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetEncoderCapacity) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &encoderQueryType, sizeof(nvmlEncoderType_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, encoderCapacity, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&encoderQueryType, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)encoderCapacity, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetEncoderStats(nvmlDevice_t device, unsigned int* sessionCount, unsigned int* averageFps, unsigned int* averageLatency)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)sessionCount, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)averageFps, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)averageLatency, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetEncoderStats) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, sessionCount, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, averageFps, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, averageLatency, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)sessionCount, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)averageFps, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)averageLatency, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetEncoderSessions(nvmlDevice_t device, unsigned int* sessionCount, nvmlEncoderSessionInfo_t* sessionInfos)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)sessionCount, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)sessionInfos, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*sessionCount) && is_unified_pointer(conn, (void*)sessionInfos); i++)
      if (maybe_copy_unified_arg(conn, (void*)&sessionInfos[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetEncoderSessions) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, sessionCount, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, sessionCount, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, sessionInfos, *sessionCount * sizeof(nvmlEncoderSessionInfo_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)sessionCount, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)sessionInfos, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*sessionCount) && is_unified_pointer(conn, (void*)sessionInfos); i++)
      if (maybe_copy_unified_arg(conn, (void*)&sessionInfos[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetDecoderUtilization(nvmlDevice_t device, unsigned int* utilization, unsigned int* samplingPeriodUs)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)utilization, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)samplingPeriodUs, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetDecoderUtilization) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, utilization, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, samplingPeriodUs, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)utilization, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)samplingPeriodUs, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetJpgUtilization(nvmlDevice_t device, unsigned int* utilization, unsigned int* samplingPeriodUs)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)utilization, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)samplingPeriodUs, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetJpgUtilization) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, utilization, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, samplingPeriodUs, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, utilization, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, samplingPeriodUs, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)utilization, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)samplingPeriodUs, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetOfaUtilization(nvmlDevice_t device, unsigned int* utilization, unsigned int* samplingPeriodUs)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)utilization, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)samplingPeriodUs, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetOfaUtilization) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, utilization, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, samplingPeriodUs, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, utilization, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, samplingPeriodUs, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)utilization, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)samplingPeriodUs, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetFBCStats(nvmlDevice_t device, nvmlFBCStats_t* fbcStats)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)fbcStats, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetFBCStats) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, fbcStats, sizeof(nvmlFBCStats_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)fbcStats, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetFBCSessions(nvmlDevice_t device, unsigned int* sessionCount, nvmlFBCSessionInfo_t* sessionInfo)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)sessionCount, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)sessionInfo, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*sessionCount) && is_unified_pointer(conn, (void*)sessionInfo); i++)
      if (maybe_copy_unified_arg(conn, (void*)&sessionInfo[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetFBCSessions) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, sessionCount, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, sessionCount, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, sessionInfo, *sessionCount * sizeof(nvmlFBCSessionInfo_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)sessionCount, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)sessionInfo, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*sessionCount) && is_unified_pointer(conn, (void*)sessionInfo); i++)
      if (maybe_copy_unified_arg(conn, (void*)&sessionInfo[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetDriverModel(nvmlDevice_t device, nvmlDriverModel_t* current, nvmlDriverModel_t* pending)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)current, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pending, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetDriverModel) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, current, sizeof(nvmlDriverModel_t)) < 0 ||
        rpc_read(conn, pending, sizeof(nvmlDriverModel_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)current, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pending, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetVbiosVersion(nvmlDevice_t device, char* version, unsigned int length)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&length, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)version, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(length) && is_unified_pointer(conn, (void*)version); i++)
      if (maybe_copy_unified_arg(conn, (void*)&version[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetVbiosVersion) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &length, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, version, length * sizeof(char)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&length, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)version, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(length) && is_unified_pointer(conn, (void*)version); i++)
      if (maybe_copy_unified_arg(conn, (void*)&version[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetBridgeChipInfo(nvmlDevice_t device, nvmlBridgeChipHierarchy_t* bridgeHierarchy)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)bridgeHierarchy, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetBridgeChipInfo) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, bridgeHierarchy, sizeof(nvmlBridgeChipHierarchy_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)bridgeHierarchy, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetComputeRunningProcesses_v3(nvmlDevice_t device, unsigned int* infoCount, nvmlProcessInfo_t* infos)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)infoCount, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)infos, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*infoCount) && is_unified_pointer(conn, (void*)infos); i++)
      if (maybe_copy_unified_arg(conn, (void*)&infos[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetComputeRunningProcesses_v3) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, infoCount, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, infoCount, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, infos, *infoCount * sizeof(nvmlProcessInfo_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)infoCount, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)infos, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*infoCount) && is_unified_pointer(conn, (void*)infos); i++)
      if (maybe_copy_unified_arg(conn, (void*)&infos[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetGraphicsRunningProcesses_v3(nvmlDevice_t device, unsigned int* infoCount, nvmlProcessInfo_t* infos)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)infoCount, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)infos, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*infoCount) && is_unified_pointer(conn, (void*)infos); i++)
      if (maybe_copy_unified_arg(conn, (void*)&infos[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetGraphicsRunningProcesses_v3) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, infoCount, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, infoCount, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, infos, *infoCount * sizeof(nvmlProcessInfo_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)infoCount, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)infos, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*infoCount) && is_unified_pointer(conn, (void*)infos); i++)
      if (maybe_copy_unified_arg(conn, (void*)&infos[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetMPSComputeRunningProcesses_v3(nvmlDevice_t device, unsigned int* infoCount, nvmlProcessInfo_t* infos)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)infoCount, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)infos, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*infoCount) && is_unified_pointer(conn, (void*)infos); i++)
      if (maybe_copy_unified_arg(conn, (void*)&infos[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetMPSComputeRunningProcesses_v3) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, infoCount, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, infoCount, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, infos, *infoCount * sizeof(nvmlProcessInfo_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)infoCount, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)infos, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*infoCount) && is_unified_pointer(conn, (void*)infos); i++)
      if (maybe_copy_unified_arg(conn, (void*)&infos[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetRunningProcessDetailList(nvmlDevice_t device, nvmlProcessDetailList_t* plist)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)plist, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetRunningProcessDetailList) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, plist, sizeof(nvmlProcessDetailList_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, plist, sizeof(nvmlProcessDetailList_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)plist, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceOnSameBoard(nvmlDevice_t device1, nvmlDevice_t device2, int* onSameBoard)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device1, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device2, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)onSameBoard, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceOnSameBoard) < 0 ||
        rpc_write(conn, &device1, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &device2, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, onSameBoard, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device1, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device2, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)onSameBoard, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetAPIRestriction(nvmlDevice_t device, nvmlRestrictedAPI_t apiType, nvmlEnableState_t* isRestricted)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&apiType, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)isRestricted, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetAPIRestriction) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &apiType, sizeof(nvmlRestrictedAPI_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, isRestricted, sizeof(nvmlEnableState_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&apiType, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)isRestricted, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetSamples(nvmlDevice_t device, nvmlSamplingType_t type, unsigned long long lastSeenTimeStamp, nvmlValueType_t* sampleValType, unsigned int* sampleCount, nvmlSample_t* samples)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&type, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&lastSeenTimeStamp, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)sampleValType, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)sampleCount, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)samples, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*sampleCount) && is_unified_pointer(conn, (void*)samples); i++)
      if (maybe_copy_unified_arg(conn, (void*)&samples[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetSamples) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &type, sizeof(nvmlSamplingType_t)) < 0 ||
        rpc_write(conn, &lastSeenTimeStamp, sizeof(unsigned long long)) < 0 ||
        rpc_write(conn, sampleCount, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, sampleValType, sizeof(nvmlValueType_t)) < 0 ||
        rpc_read(conn, sampleCount, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, samples, *sampleCount * sizeof(nvmlSample_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&type, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&lastSeenTimeStamp, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)sampleValType, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)sampleCount, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)samples, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*sampleCount) && is_unified_pointer(conn, (void*)samples); i++)
      if (maybe_copy_unified_arg(conn, (void*)&samples[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetBAR1MemoryInfo(nvmlDevice_t device, nvmlBAR1Memory_t* bar1Memory)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)bar1Memory, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetBAR1MemoryInfo) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, bar1Memory, sizeof(nvmlBAR1Memory_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)bar1Memory, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetViolationStatus(nvmlDevice_t device, nvmlPerfPolicyType_t perfPolicyType, nvmlViolationTime_t* violTime)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&perfPolicyType, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)violTime, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetViolationStatus) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &perfPolicyType, sizeof(nvmlPerfPolicyType_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, violTime, sizeof(nvmlViolationTime_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&perfPolicyType, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)violTime, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetIrqNum(nvmlDevice_t device, unsigned int* irqNum)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)irqNum, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetIrqNum) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, irqNum, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)irqNum, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetNumGpuCores(nvmlDevice_t device, unsigned int* numCores)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)numCores, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetNumGpuCores) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, numCores, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)numCores, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetPowerSource(nvmlDevice_t device, nvmlPowerSource_t* powerSource)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)powerSource, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetPowerSource) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, powerSource, sizeof(nvmlPowerSource_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)powerSource, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetMemoryBusWidth(nvmlDevice_t device, unsigned int* busWidth)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)busWidth, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetMemoryBusWidth) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, busWidth, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)busWidth, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetPcieLinkMaxSpeed(nvmlDevice_t device, unsigned int* maxSpeed)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)maxSpeed, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetPcieLinkMaxSpeed) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, maxSpeed, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)maxSpeed, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetPcieSpeed(nvmlDevice_t device, unsigned int* pcieSpeed)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pcieSpeed, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetPcieSpeed) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pcieSpeed, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pcieSpeed, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetAdaptiveClockInfoStatus(nvmlDevice_t device, unsigned int* adaptiveClockStatus)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)adaptiveClockStatus, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetAdaptiveClockInfoStatus) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, adaptiveClockStatus, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)adaptiveClockStatus, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetBusType(nvmlDevice_t device, nvmlBusType_t* type)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)type, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetBusType) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, type, sizeof(nvmlBusType_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)type, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetGpuFabricInfo(nvmlDevice_t device, nvmlGpuFabricInfo_t* gpuFabricInfo)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)gpuFabricInfo, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetGpuFabricInfo) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, gpuFabricInfo, sizeof(nvmlGpuFabricInfo_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)gpuFabricInfo, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetGpuFabricInfoV(nvmlDevice_t device, nvmlGpuFabricInfoV_t* gpuFabricInfo)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)gpuFabricInfo, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetGpuFabricInfoV) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, gpuFabricInfo, sizeof(nvmlGpuFabricInfoV_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, gpuFabricInfo, sizeof(nvmlGpuFabricInfoV_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)gpuFabricInfo, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlSystemGetConfComputeCapabilities(nvmlConfComputeSystemCaps_t* capabilities)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)capabilities, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlSystemGetConfComputeCapabilities) < 0 ||
        rpc_write(conn, capabilities, sizeof(nvmlConfComputeSystemCaps_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, capabilities, sizeof(nvmlConfComputeSystemCaps_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)capabilities, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlSystemGetConfComputeState(nvmlConfComputeSystemState_t* state)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)state, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlSystemGetConfComputeState) < 0 ||
        rpc_write(conn, state, sizeof(nvmlConfComputeSystemState_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, state, sizeof(nvmlConfComputeSystemState_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)state, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetConfComputeMemSizeInfo(nvmlDevice_t device, nvmlConfComputeMemSizeInfo_t* memInfo)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)memInfo, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetConfComputeMemSizeInfo) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, memInfo, sizeof(nvmlConfComputeMemSizeInfo_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, memInfo, sizeof(nvmlConfComputeMemSizeInfo_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)memInfo, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlSystemGetConfComputeGpusReadyState(unsigned int* isAcceptingWork)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)isAcceptingWork, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlSystemGetConfComputeGpusReadyState) < 0 ||
        rpc_write(conn, isAcceptingWork, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, isAcceptingWork, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)isAcceptingWork, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetConfComputeProtectedMemoryUsage(nvmlDevice_t device, nvmlMemory_t* memory)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)memory, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetConfComputeProtectedMemoryUsage) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, memory, sizeof(nvmlMemory_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, memory, sizeof(nvmlMemory_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)memory, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetConfComputeGpuCertificate(nvmlDevice_t device, nvmlConfComputeGpuCertificate_t* gpuCert)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)gpuCert, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetConfComputeGpuCertificate) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, gpuCert, sizeof(nvmlConfComputeGpuCertificate_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, gpuCert, sizeof(nvmlConfComputeGpuCertificate_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)gpuCert, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetConfComputeGpuAttestationReport(nvmlDevice_t device, nvmlConfComputeGpuAttestationReport_t* gpuAtstReport)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)gpuAtstReport, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetConfComputeGpuAttestationReport) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, gpuAtstReport, sizeof(nvmlConfComputeGpuAttestationReport_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, gpuAtstReport, sizeof(nvmlConfComputeGpuAttestationReport_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)gpuAtstReport, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlSystemGetConfComputeKeyRotationThresholdInfo(nvmlConfComputeGetKeyRotationThresholdInfo_t* pKeyRotationThrInfo)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pKeyRotationThrInfo, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlSystemGetConfComputeKeyRotationThresholdInfo) < 0 ||
        rpc_write(conn, pKeyRotationThrInfo, sizeof(nvmlConfComputeGetKeyRotationThresholdInfo_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pKeyRotationThrInfo, sizeof(nvmlConfComputeGetKeyRotationThresholdInfo_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pKeyRotationThrInfo, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlSystemGetConfComputeSettings(nvmlSystemConfComputeSettings_t* settings)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)settings, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlSystemGetConfComputeSettings) < 0 ||
        rpc_write(conn, settings, sizeof(nvmlSystemConfComputeSettings_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, settings, sizeof(nvmlSystemConfComputeSettings_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)settings, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetGspFirmwareVersion(nvmlDevice_t device, char* version)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)version, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetGspFirmwareVersion) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, version, sizeof(char)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)version, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetGspFirmwareMode(nvmlDevice_t device, unsigned int* isEnabled, unsigned int* defaultMode)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)isEnabled, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)defaultMode, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetGspFirmwareMode) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, isEnabled, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, defaultMode, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)isEnabled, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)defaultMode, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetAccountingMode(nvmlDevice_t device, nvmlEnableState_t* mode)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)mode, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetAccountingMode) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, mode, sizeof(nvmlEnableState_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)mode, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetAccountingStats(nvmlDevice_t device, unsigned int pid, nvmlAccountingStats_t* stats)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&pid, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)stats, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetAccountingStats) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &pid, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, stats, sizeof(nvmlAccountingStats_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&pid, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)stats, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetAccountingPids(nvmlDevice_t device, unsigned int* count, unsigned int* pids)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)count, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pids, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*count) && is_unified_pointer(conn, (void*)pids); i++)
      if (maybe_copy_unified_arg(conn, (void*)&pids[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetAccountingPids) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, count, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, count, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, pids, *count * sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)count, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pids, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*count) && is_unified_pointer(conn, (void*)pids); i++)
      if (maybe_copy_unified_arg(conn, (void*)&pids[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetAccountingBufferSize(nvmlDevice_t device, unsigned int* bufferSize)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)bufferSize, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetAccountingBufferSize) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, bufferSize, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)bufferSize, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetRetiredPages(nvmlDevice_t device, nvmlPageRetirementCause_t cause, unsigned int* pageCount, unsigned long long* addresses)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&cause, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pageCount, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)addresses, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*pageCount) && is_unified_pointer(conn, (void*)addresses); i++)
      if (maybe_copy_unified_arg(conn, (void*)&addresses[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetRetiredPages) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &cause, sizeof(nvmlPageRetirementCause_t)) < 0 ||
        rpc_write(conn, pageCount, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pageCount, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, addresses, *pageCount * sizeof(unsigned long long)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&cause, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pageCount, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)addresses, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*pageCount) && is_unified_pointer(conn, (void*)addresses); i++)
      if (maybe_copy_unified_arg(conn, (void*)&addresses[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetRetiredPages_v2(nvmlDevice_t device, nvmlPageRetirementCause_t cause, unsigned int* pageCount, unsigned long long* addresses, unsigned long long* timestamps)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&cause, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pageCount, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)addresses, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*pageCount) && is_unified_pointer(conn, (void*)addresses); i++)
      if (maybe_copy_unified_arg(conn, (void*)&addresses[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)timestamps, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*pageCount) && is_unified_pointer(conn, (void*)timestamps); i++)
      if (maybe_copy_unified_arg(conn, (void*)&timestamps[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetRetiredPages_v2) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &cause, sizeof(nvmlPageRetirementCause_t)) < 0 ||
        rpc_write(conn, pageCount, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pageCount, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, addresses, *pageCount * sizeof(unsigned long long)) < 0 ||
        rpc_read(conn, timestamps, *pageCount * sizeof(unsigned long long)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&cause, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pageCount, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)addresses, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*pageCount) && is_unified_pointer(conn, (void*)addresses); i++)
      if (maybe_copy_unified_arg(conn, (void*)&addresses[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)timestamps, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*pageCount) && is_unified_pointer(conn, (void*)timestamps); i++)
      if (maybe_copy_unified_arg(conn, (void*)&timestamps[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetRetiredPagesPendingStatus(nvmlDevice_t device, nvmlEnableState_t* isPending)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)isPending, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetRetiredPagesPendingStatus) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, isPending, sizeof(nvmlEnableState_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)isPending, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetRemappedRows(nvmlDevice_t device, unsigned int* corrRows, unsigned int* uncRows, unsigned int* isPending, unsigned int* failureOccurred)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)corrRows, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)uncRows, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)isPending, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)failureOccurred, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetRemappedRows) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, corrRows, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, uncRows, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, isPending, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, failureOccurred, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)corrRows, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)uncRows, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)isPending, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)failureOccurred, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetRowRemapperHistogram(nvmlDevice_t device, nvmlRowRemapperHistogramValues_t* values)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)values, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetRowRemapperHistogram) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, values, sizeof(nvmlRowRemapperHistogramValues_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)values, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetArchitecture(nvmlDevice_t device, nvmlDeviceArchitecture_t* arch)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)arch, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetArchitecture) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, arch, sizeof(nvmlDeviceArchitecture_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)arch, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetClkMonStatus(nvmlDevice_t device, nvmlClkMonStatus_t* status)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)status, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetClkMonStatus) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, status, sizeof(nvmlClkMonStatus_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)status, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetProcessUtilization(nvmlDevice_t device, nvmlProcessUtilizationSample_t* utilization, unsigned int* processSamplesCount, unsigned long long lastSeenTimeStamp)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)processSamplesCount, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)utilization, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*processSamplesCount) && is_unified_pointer(conn, (void*)utilization); i++)
      if (maybe_copy_unified_arg(conn, (void*)&utilization[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&lastSeenTimeStamp, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetProcessUtilization) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, processSamplesCount, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &lastSeenTimeStamp, sizeof(unsigned long long)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, processSamplesCount, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, utilization, *processSamplesCount * sizeof(nvmlProcessUtilizationSample_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)processSamplesCount, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)utilization, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*processSamplesCount) && is_unified_pointer(conn, (void*)utilization); i++)
      if (maybe_copy_unified_arg(conn, (void*)&utilization[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&lastSeenTimeStamp, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetProcessesUtilizationInfo(nvmlDevice_t device, nvmlProcessesUtilizationInfo_t* procesesUtilInfo)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)procesesUtilInfo, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetProcessesUtilizationInfo) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, procesesUtilInfo, sizeof(nvmlProcessesUtilizationInfo_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, procesesUtilInfo, sizeof(nvmlProcessesUtilizationInfo_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)procesesUtilInfo, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlUnitSetLedState(nvmlUnit_t unit, nvmlLedColor_t color)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&unit, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&color, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlUnitSetLedState) < 0 ||
        rpc_write(conn, &unit, sizeof(nvmlUnit_t)) < 0 ||
        rpc_write(conn, &color, sizeof(nvmlLedColor_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&unit, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&color, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceSetPersistenceMode(nvmlDevice_t device, nvmlEnableState_t mode)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&mode, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceSetPersistenceMode) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &mode, sizeof(nvmlEnableState_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&mode, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceSetComputeMode(nvmlDevice_t device, nvmlComputeMode_t mode)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&mode, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceSetComputeMode) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &mode, sizeof(nvmlComputeMode_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&mode, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceSetEccMode(nvmlDevice_t device, nvmlEnableState_t ecc)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&ecc, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceSetEccMode) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &ecc, sizeof(nvmlEnableState_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&ecc, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceClearEccErrorCounts(nvmlDevice_t device, nvmlEccCounterType_t counterType)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&counterType, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceClearEccErrorCounts) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &counterType, sizeof(nvmlEccCounterType_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&counterType, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceSetDriverModel(nvmlDevice_t device, nvmlDriverModel_t driverModel, unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&driverModel, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceSetDriverModel) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &driverModel, sizeof(nvmlDriverModel_t)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&driverModel, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceSetGpuLockedClocks(nvmlDevice_t device, unsigned int minGpuClockMHz, unsigned int maxGpuClockMHz)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&minGpuClockMHz, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&maxGpuClockMHz, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceSetGpuLockedClocks) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &minGpuClockMHz, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &maxGpuClockMHz, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&minGpuClockMHz, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&maxGpuClockMHz, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceResetGpuLockedClocks(nvmlDevice_t device)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceResetGpuLockedClocks) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceSetMemoryLockedClocks(nvmlDevice_t device, unsigned int minMemClockMHz, unsigned int maxMemClockMHz)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&minMemClockMHz, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&maxMemClockMHz, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceSetMemoryLockedClocks) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &minMemClockMHz, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &maxMemClockMHz, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&minMemClockMHz, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&maxMemClockMHz, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceResetMemoryLockedClocks(nvmlDevice_t device)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceResetMemoryLockedClocks) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceSetApplicationsClocks(nvmlDevice_t device, unsigned int memClockMHz, unsigned int graphicsClockMHz)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&memClockMHz, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&graphicsClockMHz, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceSetApplicationsClocks) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &memClockMHz, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &graphicsClockMHz, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&memClockMHz, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&graphicsClockMHz, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceResetApplicationsClocks(nvmlDevice_t device)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceResetApplicationsClocks) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceSetAutoBoostedClocksEnabled(nvmlDevice_t device, nvmlEnableState_t enabled)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&enabled, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceSetAutoBoostedClocksEnabled) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &enabled, sizeof(nvmlEnableState_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&enabled, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceSetDefaultAutoBoostedClocksEnabled(nvmlDevice_t device, nvmlEnableState_t enabled, unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&enabled, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceSetDefaultAutoBoostedClocksEnabled) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &enabled, sizeof(nvmlEnableState_t)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&enabled, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceSetDefaultFanSpeed_v2(nvmlDevice_t device, unsigned int fan)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&fan, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceSetDefaultFanSpeed_v2) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &fan, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&fan, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceSetFanControlPolicy(nvmlDevice_t device, unsigned int fan, nvmlFanControlPolicy_t policy)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&fan, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&policy, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceSetFanControlPolicy) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &fan, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &policy, sizeof(nvmlFanControlPolicy_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&fan, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&policy, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceSetTemperatureThreshold(nvmlDevice_t device, nvmlTemperatureThresholds_t thresholdType, int* temp)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&thresholdType, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)temp, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceSetTemperatureThreshold) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &thresholdType, sizeof(nvmlTemperatureThresholds_t)) < 0 ||
        rpc_write(conn, temp, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, temp, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&thresholdType, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)temp, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceSetPowerManagementLimit(nvmlDevice_t device, unsigned int limit)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&limit, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceSetPowerManagementLimit) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &limit, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&limit, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceSetGpuOperationMode(nvmlDevice_t device, nvmlGpuOperationMode_t mode)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&mode, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceSetGpuOperationMode) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &mode, sizeof(nvmlGpuOperationMode_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&mode, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceSetAPIRestriction(nvmlDevice_t device, nvmlRestrictedAPI_t apiType, nvmlEnableState_t isRestricted)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&apiType, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&isRestricted, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceSetAPIRestriction) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &apiType, sizeof(nvmlRestrictedAPI_t)) < 0 ||
        rpc_write(conn, &isRestricted, sizeof(nvmlEnableState_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&apiType, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&isRestricted, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceSetFanSpeed_v2(nvmlDevice_t device, unsigned int fan, unsigned int speed)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&fan, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&speed, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceSetFanSpeed_v2) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &fan, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &speed, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&fan, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&speed, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceSetGpcClkVfOffset(nvmlDevice_t device, int offset)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&offset, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceSetGpcClkVfOffset) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &offset, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&offset, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceSetMemClkVfOffset(nvmlDevice_t device, int offset)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&offset, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceSetMemClkVfOffset) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &offset, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&offset, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceSetConfComputeUnprotectedMemSize(nvmlDevice_t device, unsigned long long sizeKiB)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&sizeKiB, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceSetConfComputeUnprotectedMemSize) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &sizeKiB, sizeof(unsigned long long)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&sizeKiB, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlSystemSetConfComputeGpusReadyState(unsigned int isAcceptingWork)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&isAcceptingWork, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlSystemSetConfComputeGpusReadyState) < 0 ||
        rpc_write(conn, &isAcceptingWork, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&isAcceptingWork, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlSystemSetConfComputeKeyRotationThresholdInfo(nvmlConfComputeSetKeyRotationThresholdInfo_t* pKeyRotationThrInfo)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pKeyRotationThrInfo, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlSystemSetConfComputeKeyRotationThresholdInfo) < 0 ||
        rpc_write(conn, pKeyRotationThrInfo, sizeof(nvmlConfComputeSetKeyRotationThresholdInfo_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pKeyRotationThrInfo, sizeof(nvmlConfComputeSetKeyRotationThresholdInfo_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pKeyRotationThrInfo, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceSetAccountingMode(nvmlDevice_t device, nvmlEnableState_t mode)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&mode, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceSetAccountingMode) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &mode, sizeof(nvmlEnableState_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&mode, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceClearAccountingPids(nvmlDevice_t device)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceClearAccountingPids) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetNvLinkState(nvmlDevice_t device, unsigned int link, nvmlEnableState_t* isActive)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&link, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)isActive, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetNvLinkState) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &link, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, isActive, sizeof(nvmlEnableState_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&link, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)isActive, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetNvLinkVersion(nvmlDevice_t device, unsigned int link, unsigned int* version)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&link, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)version, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetNvLinkVersion) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &link, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, version, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&link, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)version, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetNvLinkCapability(nvmlDevice_t device, unsigned int link, nvmlNvLinkCapability_t capability, unsigned int* capResult)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&link, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&capability, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)capResult, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetNvLinkCapability) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &link, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &capability, sizeof(nvmlNvLinkCapability_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, capResult, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&link, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&capability, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)capResult, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetNvLinkRemotePciInfo_v2(nvmlDevice_t device, unsigned int link, nvmlPciInfo_t* pci)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&link, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pci, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetNvLinkRemotePciInfo_v2) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &link, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pci, sizeof(nvmlPciInfo_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&link, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pci, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetNvLinkErrorCounter(nvmlDevice_t device, unsigned int link, nvmlNvLinkErrorCounter_t counter, unsigned long long* counterValue)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&link, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&counter, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)counterValue, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetNvLinkErrorCounter) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &link, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &counter, sizeof(nvmlNvLinkErrorCounter_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, counterValue, sizeof(unsigned long long)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&link, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&counter, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)counterValue, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceResetNvLinkErrorCounters(nvmlDevice_t device, unsigned int link)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&link, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceResetNvLinkErrorCounters) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &link, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&link, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceSetNvLinkUtilizationControl(nvmlDevice_t device, unsigned int link, unsigned int counter, nvmlNvLinkUtilizationControl_t* control, unsigned int reset)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&link, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&counter, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)control, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&reset, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceSetNvLinkUtilizationControl) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &link, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &counter, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &control, sizeof(nvmlNvLinkUtilizationControl_t*)) < 0 ||
        rpc_write(conn, &reset, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&link, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&counter, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)control, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&reset, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetNvLinkUtilizationControl(nvmlDevice_t device, unsigned int link, unsigned int counter, nvmlNvLinkUtilizationControl_t* control)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&link, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&counter, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)control, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetNvLinkUtilizationControl) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &link, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &counter, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, control, sizeof(nvmlNvLinkUtilizationControl_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&link, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&counter, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)control, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetNvLinkUtilizationCounter(nvmlDevice_t device, unsigned int link, unsigned int counter, unsigned long long* rxcounter, unsigned long long* txcounter)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&link, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&counter, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)rxcounter, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)txcounter, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetNvLinkUtilizationCounter) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &link, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &counter, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, rxcounter, sizeof(unsigned long long)) < 0 ||
        rpc_read(conn, txcounter, sizeof(unsigned long long)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&link, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&counter, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)rxcounter, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)txcounter, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceFreezeNvLinkUtilizationCounter(nvmlDevice_t device, unsigned int link, unsigned int counter, nvmlEnableState_t freeze)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&link, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&counter, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&freeze, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceFreezeNvLinkUtilizationCounter) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &link, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &counter, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &freeze, sizeof(nvmlEnableState_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&link, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&counter, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&freeze, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceResetNvLinkUtilizationCounter(nvmlDevice_t device, unsigned int link, unsigned int counter)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&link, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&counter, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceResetNvLinkUtilizationCounter) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &link, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &counter, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&link, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&counter, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetNvLinkRemoteDeviceType(nvmlDevice_t device, unsigned int link, nvmlIntNvLinkDeviceType_t* pNvLinkDeviceType)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&link, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pNvLinkDeviceType, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetNvLinkRemoteDeviceType) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &link, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pNvLinkDeviceType, sizeof(nvmlIntNvLinkDeviceType_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&link, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pNvLinkDeviceType, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlEventSetCreate(nvmlEventSet_t* set)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)set, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlEventSetCreate) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, set, sizeof(nvmlEventSet_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)set, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceRegisterEvents(nvmlDevice_t device, unsigned long long eventTypes, nvmlEventSet_t set)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&eventTypes, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&set, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceRegisterEvents) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &eventTypes, sizeof(unsigned long long)) < 0 ||
        rpc_write(conn, &set, sizeof(nvmlEventSet_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&eventTypes, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&set, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetSupportedEventTypes(nvmlDevice_t device, unsigned long long* eventTypes)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)eventTypes, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetSupportedEventTypes) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, eventTypes, sizeof(unsigned long long)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)eventTypes, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlEventSetWait_v2(nvmlEventSet_t set, nvmlEventData_t* data, unsigned int timeoutms)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&set, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)data, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&timeoutms, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlEventSetWait_v2) < 0 ||
        rpc_write(conn, &set, sizeof(nvmlEventSet_t)) < 0 ||
        rpc_write(conn, &timeoutms, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, data, sizeof(nvmlEventData_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&set, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)data, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&timeoutms, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlEventSetFree(nvmlEventSet_t set)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&set, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlEventSetFree) < 0 ||
        rpc_write(conn, &set, sizeof(nvmlEventSet_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&set, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceModifyDrainState(nvmlPciInfo_t* pciInfo, nvmlEnableState_t newState)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pciInfo, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&newState, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceModifyDrainState) < 0 ||
        rpc_write(conn, pciInfo, sizeof(nvmlPciInfo_t)) < 0 ||
        rpc_write(conn, &newState, sizeof(nvmlEnableState_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pciInfo, sizeof(nvmlPciInfo_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pciInfo, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&newState, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceQueryDrainState(nvmlPciInfo_t* pciInfo, nvmlEnableState_t* currentState)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pciInfo, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)currentState, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceQueryDrainState) < 0 ||
        rpc_write(conn, pciInfo, sizeof(nvmlPciInfo_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pciInfo, sizeof(nvmlPciInfo_t)) < 0 ||
        rpc_read(conn, currentState, sizeof(nvmlEnableState_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pciInfo, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)currentState, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceRemoveGpu_v2(nvmlPciInfo_t* pciInfo, nvmlDetachGpuState_t gpuState, nvmlPcieLinkState_t linkState)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pciInfo, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&gpuState, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&linkState, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceRemoveGpu_v2) < 0 ||
        rpc_write(conn, pciInfo, sizeof(nvmlPciInfo_t)) < 0 ||
        rpc_write(conn, &gpuState, sizeof(nvmlDetachGpuState_t)) < 0 ||
        rpc_write(conn, &linkState, sizeof(nvmlPcieLinkState_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pciInfo, sizeof(nvmlPciInfo_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pciInfo, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&gpuState, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&linkState, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceDiscoverGpus(nvmlPciInfo_t* pciInfo)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pciInfo, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceDiscoverGpus) < 0 ||
        rpc_write(conn, pciInfo, sizeof(nvmlPciInfo_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pciInfo, sizeof(nvmlPciInfo_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pciInfo, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetFieldValues(nvmlDevice_t device, int valuesCount, nvmlFieldValue_t* values)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&valuesCount, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)values, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(valuesCount) && is_unified_pointer(conn, (void*)values); i++)
      if (maybe_copy_unified_arg(conn, (void*)&values[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetFieldValues) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &valuesCount, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, values, valuesCount * sizeof(nvmlFieldValue_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&valuesCount, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)values, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(valuesCount) && is_unified_pointer(conn, (void*)values); i++)
      if (maybe_copy_unified_arg(conn, (void*)&values[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceClearFieldValues(nvmlDevice_t device, int valuesCount, nvmlFieldValue_t* values)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&valuesCount, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)values, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(valuesCount) && is_unified_pointer(conn, (void*)values); i++)
      if (maybe_copy_unified_arg(conn, (void*)&values[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceClearFieldValues) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &valuesCount, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, values, valuesCount * sizeof(nvmlFieldValue_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&valuesCount, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)values, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(valuesCount) && is_unified_pointer(conn, (void*)values); i++)
      if (maybe_copy_unified_arg(conn, (void*)&values[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetVirtualizationMode(nvmlDevice_t device, nvmlGpuVirtualizationMode_t* pVirtualMode)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pVirtualMode, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetVirtualizationMode) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pVirtualMode, sizeof(nvmlGpuVirtualizationMode_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pVirtualMode, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetHostVgpuMode(nvmlDevice_t device, nvmlHostVgpuMode_t* pHostVgpuMode)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pHostVgpuMode, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetHostVgpuMode) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pHostVgpuMode, sizeof(nvmlHostVgpuMode_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pHostVgpuMode, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceSetVirtualizationMode(nvmlDevice_t device, nvmlGpuVirtualizationMode_t virtualMode)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&virtualMode, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceSetVirtualizationMode) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &virtualMode, sizeof(nvmlGpuVirtualizationMode_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&virtualMode, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetVgpuHeterogeneousMode(nvmlDevice_t device, nvmlVgpuHeterogeneousMode_t* pHeterogeneousMode)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pHeterogeneousMode, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetVgpuHeterogeneousMode) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, pHeterogeneousMode, sizeof(nvmlVgpuHeterogeneousMode_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pHeterogeneousMode, sizeof(nvmlVgpuHeterogeneousMode_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pHeterogeneousMode, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceSetVgpuHeterogeneousMode(nvmlDevice_t device, const nvmlVgpuHeterogeneousMode_t* pHeterogeneousMode)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pHeterogeneousMode, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceSetVgpuHeterogeneousMode) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &pHeterogeneousMode, sizeof(const nvmlVgpuHeterogeneousMode_t*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pHeterogeneousMode, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlVgpuInstanceGetPlacementId(nvmlVgpuInstance_t vgpuInstance, nvmlVgpuPlacementId_t* pPlacement)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pPlacement, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlVgpuInstanceGetPlacementId) < 0 ||
        rpc_write(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_write(conn, pPlacement, sizeof(nvmlVgpuPlacementId_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pPlacement, sizeof(nvmlVgpuPlacementId_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pPlacement, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetVgpuTypeSupportedPlacements(nvmlDevice_t device, nvmlVgpuTypeId_t vgpuTypeId, nvmlVgpuPlacementList_t* pPlacementList)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeId, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pPlacementList, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetVgpuTypeSupportedPlacements) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &vgpuTypeId, sizeof(nvmlVgpuTypeId_t)) < 0 ||
        rpc_write(conn, pPlacementList, sizeof(nvmlVgpuPlacementList_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pPlacementList, sizeof(nvmlVgpuPlacementList_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeId, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pPlacementList, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetVgpuTypeCreatablePlacements(nvmlDevice_t device, nvmlVgpuTypeId_t vgpuTypeId, nvmlVgpuPlacementList_t* pPlacementList)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeId, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pPlacementList, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetVgpuTypeCreatablePlacements) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &vgpuTypeId, sizeof(nvmlVgpuTypeId_t)) < 0 ||
        rpc_write(conn, pPlacementList, sizeof(nvmlVgpuPlacementList_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pPlacementList, sizeof(nvmlVgpuPlacementList_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeId, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pPlacementList, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlVgpuTypeGetGspHeapSize(nvmlVgpuTypeId_t vgpuTypeId, unsigned long long* gspHeapSize)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeId, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)gspHeapSize, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlVgpuTypeGetGspHeapSize) < 0 ||
        rpc_write(conn, &vgpuTypeId, sizeof(nvmlVgpuTypeId_t)) < 0 ||
        rpc_write(conn, gspHeapSize, sizeof(unsigned long long)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, gspHeapSize, sizeof(unsigned long long)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeId, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)gspHeapSize, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlVgpuTypeGetFbReservation(nvmlVgpuTypeId_t vgpuTypeId, unsigned long long* fbReservation)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeId, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)fbReservation, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlVgpuTypeGetFbReservation) < 0 ||
        rpc_write(conn, &vgpuTypeId, sizeof(nvmlVgpuTypeId_t)) < 0 ||
        rpc_write(conn, fbReservation, sizeof(unsigned long long)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, fbReservation, sizeof(unsigned long long)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeId, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)fbReservation, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceSetVgpuCapabilities(nvmlDevice_t device, nvmlDeviceVgpuCapability_t capability, nvmlEnableState_t state)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&capability, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&state, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceSetVgpuCapabilities) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &capability, sizeof(nvmlDeviceVgpuCapability_t)) < 0 ||
        rpc_write(conn, &state, sizeof(nvmlEnableState_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&capability, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&state, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetGridLicensableFeatures_v4(nvmlDevice_t device, nvmlGridLicensableFeatures_t* pGridLicensableFeatures)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pGridLicensableFeatures, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetGridLicensableFeatures_v4) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pGridLicensableFeatures, sizeof(nvmlGridLicensableFeatures_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pGridLicensableFeatures, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlGetVgpuDriverCapabilities(nvmlVgpuDriverCapability_t capability, unsigned int* capResult)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&capability, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)capResult, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlGetVgpuDriverCapabilities) < 0 ||
        rpc_write(conn, &capability, sizeof(nvmlVgpuDriverCapability_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, capResult, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&capability, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)capResult, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetVgpuCapabilities(nvmlDevice_t device, nvmlDeviceVgpuCapability_t capability, unsigned int* capResult)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&capability, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)capResult, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetVgpuCapabilities) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &capability, sizeof(nvmlDeviceVgpuCapability_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, capResult, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&capability, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)capResult, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetSupportedVgpus(nvmlDevice_t device, unsigned int* vgpuCount, nvmlVgpuTypeId_t* vgpuTypeIds)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vgpuCount, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vgpuTypeIds, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*vgpuCount) && is_unified_pointer(conn, (void*)vgpuTypeIds); i++)
      if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeIds[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetSupportedVgpus) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, vgpuCount, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, vgpuCount, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, vgpuTypeIds, *vgpuCount * sizeof(nvmlVgpuTypeId_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vgpuCount, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vgpuTypeIds, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*vgpuCount) && is_unified_pointer(conn, (void*)vgpuTypeIds); i++)
      if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeIds[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetCreatableVgpus(nvmlDevice_t device, unsigned int* vgpuCount, nvmlVgpuTypeId_t* vgpuTypeIds)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vgpuCount, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vgpuTypeIds, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*vgpuCount) && is_unified_pointer(conn, (void*)vgpuTypeIds); i++)
      if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeIds[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetCreatableVgpus) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, vgpuCount, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, vgpuCount, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, vgpuTypeIds, *vgpuCount * sizeof(nvmlVgpuTypeId_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vgpuCount, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vgpuTypeIds, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*vgpuCount) && is_unified_pointer(conn, (void*)vgpuTypeIds); i++)
      if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeIds[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlVgpuTypeGetClass(nvmlVgpuTypeId_t vgpuTypeId, char* vgpuTypeClass, unsigned int* size)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeId, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)size, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vgpuTypeClass, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*size) && is_unified_pointer(conn, (void*)vgpuTypeClass); i++)
      if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeClass[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlVgpuTypeGetClass) < 0 ||
        rpc_write(conn, &vgpuTypeId, sizeof(nvmlVgpuTypeId_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, size, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, vgpuTypeClass, *size * sizeof(char)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeId, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)size, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vgpuTypeClass, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*size) && is_unified_pointer(conn, (void*)vgpuTypeClass); i++)
      if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeClass[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlVgpuTypeGetName(nvmlVgpuTypeId_t vgpuTypeId, char* vgpuTypeName, unsigned int* size)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeId, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)size, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vgpuTypeName, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*size) && is_unified_pointer(conn, (void*)vgpuTypeName); i++)
      if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeName[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlVgpuTypeGetName) < 0 ||
        rpc_write(conn, &vgpuTypeId, sizeof(nvmlVgpuTypeId_t)) < 0 ||
        rpc_write(conn, size, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, size, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, vgpuTypeName, *size * sizeof(char)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeId, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)size, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vgpuTypeName, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*size) && is_unified_pointer(conn, (void*)vgpuTypeName); i++)
      if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeName[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlVgpuTypeGetGpuInstanceProfileId(nvmlVgpuTypeId_t vgpuTypeId, unsigned int* gpuInstanceProfileId)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeId, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)gpuInstanceProfileId, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlVgpuTypeGetGpuInstanceProfileId) < 0 ||
        rpc_write(conn, &vgpuTypeId, sizeof(nvmlVgpuTypeId_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, gpuInstanceProfileId, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeId, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)gpuInstanceProfileId, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlVgpuTypeGetDeviceID(nvmlVgpuTypeId_t vgpuTypeId, unsigned long long* deviceID, unsigned long long* subsystemID)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeId, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)deviceID, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)subsystemID, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlVgpuTypeGetDeviceID) < 0 ||
        rpc_write(conn, &vgpuTypeId, sizeof(nvmlVgpuTypeId_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, deviceID, sizeof(unsigned long long)) < 0 ||
        rpc_read(conn, subsystemID, sizeof(unsigned long long)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeId, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)deviceID, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)subsystemID, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlVgpuTypeGetFramebufferSize(nvmlVgpuTypeId_t vgpuTypeId, unsigned long long* fbSize)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeId, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)fbSize, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlVgpuTypeGetFramebufferSize) < 0 ||
        rpc_write(conn, &vgpuTypeId, sizeof(nvmlVgpuTypeId_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, fbSize, sizeof(unsigned long long)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeId, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)fbSize, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlVgpuTypeGetNumDisplayHeads(nvmlVgpuTypeId_t vgpuTypeId, unsigned int* numDisplayHeads)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeId, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)numDisplayHeads, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlVgpuTypeGetNumDisplayHeads) < 0 ||
        rpc_write(conn, &vgpuTypeId, sizeof(nvmlVgpuTypeId_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, numDisplayHeads, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeId, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)numDisplayHeads, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlVgpuTypeGetResolution(nvmlVgpuTypeId_t vgpuTypeId, unsigned int displayIndex, unsigned int* xdim, unsigned int* ydim)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeId, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&displayIndex, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)xdim, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)ydim, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlVgpuTypeGetResolution) < 0 ||
        rpc_write(conn, &vgpuTypeId, sizeof(nvmlVgpuTypeId_t)) < 0 ||
        rpc_write(conn, &displayIndex, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, xdim, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, ydim, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeId, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&displayIndex, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)xdim, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)ydim, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlVgpuTypeGetLicense(nvmlVgpuTypeId_t vgpuTypeId, char* vgpuTypeLicenseString, unsigned int size)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeId, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&size, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vgpuTypeLicenseString, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(size) && is_unified_pointer(conn, (void*)vgpuTypeLicenseString); i++)
      if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeLicenseString[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlVgpuTypeGetLicense) < 0 ||
        rpc_write(conn, &vgpuTypeId, sizeof(nvmlVgpuTypeId_t)) < 0 ||
        rpc_write(conn, &size, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, vgpuTypeLicenseString, size * sizeof(char)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeId, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&size, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vgpuTypeLicenseString, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(size) && is_unified_pointer(conn, (void*)vgpuTypeLicenseString); i++)
      if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeLicenseString[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlVgpuTypeGetFrameRateLimit(nvmlVgpuTypeId_t vgpuTypeId, unsigned int* frameRateLimit)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeId, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)frameRateLimit, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlVgpuTypeGetFrameRateLimit) < 0 ||
        rpc_write(conn, &vgpuTypeId, sizeof(nvmlVgpuTypeId_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, frameRateLimit, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeId, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)frameRateLimit, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlVgpuTypeGetMaxInstances(nvmlDevice_t device, nvmlVgpuTypeId_t vgpuTypeId, unsigned int* vgpuInstanceCount)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeId, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vgpuInstanceCount, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlVgpuTypeGetMaxInstances) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &vgpuTypeId, sizeof(nvmlVgpuTypeId_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, vgpuInstanceCount, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeId, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vgpuInstanceCount, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlVgpuTypeGetMaxInstancesPerVm(nvmlVgpuTypeId_t vgpuTypeId, unsigned int* vgpuInstanceCountPerVm)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeId, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vgpuInstanceCountPerVm, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlVgpuTypeGetMaxInstancesPerVm) < 0 ||
        rpc_write(conn, &vgpuTypeId, sizeof(nvmlVgpuTypeId_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, vgpuInstanceCountPerVm, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeId, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vgpuInstanceCountPerVm, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetActiveVgpus(nvmlDevice_t device, unsigned int* vgpuCount, nvmlVgpuInstance_t* vgpuInstances)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vgpuCount, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vgpuInstances, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*vgpuCount) && is_unified_pointer(conn, (void*)vgpuInstances); i++)
      if (maybe_copy_unified_arg(conn, (void*)&vgpuInstances[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetActiveVgpus) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, vgpuCount, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, vgpuCount, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, vgpuInstances, *vgpuCount * sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vgpuCount, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vgpuInstances, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*vgpuCount) && is_unified_pointer(conn, (void*)vgpuInstances); i++)
      if (maybe_copy_unified_arg(conn, (void*)&vgpuInstances[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlVgpuInstanceGetVmID(nvmlVgpuInstance_t vgpuInstance, char* vmId, unsigned int size, nvmlVgpuVmIdType_t* vmIdType)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&size, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vmId, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(size) && is_unified_pointer(conn, (void*)vmId); i++)
      if (maybe_copy_unified_arg(conn, (void*)&vmId[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vmIdType, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlVgpuInstanceGetVmID) < 0 ||
        rpc_write(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_write(conn, &size, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, vmId, size * sizeof(char)) < 0 ||
        rpc_read(conn, vmIdType, sizeof(nvmlVgpuVmIdType_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&size, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vmId, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(size) && is_unified_pointer(conn, (void*)vmId); i++)
      if (maybe_copy_unified_arg(conn, (void*)&vmId[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vmIdType, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlVgpuInstanceGetUUID(nvmlVgpuInstance_t vgpuInstance, char* uuid, unsigned int size)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&size, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)uuid, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(size) && is_unified_pointer(conn, (void*)uuid); i++)
      if (maybe_copy_unified_arg(conn, (void*)&uuid[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlVgpuInstanceGetUUID) < 0 ||
        rpc_write(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_write(conn, &size, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, uuid, size * sizeof(char)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&size, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)uuid, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(size) && is_unified_pointer(conn, (void*)uuid); i++)
      if (maybe_copy_unified_arg(conn, (void*)&uuid[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlVgpuInstanceGetVmDriverVersion(nvmlVgpuInstance_t vgpuInstance, char* version, unsigned int length)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&length, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)version, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(length) && is_unified_pointer(conn, (void*)version); i++)
      if (maybe_copy_unified_arg(conn, (void*)&version[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlVgpuInstanceGetVmDriverVersion) < 0 ||
        rpc_write(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_write(conn, &length, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, version, length * sizeof(char)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&length, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)version, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(length) && is_unified_pointer(conn, (void*)version); i++)
      if (maybe_copy_unified_arg(conn, (void*)&version[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlVgpuInstanceGetFbUsage(nvmlVgpuInstance_t vgpuInstance, unsigned long long* fbUsage)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)fbUsage, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlVgpuInstanceGetFbUsage) < 0 ||
        rpc_write(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, fbUsage, sizeof(unsigned long long)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)fbUsage, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlVgpuInstanceGetLicenseStatus(nvmlVgpuInstance_t vgpuInstance, unsigned int* licensed)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)licensed, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlVgpuInstanceGetLicenseStatus) < 0 ||
        rpc_write(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, licensed, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)licensed, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlVgpuInstanceGetType(nvmlVgpuInstance_t vgpuInstance, nvmlVgpuTypeId_t* vgpuTypeId)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vgpuTypeId, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlVgpuInstanceGetType) < 0 ||
        rpc_write(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, vgpuTypeId, sizeof(nvmlVgpuTypeId_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vgpuTypeId, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlVgpuInstanceGetFrameRateLimit(nvmlVgpuInstance_t vgpuInstance, unsigned int* frameRateLimit)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)frameRateLimit, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlVgpuInstanceGetFrameRateLimit) < 0 ||
        rpc_write(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, frameRateLimit, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)frameRateLimit, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlVgpuInstanceGetEccMode(nvmlVgpuInstance_t vgpuInstance, nvmlEnableState_t* eccMode)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)eccMode, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlVgpuInstanceGetEccMode) < 0 ||
        rpc_write(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, eccMode, sizeof(nvmlEnableState_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)eccMode, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlVgpuInstanceGetEncoderCapacity(nvmlVgpuInstance_t vgpuInstance, unsigned int* encoderCapacity)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)encoderCapacity, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlVgpuInstanceGetEncoderCapacity) < 0 ||
        rpc_write(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, encoderCapacity, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)encoderCapacity, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlVgpuInstanceSetEncoderCapacity(nvmlVgpuInstance_t vgpuInstance, unsigned int encoderCapacity)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&encoderCapacity, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlVgpuInstanceSetEncoderCapacity) < 0 ||
        rpc_write(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_write(conn, &encoderCapacity, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&encoderCapacity, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlVgpuInstanceGetEncoderStats(nvmlVgpuInstance_t vgpuInstance, unsigned int* sessionCount, unsigned int* averageFps, unsigned int* averageLatency)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)sessionCount, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)averageFps, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)averageLatency, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlVgpuInstanceGetEncoderStats) < 0 ||
        rpc_write(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, sessionCount, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, averageFps, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, averageLatency, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)sessionCount, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)averageFps, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)averageLatency, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlVgpuInstanceGetEncoderSessions(nvmlVgpuInstance_t vgpuInstance, unsigned int* sessionCount, nvmlEncoderSessionInfo_t* sessionInfo)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)sessionCount, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)sessionInfo, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*sessionCount) && is_unified_pointer(conn, (void*)sessionInfo); i++)
      if (maybe_copy_unified_arg(conn, (void*)&sessionInfo[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlVgpuInstanceGetEncoderSessions) < 0 ||
        rpc_write(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_write(conn, sessionCount, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, sessionCount, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, sessionInfo, *sessionCount * sizeof(nvmlEncoderSessionInfo_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)sessionCount, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)sessionInfo, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*sessionCount) && is_unified_pointer(conn, (void*)sessionInfo); i++)
      if (maybe_copy_unified_arg(conn, (void*)&sessionInfo[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlVgpuInstanceGetFBCStats(nvmlVgpuInstance_t vgpuInstance, nvmlFBCStats_t* fbcStats)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)fbcStats, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlVgpuInstanceGetFBCStats) < 0 ||
        rpc_write(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, fbcStats, sizeof(nvmlFBCStats_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)fbcStats, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlVgpuInstanceGetFBCSessions(nvmlVgpuInstance_t vgpuInstance, unsigned int* sessionCount, nvmlFBCSessionInfo_t* sessionInfo)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)sessionCount, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)sessionInfo, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*sessionCount) && is_unified_pointer(conn, (void*)sessionInfo); i++)
      if (maybe_copy_unified_arg(conn, (void*)&sessionInfo[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlVgpuInstanceGetFBCSessions) < 0 ||
        rpc_write(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_write(conn, sessionCount, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, sessionCount, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, sessionInfo, *sessionCount * sizeof(nvmlFBCSessionInfo_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)sessionCount, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)sessionInfo, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*sessionCount) && is_unified_pointer(conn, (void*)sessionInfo); i++)
      if (maybe_copy_unified_arg(conn, (void*)&sessionInfo[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlVgpuInstanceGetGpuInstanceId(nvmlVgpuInstance_t vgpuInstance, unsigned int* gpuInstanceId)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)gpuInstanceId, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlVgpuInstanceGetGpuInstanceId) < 0 ||
        rpc_write(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, gpuInstanceId, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)gpuInstanceId, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlVgpuInstanceGetGpuPciId(nvmlVgpuInstance_t vgpuInstance, char* vgpuPciId, unsigned int* length)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)length, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vgpuPciId, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*length) && is_unified_pointer(conn, (void*)vgpuPciId); i++)
      if (maybe_copy_unified_arg(conn, (void*)&vgpuPciId[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlVgpuInstanceGetGpuPciId) < 0 ||
        rpc_write(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_write(conn, length, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, length, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, vgpuPciId, *length * sizeof(char)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)length, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vgpuPciId, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*length) && is_unified_pointer(conn, (void*)vgpuPciId); i++)
      if (maybe_copy_unified_arg(conn, (void*)&vgpuPciId[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlVgpuTypeGetCapabilities(nvmlVgpuTypeId_t vgpuTypeId, nvmlVgpuCapability_t capability, unsigned int* capResult)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeId, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&capability, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)capResult, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlVgpuTypeGetCapabilities) < 0 ||
        rpc_write(conn, &vgpuTypeId, sizeof(nvmlVgpuTypeId_t)) < 0 ||
        rpc_write(conn, &capability, sizeof(nvmlVgpuCapability_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, capResult, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuTypeId, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&capability, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)capResult, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlVgpuInstanceGetMdevUUID(nvmlVgpuInstance_t vgpuInstance, char* mdevUuid, unsigned int size)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&size, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)mdevUuid, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(size) && is_unified_pointer(conn, (void*)mdevUuid); i++)
      if (maybe_copy_unified_arg(conn, (void*)&mdevUuid[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlVgpuInstanceGetMdevUUID) < 0 ||
        rpc_write(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_write(conn, &size, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, mdevUuid, size * sizeof(char)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&size, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)mdevUuid, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(size) && is_unified_pointer(conn, (void*)mdevUuid); i++)
      if (maybe_copy_unified_arg(conn, (void*)&mdevUuid[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlVgpuInstanceGetMetadata(nvmlVgpuInstance_t vgpuInstance, nvmlVgpuMetadata_t* vgpuMetadata, unsigned int* bufferSize)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)bufferSize, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vgpuMetadata, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*bufferSize) && is_unified_pointer(conn, (void*)vgpuMetadata); i++)
      if (maybe_copy_unified_arg(conn, (void*)&vgpuMetadata[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlVgpuInstanceGetMetadata) < 0 ||
        rpc_write(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_write(conn, bufferSize, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, bufferSize, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, vgpuMetadata, *bufferSize * sizeof(nvmlVgpuMetadata_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)bufferSize, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vgpuMetadata, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*bufferSize) && is_unified_pointer(conn, (void*)vgpuMetadata); i++)
      if (maybe_copy_unified_arg(conn, (void*)&vgpuMetadata[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetVgpuMetadata(nvmlDevice_t device, nvmlVgpuPgpuMetadata_t* pgpuMetadata, unsigned int* bufferSize)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)bufferSize, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pgpuMetadata, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*bufferSize) && is_unified_pointer(conn, (void*)pgpuMetadata); i++)
      if (maybe_copy_unified_arg(conn, (void*)&pgpuMetadata[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetVgpuMetadata) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, bufferSize, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, bufferSize, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, pgpuMetadata, *bufferSize * sizeof(nvmlVgpuPgpuMetadata_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)bufferSize, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pgpuMetadata, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*bufferSize) && is_unified_pointer(conn, (void*)pgpuMetadata); i++)
      if (maybe_copy_unified_arg(conn, (void*)&pgpuMetadata[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlGetVgpuCompatibility(nvmlVgpuMetadata_t* vgpuMetadata, nvmlVgpuPgpuMetadata_t* pgpuMetadata, nvmlVgpuPgpuCompatibility_t* compatibilityInfo)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)vgpuMetadata, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pgpuMetadata, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)compatibilityInfo, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlGetVgpuCompatibility) < 0 ||
        rpc_write(conn, vgpuMetadata, sizeof(nvmlVgpuMetadata_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, vgpuMetadata, sizeof(nvmlVgpuMetadata_t)) < 0 ||
        rpc_read(conn, pgpuMetadata, sizeof(nvmlVgpuPgpuMetadata_t)) < 0 ||
        rpc_read(conn, compatibilityInfo, sizeof(nvmlVgpuPgpuCompatibility_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vgpuMetadata, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pgpuMetadata, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)compatibilityInfo, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetPgpuMetadataString(nvmlDevice_t device, char* pgpuMetadata, unsigned int* bufferSize)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)bufferSize, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pgpuMetadata, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*bufferSize) && is_unified_pointer(conn, (void*)pgpuMetadata); i++)
      if (maybe_copy_unified_arg(conn, (void*)&pgpuMetadata[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetPgpuMetadataString) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, bufferSize, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, bufferSize, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, pgpuMetadata, *bufferSize * sizeof(char)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)bufferSize, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pgpuMetadata, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*bufferSize) && is_unified_pointer(conn, (void*)pgpuMetadata); i++)
      if (maybe_copy_unified_arg(conn, (void*)&pgpuMetadata[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetVgpuSchedulerLog(nvmlDevice_t device, nvmlVgpuSchedulerLog_t* pSchedulerLog)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pSchedulerLog, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetVgpuSchedulerLog) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pSchedulerLog, sizeof(nvmlVgpuSchedulerLog_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pSchedulerLog, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetVgpuSchedulerState(nvmlDevice_t device, nvmlVgpuSchedulerGetState_t* pSchedulerState)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pSchedulerState, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetVgpuSchedulerState) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pSchedulerState, sizeof(nvmlVgpuSchedulerGetState_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pSchedulerState, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetVgpuSchedulerCapabilities(nvmlDevice_t device, nvmlVgpuSchedulerCapabilities_t* pCapabilities)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pCapabilities, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetVgpuSchedulerCapabilities) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pCapabilities, sizeof(nvmlVgpuSchedulerCapabilities_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pCapabilities, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceSetVgpuSchedulerState(nvmlDevice_t device, nvmlVgpuSchedulerSetState_t* pSchedulerState)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pSchedulerState, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceSetVgpuSchedulerState) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, pSchedulerState, sizeof(nvmlVgpuSchedulerSetState_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pSchedulerState, sizeof(nvmlVgpuSchedulerSetState_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pSchedulerState, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlGetVgpuVersion(nvmlVgpuVersion_t* supported, nvmlVgpuVersion_t* current)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)supported, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)current, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlGetVgpuVersion) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, supported, sizeof(nvmlVgpuVersion_t)) < 0 ||
        rpc_read(conn, current, sizeof(nvmlVgpuVersion_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)supported, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)current, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlSetVgpuVersion(nvmlVgpuVersion_t* vgpuVersion)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)vgpuVersion, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlSetVgpuVersion) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, vgpuVersion, sizeof(nvmlVgpuVersion_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vgpuVersion, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetVgpuUtilization(nvmlDevice_t device, unsigned long long lastSeenTimeStamp, nvmlValueType_t* sampleValType, unsigned int* vgpuInstanceSamplesCount, nvmlVgpuInstanceUtilizationSample_t* utilizationSamples)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&lastSeenTimeStamp, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)sampleValType, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vgpuInstanceSamplesCount, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)utilizationSamples, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*vgpuInstanceSamplesCount) && is_unified_pointer(conn, (void*)utilizationSamples); i++)
      if (maybe_copy_unified_arg(conn, (void*)&utilizationSamples[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetVgpuUtilization) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &lastSeenTimeStamp, sizeof(unsigned long long)) < 0 ||
        rpc_write(conn, sampleValType, sizeof(nvmlValueType_t)) < 0 ||
        rpc_write(conn, vgpuInstanceSamplesCount, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, sampleValType, sizeof(nvmlValueType_t)) < 0 ||
        rpc_read(conn, vgpuInstanceSamplesCount, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, utilizationSamples, *vgpuInstanceSamplesCount * sizeof(nvmlVgpuInstanceUtilizationSample_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&lastSeenTimeStamp, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)sampleValType, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vgpuInstanceSamplesCount, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)utilizationSamples, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*vgpuInstanceSamplesCount) && is_unified_pointer(conn, (void*)utilizationSamples); i++)
      if (maybe_copy_unified_arg(conn, (void*)&utilizationSamples[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetVgpuInstancesUtilizationInfo(nvmlDevice_t device, nvmlVgpuInstancesUtilizationInfo_t* vgpuUtilInfo)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vgpuUtilInfo, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetVgpuInstancesUtilizationInfo) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, vgpuUtilInfo, sizeof(nvmlVgpuInstancesUtilizationInfo_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, vgpuUtilInfo, sizeof(nvmlVgpuInstancesUtilizationInfo_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vgpuUtilInfo, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetVgpuProcessUtilization(nvmlDevice_t device, unsigned long long lastSeenTimeStamp, unsigned int* vgpuProcessSamplesCount, nvmlVgpuProcessUtilizationSample_t* utilizationSamples)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&lastSeenTimeStamp, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vgpuProcessSamplesCount, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)utilizationSamples, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*vgpuProcessSamplesCount) && is_unified_pointer(conn, (void*)utilizationSamples); i++)
      if (maybe_copy_unified_arg(conn, (void*)&utilizationSamples[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetVgpuProcessUtilization) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &lastSeenTimeStamp, sizeof(unsigned long long)) < 0 ||
        rpc_write(conn, vgpuProcessSamplesCount, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, vgpuProcessSamplesCount, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, utilizationSamples, *vgpuProcessSamplesCount * sizeof(nvmlVgpuProcessUtilizationSample_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&lastSeenTimeStamp, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vgpuProcessSamplesCount, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)utilizationSamples, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*vgpuProcessSamplesCount) && is_unified_pointer(conn, (void*)utilizationSamples); i++)
      if (maybe_copy_unified_arg(conn, (void*)&utilizationSamples[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetVgpuProcessesUtilizationInfo(nvmlDevice_t device, nvmlVgpuProcessesUtilizationInfo_t* vgpuProcUtilInfo)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vgpuProcUtilInfo, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetVgpuProcessesUtilizationInfo) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, vgpuProcUtilInfo, sizeof(nvmlVgpuProcessesUtilizationInfo_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, vgpuProcUtilInfo, sizeof(nvmlVgpuProcessesUtilizationInfo_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)vgpuProcUtilInfo, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlVgpuInstanceGetAccountingMode(nvmlVgpuInstance_t vgpuInstance, nvmlEnableState_t* mode)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)mode, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlVgpuInstanceGetAccountingMode) < 0 ||
        rpc_write(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, mode, sizeof(nvmlEnableState_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)mode, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlVgpuInstanceGetAccountingPids(nvmlVgpuInstance_t vgpuInstance, unsigned int* count, unsigned int* pids)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)count, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pids, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*count) && is_unified_pointer(conn, (void*)pids); i++)
      if (maybe_copy_unified_arg(conn, (void*)&pids[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlVgpuInstanceGetAccountingPids) < 0 ||
        rpc_write(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_write(conn, count, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, count, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, pids, *count * sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)count, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pids, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*count) && is_unified_pointer(conn, (void*)pids); i++)
      if (maybe_copy_unified_arg(conn, (void*)&pids[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlVgpuInstanceGetAccountingStats(nvmlVgpuInstance_t vgpuInstance, unsigned int pid, nvmlAccountingStats_t* stats)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&pid, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)stats, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlVgpuInstanceGetAccountingStats) < 0 ||
        rpc_write(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_write(conn, &pid, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, stats, sizeof(nvmlAccountingStats_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&pid, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)stats, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlVgpuInstanceClearAccountingPids(nvmlVgpuInstance_t vgpuInstance)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlVgpuInstanceClearAccountingPids) < 0 ||
        rpc_write(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlVgpuInstanceGetLicenseInfo_v2(nvmlVgpuInstance_t vgpuInstance, nvmlVgpuLicenseInfo_t* licenseInfo)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)licenseInfo, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlVgpuInstanceGetLicenseInfo_v2) < 0 ||
        rpc_write(conn, &vgpuInstance, sizeof(nvmlVgpuInstance_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, licenseInfo, sizeof(nvmlVgpuLicenseInfo_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&vgpuInstance, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)licenseInfo, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlGetExcludedDeviceCount(unsigned int* deviceCount)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)deviceCount, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlGetExcludedDeviceCount) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, deviceCount, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)deviceCount, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlGetExcludedDeviceInfoByIndex(unsigned int index, nvmlExcludedDeviceInfo_t* info)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&index, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)info, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlGetExcludedDeviceInfoByIndex) < 0 ||
        rpc_write(conn, &index, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, info, sizeof(nvmlExcludedDeviceInfo_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&index, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)info, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceSetMigMode(nvmlDevice_t device, unsigned int mode, nvmlReturn_t* activationStatus)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&mode, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)activationStatus, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceSetMigMode) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &mode, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, activationStatus, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&mode, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)activationStatus, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetMigMode(nvmlDevice_t device, unsigned int* currentMode, unsigned int* pendingMode)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)currentMode, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pendingMode, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetMigMode) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, currentMode, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, pendingMode, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)currentMode, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)pendingMode, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetGpuInstanceProfileInfo(nvmlDevice_t device, unsigned int profile, nvmlGpuInstanceProfileInfo_t* info)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&profile, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)info, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetGpuInstanceProfileInfo) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &profile, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, info, sizeof(nvmlGpuInstanceProfileInfo_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&profile, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)info, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetGpuInstanceProfileInfoV(nvmlDevice_t device, unsigned int profile, nvmlGpuInstanceProfileInfo_v2_t* info)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&profile, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)info, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetGpuInstanceProfileInfoV) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &profile, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, info, sizeof(nvmlGpuInstanceProfileInfo_v2_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&profile, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)info, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetGpuInstancePossiblePlacements_v2(nvmlDevice_t device, unsigned int profileId, nvmlGpuInstancePlacement_t* placements, unsigned int* count)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&profileId, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)count, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)placements, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*count) && is_unified_pointer(conn, (void*)placements); i++)
      if (maybe_copy_unified_arg(conn, (void*)&placements[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetGpuInstancePossiblePlacements_v2) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &profileId, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, count, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, count, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, placements, *count * sizeof(nvmlGpuInstancePlacement_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&profileId, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)count, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)placements, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*count) && is_unified_pointer(conn, (void*)placements); i++)
      if (maybe_copy_unified_arg(conn, (void*)&placements[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetGpuInstanceRemainingCapacity(nvmlDevice_t device, unsigned int profileId, unsigned int* count)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&profileId, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)count, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetGpuInstanceRemainingCapacity) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &profileId, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, count, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&profileId, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)count, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceCreateGpuInstance(nvmlDevice_t device, unsigned int profileId, nvmlGpuInstance_t* gpuInstance)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&profileId, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)gpuInstance, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceCreateGpuInstance) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &profileId, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, gpuInstance, sizeof(nvmlGpuInstance_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&profileId, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)gpuInstance, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlGpuInstanceDestroy(nvmlGpuInstance_t gpuInstance)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&gpuInstance, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlGpuInstanceDestroy) < 0 ||
        rpc_write(conn, &gpuInstance, sizeof(nvmlGpuInstance_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&gpuInstance, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetGpuInstances(nvmlDevice_t device, unsigned int profileId, nvmlGpuInstance_t* gpuInstances, unsigned int* count)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&profileId, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)count, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)gpuInstances, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*count) && is_unified_pointer(conn, (void*)gpuInstances); i++)
      if (maybe_copy_unified_arg(conn, (void*)&gpuInstances[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetGpuInstances) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &profileId, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, count, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, count, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, gpuInstances, *count * sizeof(nvmlGpuInstance_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&profileId, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)count, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)gpuInstances, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*count) && is_unified_pointer(conn, (void*)gpuInstances); i++)
      if (maybe_copy_unified_arg(conn, (void*)&gpuInstances[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetGpuInstanceById(nvmlDevice_t device, unsigned int id, nvmlGpuInstance_t* gpuInstance)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&id, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)gpuInstance, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetGpuInstanceById) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &id, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, gpuInstance, sizeof(nvmlGpuInstance_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&id, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)gpuInstance, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlGpuInstanceGetInfo(nvmlGpuInstance_t gpuInstance, nvmlGpuInstanceInfo_t* info)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&gpuInstance, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)info, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlGpuInstanceGetInfo) < 0 ||
        rpc_write(conn, &gpuInstance, sizeof(nvmlGpuInstance_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, info, sizeof(nvmlGpuInstanceInfo_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&gpuInstance, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)info, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlGpuInstanceGetComputeInstanceProfileInfo(nvmlGpuInstance_t gpuInstance, unsigned int profile, unsigned int engProfile, nvmlComputeInstanceProfileInfo_t* info)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&gpuInstance, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&profile, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&engProfile, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)info, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlGpuInstanceGetComputeInstanceProfileInfo) < 0 ||
        rpc_write(conn, &gpuInstance, sizeof(nvmlGpuInstance_t)) < 0 ||
        rpc_write(conn, &profile, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &engProfile, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, info, sizeof(nvmlComputeInstanceProfileInfo_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&gpuInstance, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&profile, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&engProfile, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)info, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlGpuInstanceGetComputeInstanceProfileInfoV(nvmlGpuInstance_t gpuInstance, unsigned int profile, unsigned int engProfile, nvmlComputeInstanceProfileInfo_v2_t* info)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&gpuInstance, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&profile, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&engProfile, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)info, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlGpuInstanceGetComputeInstanceProfileInfoV) < 0 ||
        rpc_write(conn, &gpuInstance, sizeof(nvmlGpuInstance_t)) < 0 ||
        rpc_write(conn, &profile, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &engProfile, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, info, sizeof(nvmlComputeInstanceProfileInfo_v2_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&gpuInstance, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&profile, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&engProfile, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)info, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlGpuInstanceGetComputeInstanceRemainingCapacity(nvmlGpuInstance_t gpuInstance, unsigned int profileId, unsigned int* count)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&gpuInstance, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&profileId, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)count, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlGpuInstanceGetComputeInstanceRemainingCapacity) < 0 ||
        rpc_write(conn, &gpuInstance, sizeof(nvmlGpuInstance_t)) < 0 ||
        rpc_write(conn, &profileId, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, count, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&gpuInstance, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&profileId, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)count, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlGpuInstanceGetComputeInstancePossiblePlacements(nvmlGpuInstance_t gpuInstance, unsigned int profileId, nvmlComputeInstancePlacement_t* placements, unsigned int* count)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&gpuInstance, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&profileId, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)count, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)placements, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*count) && is_unified_pointer(conn, (void*)placements); i++)
      if (maybe_copy_unified_arg(conn, (void*)&placements[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlGpuInstanceGetComputeInstancePossiblePlacements) < 0 ||
        rpc_write(conn, &gpuInstance, sizeof(nvmlGpuInstance_t)) < 0 ||
        rpc_write(conn, &profileId, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, count, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, count, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, placements, *count * sizeof(nvmlComputeInstancePlacement_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&gpuInstance, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&profileId, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)count, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)placements, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*count) && is_unified_pointer(conn, (void*)placements); i++)
      if (maybe_copy_unified_arg(conn, (void*)&placements[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlGpuInstanceCreateComputeInstance(nvmlGpuInstance_t gpuInstance, unsigned int profileId, nvmlComputeInstance_t* computeInstance)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&gpuInstance, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&profileId, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)computeInstance, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlGpuInstanceCreateComputeInstance) < 0 ||
        rpc_write(conn, &gpuInstance, sizeof(nvmlGpuInstance_t)) < 0 ||
        rpc_write(conn, &profileId, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, computeInstance, sizeof(nvmlComputeInstance_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&gpuInstance, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&profileId, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)computeInstance, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlComputeInstanceDestroy(nvmlComputeInstance_t computeInstance)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&computeInstance, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlComputeInstanceDestroy) < 0 ||
        rpc_write(conn, &computeInstance, sizeof(nvmlComputeInstance_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&computeInstance, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlGpuInstanceGetComputeInstances(nvmlGpuInstance_t gpuInstance, unsigned int profileId, nvmlComputeInstance_t* computeInstances, unsigned int* count)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&gpuInstance, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&profileId, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)count, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)computeInstances, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*count) && is_unified_pointer(conn, (void*)computeInstances); i++)
      if (maybe_copy_unified_arg(conn, (void*)&computeInstances[i], cudaMemcpyHostToDevice) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlGpuInstanceGetComputeInstances) < 0 ||
        rpc_write(conn, &gpuInstance, sizeof(nvmlGpuInstance_t)) < 0 ||
        rpc_write(conn, &profileId, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, count, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, count, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, computeInstances, *count * sizeof(nvmlComputeInstance_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&gpuInstance, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&profileId, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)count, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)computeInstances, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    for (int i = 0; i < static_cast<int>(*count) && is_unified_pointer(conn, (void*)computeInstances); i++)
      if (maybe_copy_unified_arg(conn, (void*)&computeInstances[i], cudaMemcpyDeviceToHost) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlGpuInstanceGetComputeInstanceById(nvmlGpuInstance_t gpuInstance, unsigned int id, nvmlComputeInstance_t* computeInstance)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&gpuInstance, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&id, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)computeInstance, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlGpuInstanceGetComputeInstanceById) < 0 ||
        rpc_write(conn, &gpuInstance, sizeof(nvmlGpuInstance_t)) < 0 ||
        rpc_write(conn, &id, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, computeInstance, sizeof(nvmlComputeInstance_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&gpuInstance, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&id, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)computeInstance, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlComputeInstanceGetInfo_v2(nvmlComputeInstance_t computeInstance, nvmlComputeInstanceInfo_t* info)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&computeInstance, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)info, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlComputeInstanceGetInfo_v2) < 0 ||
        rpc_write(conn, &computeInstance, sizeof(nvmlComputeInstance_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, info, sizeof(nvmlComputeInstanceInfo_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&computeInstance, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)info, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceIsMigDeviceHandle(nvmlDevice_t device, unsigned int* isMigDevice)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)isMigDevice, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceIsMigDeviceHandle) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, isMigDevice, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)isMigDevice, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetGpuInstanceId(nvmlDevice_t device, unsigned int* id)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)id, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetGpuInstanceId) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, id, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)id, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetComputeInstanceId(nvmlDevice_t device, unsigned int* id)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)id, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetComputeInstanceId) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, id, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)id, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetMaxMigDeviceCount(nvmlDevice_t device, unsigned int* count)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)count, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetMaxMigDeviceCount) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, count, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)count, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetMigDeviceHandleByIndex(nvmlDevice_t device, unsigned int index, nvmlDevice_t* migDevice)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&index, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)migDevice, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetMigDeviceHandleByIndex) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &index, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, migDevice, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&index, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)migDevice, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetDeviceHandleFromMigDeviceHandle(nvmlDevice_t migDevice, nvmlDevice_t* device)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&migDevice, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetDeviceHandleFromMigDeviceHandle) < 0 ||
        rpc_write(conn, &migDevice, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&migDevice, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlGpmMetricsGet(nvmlGpmMetricsGet_t* metricsGet)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)metricsGet, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlGpmMetricsGet) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, metricsGet, sizeof(nvmlGpmMetricsGet_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)metricsGet, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlGpmSampleFree(nvmlGpmSample_t gpmSample)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&gpmSample, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlGpmSampleFree) < 0 ||
        rpc_write(conn, &gpmSample, sizeof(nvmlGpmSample_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&gpmSample, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlGpmSampleAlloc(nvmlGpmSample_t* gpmSample)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)gpmSample, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlGpmSampleAlloc) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, gpmSample, sizeof(nvmlGpmSample_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)gpmSample, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlGpmSampleGet(nvmlDevice_t device, nvmlGpmSample_t gpmSample)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&gpmSample, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlGpmSampleGet) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &gpmSample, sizeof(nvmlGpmSample_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&gpmSample, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlGpmMigSampleGet(nvmlDevice_t device, unsigned int gpuInstanceId, nvmlGpmSample_t gpmSample)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&gpuInstanceId, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&gpmSample, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlGpmMigSampleGet) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &gpuInstanceId, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &gpmSample, sizeof(nvmlGpmSample_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&gpuInstanceId, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&gpmSample, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlGpmQueryDeviceSupport(nvmlDevice_t device, nvmlGpmSupport_t* gpmSupport)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)gpmSupport, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlGpmQueryDeviceSupport) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, gpmSupport, sizeof(nvmlGpmSupport_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)gpmSupport, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlGpmQueryIfStreamingEnabled(nvmlDevice_t device, unsigned int* state)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)state, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlGpmQueryIfStreamingEnabled) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, state, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, state, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)state, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlGpmSetStreamingEnabled(nvmlDevice_t device, unsigned int state)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&state, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlGpmSetStreamingEnabled) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, &state, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&state, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceSetNvLinkDeviceLowPowerThreshold(nvmlDevice_t device, nvmlNvLinkPowerThres_t* info)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)info, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceSetNvLinkDeviceLowPowerThreshold) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, info, sizeof(nvmlNvLinkPowerThres_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)info, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlSystemSetNvlinkBwMode(unsigned int nvlinkBwMode)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&nvlinkBwMode, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlSystemSetNvlinkBwMode) < 0 ||
        rpc_write(conn, &nvlinkBwMode, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&nvlinkBwMode, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlSystemGetNvlinkBwMode(unsigned int* nvlinkBwMode)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)nvlinkBwMode, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlSystemGetNvlinkBwMode) < 0 ||
        rpc_write(conn, nvlinkBwMode, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, nvlinkBwMode, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)nvlinkBwMode, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceSetPowerManagementLimit_v2(nvmlDevice_t device, nvmlPowerValue_v2_t* powerValue)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)powerValue, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceSetPowerManagementLimit_v2) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, powerValue, sizeof(nvmlPowerValue_v2_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, powerValue, sizeof(nvmlPowerValue_v2_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)powerValue, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

nvmlReturn_t nvmlDeviceGetSramEccErrorStatus(nvmlDevice_t device, nvmlEccSramErrorStatus_t* status)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)status, cudaMemcpyHostToDevice) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    nvmlReturn_t return_value;
    if (rpc_write_start_request(conn, RPC_nvmlDeviceGetSramEccErrorStatus) < 0 ||
        rpc_write(conn, &device, sizeof(nvmlDevice_t)) < 0 ||
        rpc_write(conn, status, sizeof(nvmlEccSramErrorStatus_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, status, sizeof(nvmlEccSramErrorStatus_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(nvmlReturn_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    if (maybe_copy_unified_arg(conn, (void*)status, cudaMemcpyDeviceToHost) < 0)
      return NVML_ERROR_GPU_IS_LOST;
    return return_value;
}

CUresult cuInit(unsigned int Flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&Flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuInit) < 0 ||
        rpc_write(conn, &Flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuDriverGetVersion(int* driverVersion)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)driverVersion, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuDriverGetVersion) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, driverVersion, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)driverVersion, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuDeviceGet(CUdevice* device, int ordinal)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)device, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ordinal, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuDeviceGet) < 0 ||
        rpc_write(conn, &ordinal, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, device, sizeof(CUdevice)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)device, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ordinal, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuDeviceGetCount(int* count)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)count, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuDeviceGetCount) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, count, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)count, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuDeviceGetName(char* name, int len, CUdevice dev)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&len, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)name, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    for (int i = 0; i < static_cast<int>(len) && is_unified_pointer(conn, (void*)name); i++)
      if (maybe_copy_unified_arg(conn, (void*)&name[i], cudaMemcpyHostToDevice) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuDeviceGetName) < 0 ||
        rpc_write(conn, &len, sizeof(int)) < 0 ||
        rpc_write(conn, &dev, sizeof(CUdevice)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, name, len * sizeof(char)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&len, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)name, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    for (int i = 0; i < static_cast<int>(len) && is_unified_pointer(conn, (void*)name); i++)
      if (maybe_copy_unified_arg(conn, (void*)&name[i], cudaMemcpyDeviceToHost) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuDeviceGetUuid(CUuuid* uuid, CUdevice dev)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)uuid, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    for (int i = 0; i < 16 && is_unified_pointer(conn, (void*)uuid); i++)
      if (maybe_copy_unified_arg(conn, (void*)&uuid[i], cudaMemcpyHostToDevice) < 0 )
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuDeviceGetUuid) < 0 ||
        rpc_write(conn, &dev, sizeof(CUdevice)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, uuid, 16) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)uuid, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    for (int i = 0; i < 16 && is_unified_pointer(conn, (void*)uuid); i++)
      if (maybe_copy_unified_arg(conn, (void*)&uuid[i], cudaMemcpyDeviceToHost) < 0 )
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuDeviceGetUuid_v2(CUuuid* uuid, CUdevice dev)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)uuid, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    for (int i = 0; i < 16 && is_unified_pointer(conn, (void*)uuid); i++)
      if (maybe_copy_unified_arg(conn, (void*)&uuid[i], cudaMemcpyHostToDevice) < 0 )
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuDeviceGetUuid_v2) < 0 ||
        rpc_write(conn, &dev, sizeof(CUdevice)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, uuid, 16) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)uuid, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    for (int i = 0; i < 16 && is_unified_pointer(conn, (void*)uuid); i++)
      if (maybe_copy_unified_arg(conn, (void*)&uuid[i], cudaMemcpyDeviceToHost) < 0 )
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuDeviceGetLuid(char* luid, unsigned int* deviceNodeMask, CUdevice dev)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)luid, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)deviceNodeMask, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    std::size_t luid_len;
    if (rpc_write_start_request(conn, RPC_cuDeviceGetLuid) < 0 ||
        rpc_write(conn, &dev, sizeof(CUdevice)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &luid_len, sizeof(std::size_t)) < 0 ||
        rpc_read(conn, luid, luid_len) < 0 ||
        rpc_read(conn, deviceNodeMask, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)luid, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)deviceNodeMask, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuDeviceTotalMem_v2(size_t* bytes, CUdevice dev)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)bytes, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuDeviceTotalMem_v2) < 0 ||
        rpc_write(conn, &dev, sizeof(CUdevice)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, bytes, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)bytes, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuDeviceGetTexture1DLinearMaxWidth(size_t* maxWidthInElements, CUarray_format format, unsigned numChannels, CUdevice dev)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)maxWidthInElements, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&format, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numChannels, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuDeviceGetTexture1DLinearMaxWidth) < 0 ||
        rpc_write(conn, &format, sizeof(CUarray_format)) < 0 ||
        rpc_write(conn, &numChannels, sizeof(unsigned)) < 0 ||
        rpc_write(conn, &dev, sizeof(CUdevice)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, maxWidthInElements, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)maxWidthInElements, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&format, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numChannels, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuDeviceGetAttribute(int* pi, CUdevice_attribute attrib, CUdevice dev)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pi, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&attrib, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuDeviceGetAttribute) < 0 ||
        rpc_write(conn, &attrib, sizeof(CUdevice_attribute)) < 0 ||
        rpc_write(conn, &dev, sizeof(CUdevice)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pi, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pi, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&attrib, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuDeviceSetMemPool(CUdevice dev, CUmemoryPool pool)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&pool, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuDeviceSetMemPool) < 0 ||
        rpc_write(conn, &dev, sizeof(CUdevice)) < 0 ||
        rpc_write(conn, &pool, sizeof(CUmemoryPool)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&pool, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuDeviceGetMemPool(CUmemoryPool* pool, CUdevice dev)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pool, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuDeviceGetMemPool) < 0 ||
        rpc_write(conn, &dev, sizeof(CUdevice)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pool, sizeof(CUmemoryPool)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pool, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuDeviceGetDefaultMemPool(CUmemoryPool* pool_out, CUdevice dev)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pool_out, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuDeviceGetDefaultMemPool) < 0 ||
        rpc_write(conn, &dev, sizeof(CUdevice)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pool_out, sizeof(CUmemoryPool)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pool_out, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuDeviceGetExecAffinitySupport(int* pi, CUexecAffinityType type, CUdevice dev)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pi, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&type, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuDeviceGetExecAffinitySupport) < 0 ||
        rpc_write(conn, &type, sizeof(CUexecAffinityType)) < 0 ||
        rpc_write(conn, &dev, sizeof(CUdevice)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pi, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pi, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&type, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuFlushGPUDirectRDMAWrites(CUflushGPUDirectRDMAWritesTarget target, CUflushGPUDirectRDMAWritesScope scope)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&target, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&scope, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuFlushGPUDirectRDMAWrites) < 0 ||
        rpc_write(conn, &target, sizeof(CUflushGPUDirectRDMAWritesTarget)) < 0 ||
        rpc_write(conn, &scope, sizeof(CUflushGPUDirectRDMAWritesScope)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&target, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&scope, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuDeviceGetProperties(CUdevprop* prop, CUdevice dev)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)prop, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuDeviceGetProperties) < 0 ||
        rpc_write(conn, &dev, sizeof(CUdevice)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, prop, sizeof(CUdevprop)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)prop, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuDeviceComputeCapability(int* major, int* minor, CUdevice dev)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)major, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)minor, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuDeviceComputeCapability) < 0 ||
        rpc_write(conn, &dev, sizeof(CUdevice)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, major, sizeof(int)) < 0 ||
        rpc_read(conn, minor, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)major, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)minor, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuDevicePrimaryCtxRetain(CUcontext* pctx, CUdevice dev)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pctx, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuDevicePrimaryCtxRetain) < 0 ||
        rpc_write(conn, &dev, sizeof(CUdevice)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pctx, sizeof(CUcontext)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pctx, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuDevicePrimaryCtxRelease_v2(CUdevice dev)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuDevicePrimaryCtxRelease_v2) < 0 ||
        rpc_write(conn, &dev, sizeof(CUdevice)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuDevicePrimaryCtxSetFlags_v2(CUdevice dev, unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuDevicePrimaryCtxSetFlags_v2) < 0 ||
        rpc_write(conn, &dev, sizeof(CUdevice)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuDevicePrimaryCtxGetState(CUdevice dev, unsigned int* flags, int* active)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)active, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuDevicePrimaryCtxGetState) < 0 ||
        rpc_write(conn, &dev, sizeof(CUdevice)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, flags, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, active, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)active, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuDevicePrimaryCtxReset_v2(CUdevice dev)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuDevicePrimaryCtxReset_v2) < 0 ||
        rpc_write(conn, &dev, sizeof(CUdevice)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuCtxCreate_v2(CUcontext* pctx, unsigned int flags, CUdevice dev)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pctx, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuCtxCreate_v2) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &dev, sizeof(CUdevice)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pctx, sizeof(CUcontext)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pctx, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuCtxCreate_v3(CUcontext* pctx, CUexecAffinityParam* paramsArray, int numParams, unsigned int flags, CUdevice dev)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pctx, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numParams, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)paramsArray, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    for (int i = 0; i < static_cast<int>(numParams) && is_unified_pointer(conn, (void*)paramsArray); i++)
      if (maybe_copy_unified_arg(conn, (void*)&paramsArray[i], cudaMemcpyHostToDevice) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuCtxCreate_v3) < 0 ||
        rpc_write(conn, &numParams, sizeof(int)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &dev, sizeof(CUdevice)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pctx, sizeof(CUcontext)) < 0 ||
        rpc_read(conn, paramsArray, numParams * sizeof(CUexecAffinityParam)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pctx, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numParams, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)paramsArray, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    for (int i = 0; i < static_cast<int>(numParams) && is_unified_pointer(conn, (void*)paramsArray); i++)
      if (maybe_copy_unified_arg(conn, (void*)&paramsArray[i], cudaMemcpyDeviceToHost) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuCtxDestroy_v2(CUcontext ctx)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&ctx, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuCtxDestroy_v2) < 0 ||
        rpc_write(conn, &ctx, sizeof(CUcontext)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ctx, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuCtxPushCurrent_v2(CUcontext ctx)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&ctx, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuCtxPushCurrent_v2) < 0 ||
        rpc_write(conn, &ctx, sizeof(CUcontext)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ctx, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuCtxPopCurrent_v2(CUcontext* pctx)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pctx, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuCtxPopCurrent_v2) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pctx, sizeof(CUcontext)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pctx, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuCtxSetCurrent(CUcontext ctx)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&ctx, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuCtxSetCurrent) < 0 ||
        rpc_write(conn, &ctx, sizeof(CUcontext)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ctx, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuCtxGetCurrent(CUcontext* pctx)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pctx, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuCtxGetCurrent) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pctx, sizeof(CUcontext)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pctx, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuCtxGetDevice(CUdevice* device)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)device, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuCtxGetDevice) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, device, sizeof(CUdevice)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)device, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuCtxGetFlags(unsigned int* flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuCtxGetFlags) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, flags, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuCtxSetFlags(unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuCtxSetFlags) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuCtxGetId(CUcontext ctx, unsigned long long* ctxId)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&ctx, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)ctxId, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuCtxGetId) < 0 ||
        rpc_write(conn, &ctx, sizeof(CUcontext)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, ctxId, sizeof(unsigned long long)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ctx, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)ctxId, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuCtxSynchronize()
{
    conn_t *conn = rpc_client_get_connection(0);
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuCtxSynchronize) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuCtxSetLimit(CUlimit limit, size_t value)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&limit, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&value, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuCtxSetLimit) < 0 ||
        rpc_write(conn, &limit, sizeof(CUlimit)) < 0 ||
        rpc_write(conn, &value, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&limit, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&value, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuCtxGetLimit(size_t* pvalue, CUlimit limit)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pvalue, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&limit, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuCtxGetLimit) < 0 ||
        rpc_write(conn, &limit, sizeof(CUlimit)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pvalue, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pvalue, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&limit, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuCtxGetCacheConfig(CUfunc_cache* pconfig)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pconfig, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuCtxGetCacheConfig) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pconfig, sizeof(CUfunc_cache)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pconfig, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuCtxSetCacheConfig(CUfunc_cache config)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&config, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuCtxSetCacheConfig) < 0 ||
        rpc_write(conn, &config, sizeof(CUfunc_cache)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&config, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuCtxGetApiVersion(CUcontext ctx, unsigned int* version)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&ctx, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)version, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuCtxGetApiVersion) < 0 ||
        rpc_write(conn, &ctx, sizeof(CUcontext)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, version, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ctx, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)version, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuCtxGetStreamPriorityRange(int* leastPriority, int* greatestPriority)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)leastPriority, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)greatestPriority, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuCtxGetStreamPriorityRange) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, leastPriority, sizeof(int)) < 0 ||
        rpc_read(conn, greatestPriority, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)leastPriority, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)greatestPriority, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuCtxResetPersistingL2Cache()
{
    conn_t *conn = rpc_client_get_connection(0);
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuCtxResetPersistingL2Cache) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuCtxGetExecAffinity(CUexecAffinityParam* pExecAffinity, CUexecAffinityType type)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pExecAffinity, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&type, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuCtxGetExecAffinity) < 0 ||
        rpc_write(conn, &type, sizeof(CUexecAffinityType)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pExecAffinity, sizeof(CUexecAffinityParam)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pExecAffinity, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&type, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuCtxAttach(CUcontext* pctx, unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pctx, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuCtxAttach) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pctx, sizeof(CUcontext)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pctx, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuCtxDetach(CUcontext ctx)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&ctx, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuCtxDetach) < 0 ||
        rpc_write(conn, &ctx, sizeof(CUcontext)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ctx, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuCtxGetSharedMemConfig(CUsharedconfig* pConfig)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pConfig, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuCtxGetSharedMemConfig) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pConfig, sizeof(CUsharedconfig)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pConfig, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuCtxSetSharedMemConfig(CUsharedconfig config)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&config, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuCtxSetSharedMemConfig) < 0 ||
        rpc_write(conn, &config, sizeof(CUsharedconfig)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&config, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuModuleLoad(CUmodule* module, const char* fname)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)module, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)fname, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    std::size_t fname_len = std::strlen(fname) + 1;
    if (rpc_write_start_request(conn, RPC_cuModuleLoad) < 0 ||
        rpc_write(conn, &fname_len, sizeof(std::size_t)) < 0 ||
        rpc_write(conn, fname, fname_len) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, module, sizeof(CUmodule)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)module, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)fname, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuModuleUnload(CUmodule hmod)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hmod, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuModuleUnload) < 0 ||
        rpc_write(conn, &hmod, sizeof(CUmodule)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hmod, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuModuleGetLoadingMode(CUmoduleLoadingMode* mode)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)mode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuModuleGetLoadingMode) < 0 ||
        rpc_write(conn, mode, sizeof(CUmoduleLoadingMode)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, mode, sizeof(CUmoduleLoadingMode)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)mode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuModuleGetFunction(CUfunction* hfunc, CUmodule hmod, const char* name)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)hfunc, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hmod, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)name, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    std::size_t name_len = std::strlen(name) + 1;
    if (rpc_write_start_request(conn, RPC_cuModuleGetFunction) < 0 ||
        rpc_write(conn, &hmod, sizeof(CUmodule)) < 0 ||
        rpc_write(conn, &name_len, sizeof(std::size_t)) < 0 ||
        rpc_write(conn, name, name_len) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, hfunc, sizeof(CUfunction)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)hfunc, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hmod, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)name, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuModuleGetFunctionCount(unsigned int* count, CUmodule mod)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)count, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&mod, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuModuleGetFunctionCount) < 0 ||
        rpc_write(conn, count, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &mod, sizeof(CUmodule)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, count, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)count, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&mod, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuModuleEnumerateFunctions(CUfunction* functions, unsigned int numFunctions, CUmodule mod)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)functions, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numFunctions, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&mod, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuModuleEnumerateFunctions) < 0 ||
        rpc_write(conn, functions, sizeof(CUfunction)) < 0 ||
        rpc_write(conn, &numFunctions, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &mod, sizeof(CUmodule)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, functions, sizeof(CUfunction)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)functions, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numFunctions, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&mod, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuModuleGetGlobal_v2(CUdeviceptr* dptr, size_t* bytes, CUmodule hmod, const char* name)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)dptr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)bytes, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hmod, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)name, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    std::size_t name_len = std::strlen(name) + 1;
    if (rpc_write_start_request(conn, RPC_cuModuleGetGlobal_v2) < 0 ||
        rpc_write(conn, &hmod, sizeof(CUmodule)) < 0 ||
        rpc_write(conn, &name_len, sizeof(std::size_t)) < 0 ||
        rpc_write(conn, name, name_len) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, dptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, bytes, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dptr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)bytes, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hmod, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)name, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuLinkCreate_v2(unsigned int numOptions, CUjit_option* options, void** optionValues, CUlinkState* stateOut)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&numOptions, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)options, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)optionValues, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)stateOut, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuLinkCreate_v2) < 0 ||
        rpc_write(conn, &numOptions, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, options, sizeof(CUjit_option)) < 0 ||
        rpc_write(conn, optionValues, sizeof(void*)) < 0 ||
        rpc_write(conn, stateOut, sizeof(CUlinkState)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, options, sizeof(CUjit_option)) < 0 ||
        rpc_read(conn, optionValues, sizeof(void*)) < 0 ||
        rpc_read(conn, stateOut, sizeof(CUlinkState)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numOptions, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)options, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)optionValues, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)stateOut, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuLinkAddFile_v2(CUlinkState state, CUjitInputType type, const char* path, unsigned int numOptions, CUjit_option* options, void** optionValues)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&state, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&type, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)path, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numOptions, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)options, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    for (int i = 0; i < static_cast<int>(numOptions) && is_unified_pointer(conn, (void*)options); i++)
      if (maybe_copy_unified_arg(conn, (void*)&options[i], cudaMemcpyHostToDevice) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)optionValues, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    for (int i = 0; i < static_cast<int>(numOptions) && is_unified_pointer(conn, (void*)optionValues); i++)
      if (maybe_copy_unified_arg(conn, (void*)&optionValues[i], cudaMemcpyHostToDevice) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    std::size_t path_len = std::strlen(path) + 1;
    if (rpc_write_start_request(conn, RPC_cuLinkAddFile_v2) < 0 ||
        rpc_write(conn, &state, sizeof(CUlinkState)) < 0 ||
        rpc_write(conn, &type, sizeof(CUjitInputType)) < 0 ||
        rpc_write(conn, &path_len, sizeof(std::size_t)) < 0 ||
        rpc_write(conn, path, path_len) < 0 ||
        rpc_write(conn, &numOptions, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, options, numOptions * sizeof(CUjit_option)) < 0 ||
        rpc_write(conn, optionValues, numOptions) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&state, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&type, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)path, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numOptions, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)options, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    for (int i = 0; i < static_cast<int>(numOptions) && is_unified_pointer(conn, (void*)options); i++)
      if (maybe_copy_unified_arg(conn, (void*)&options[i], cudaMemcpyDeviceToHost) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)optionValues, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    for (int i = 0; i < static_cast<int>(numOptions) && is_unified_pointer(conn, (void*)optionValues); i++)
      if (maybe_copy_unified_arg(conn, (void*)&optionValues[i], cudaMemcpyDeviceToHost) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuLinkComplete(CUlinkState state, void** cubinOut, size_t* sizeOut)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&state, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)cubinOut, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)sizeOut, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuLinkComplete) < 0 ||
        rpc_write(conn, &state, sizeof(CUlinkState)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, cubinOut, sizeof(void*)) < 0 ||
        rpc_read(conn, sizeOut, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&state, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)cubinOut, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)sizeOut, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuLinkDestroy(CUlinkState state)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&state, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuLinkDestroy) < 0 ||
        rpc_write(conn, &state, sizeof(CUlinkState)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&state, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuModuleGetTexRef(CUtexref* pTexRef, CUmodule hmod, const char* name)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pTexRef, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hmod, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)name, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    std::size_t name_len = std::strlen(name) + 1;
    if (rpc_write_start_request(conn, RPC_cuModuleGetTexRef) < 0 ||
        rpc_write(conn, &hmod, sizeof(CUmodule)) < 0 ||
        rpc_write(conn, &name_len, sizeof(std::size_t)) < 0 ||
        rpc_write(conn, name, name_len) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pTexRef, sizeof(CUtexref)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pTexRef, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hmod, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)name, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuModuleGetSurfRef(CUsurfref* pSurfRef, CUmodule hmod, const char* name)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pSurfRef, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hmod, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)name, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    std::size_t name_len = std::strlen(name) + 1;
    if (rpc_write_start_request(conn, RPC_cuModuleGetSurfRef) < 0 ||
        rpc_write(conn, &hmod, sizeof(CUmodule)) < 0 ||
        rpc_write(conn, &name_len, sizeof(std::size_t)) < 0 ||
        rpc_write(conn, name, name_len) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pSurfRef, sizeof(CUsurfref)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pSurfRef, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hmod, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)name, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuLibraryLoadFromFile(CUlibrary* library, const char* fileName, CUjit_option* jitOptions, void** jitOptionsValues, unsigned int numJitOptions, CUlibraryOption* libraryOptions, void** libraryOptionValues, unsigned int numLibraryOptions)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)library, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)fileName, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numJitOptions, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)jitOptions, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    for (int i = 0; i < static_cast<int>(numJitOptions) && is_unified_pointer(conn, (void*)jitOptions); i++)
      if (maybe_copy_unified_arg(conn, (void*)&jitOptions[i], cudaMemcpyHostToDevice) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)jitOptionsValues, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    for (int i = 0; i < static_cast<int>(numJitOptions) && is_unified_pointer(conn, (void*)jitOptionsValues); i++)
      if (maybe_copy_unified_arg(conn, (void*)&jitOptionsValues[i], cudaMemcpyHostToDevice) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numLibraryOptions, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)libraryOptions, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    for (int i = 0; i < static_cast<int>(numLibraryOptions) && is_unified_pointer(conn, (void*)libraryOptions); i++)
      if (maybe_copy_unified_arg(conn, (void*)&libraryOptions[i], cudaMemcpyHostToDevice) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)libraryOptionValues, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    for (int i = 0; i < static_cast<int>(numLibraryOptions) && is_unified_pointer(conn, (void*)libraryOptionValues); i++)
      if (maybe_copy_unified_arg(conn, (void*)&libraryOptionValues[i], cudaMemcpyHostToDevice) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    std::size_t fileName_len = std::strlen(fileName) + 1;
    if (rpc_write_start_request(conn, RPC_cuLibraryLoadFromFile) < 0 ||
        rpc_write(conn, &fileName_len, sizeof(std::size_t)) < 0 ||
        rpc_write(conn, fileName, fileName_len) < 0 ||
        rpc_write(conn, &numJitOptions, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, jitOptions, numJitOptions * sizeof(CUjit_option)) < 0 ||
        rpc_write(conn, jitOptionsValues, numJitOptions) < 0 ||
        rpc_write(conn, &numLibraryOptions, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, libraryOptions, numLibraryOptions * sizeof(CUlibraryOption)) < 0 ||
        rpc_write(conn, libraryOptionValues, numLibraryOptions) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, library, sizeof(CUlibrary)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)library, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)fileName, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numJitOptions, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)jitOptions, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    for (int i = 0; i < static_cast<int>(numJitOptions) && is_unified_pointer(conn, (void*)jitOptions); i++)
      if (maybe_copy_unified_arg(conn, (void*)&jitOptions[i], cudaMemcpyDeviceToHost) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)jitOptionsValues, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    for (int i = 0; i < static_cast<int>(numJitOptions) && is_unified_pointer(conn, (void*)jitOptionsValues); i++)
      if (maybe_copy_unified_arg(conn, (void*)&jitOptionsValues[i], cudaMemcpyDeviceToHost) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numLibraryOptions, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)libraryOptions, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    for (int i = 0; i < static_cast<int>(numLibraryOptions) && is_unified_pointer(conn, (void*)libraryOptions); i++)
      if (maybe_copy_unified_arg(conn, (void*)&libraryOptions[i], cudaMemcpyDeviceToHost) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)libraryOptionValues, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    for (int i = 0; i < static_cast<int>(numLibraryOptions) && is_unified_pointer(conn, (void*)libraryOptionValues); i++)
      if (maybe_copy_unified_arg(conn, (void*)&libraryOptionValues[i], cudaMemcpyDeviceToHost) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuLibraryUnload(CUlibrary library)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&library, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuLibraryUnload) < 0 ||
        rpc_write(conn, &library, sizeof(CUlibrary)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&library, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuLibraryGetKernel(CUkernel* pKernel, CUlibrary library, const char* name)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pKernel, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&library, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)name, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    std::size_t name_len = std::strlen(name) + 1;
    if (rpc_write_start_request(conn, RPC_cuLibraryGetKernel) < 0 ||
        rpc_write(conn, &library, sizeof(CUlibrary)) < 0 ||
        rpc_write(conn, &name_len, sizeof(std::size_t)) < 0 ||
        rpc_write(conn, name, name_len) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pKernel, sizeof(CUkernel)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pKernel, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&library, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)name, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuLibraryGetKernelCount(unsigned int* count, CUlibrary lib)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)count, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&lib, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuLibraryGetKernelCount) < 0 ||
        rpc_write(conn, count, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &lib, sizeof(CUlibrary)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, count, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)count, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&lib, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuLibraryEnumerateKernels(CUkernel* kernels, unsigned int numKernels, CUlibrary lib)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)kernels, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numKernels, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&lib, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuLibraryEnumerateKernels) < 0 ||
        rpc_write(conn, kernels, sizeof(CUkernel)) < 0 ||
        rpc_write(conn, &numKernels, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &lib, sizeof(CUlibrary)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, kernels, sizeof(CUkernel)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)kernels, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numKernels, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&lib, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuLibraryGetModule(CUmodule* pMod, CUlibrary library)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pMod, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&library, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuLibraryGetModule) < 0 ||
        rpc_write(conn, &library, sizeof(CUlibrary)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pMod, sizeof(CUmodule)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pMod, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&library, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuKernelGetFunction(CUfunction* pFunc, CUkernel kernel)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pFunc, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&kernel, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuKernelGetFunction) < 0 ||
        rpc_write(conn, &kernel, sizeof(CUkernel)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pFunc, sizeof(CUfunction)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pFunc, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&kernel, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuLibraryGetGlobal(CUdeviceptr* dptr, size_t* bytes, CUlibrary library, const char* name)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)dptr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)bytes, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&library, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)name, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    std::size_t name_len = std::strlen(name) + 1;
    if (rpc_write_start_request(conn, RPC_cuLibraryGetGlobal) < 0 ||
        rpc_write(conn, &library, sizeof(CUlibrary)) < 0 ||
        rpc_write(conn, &name_len, sizeof(std::size_t)) < 0 ||
        rpc_write(conn, name, name_len) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, dptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, bytes, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dptr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)bytes, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&library, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)name, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuLibraryGetManaged(CUdeviceptr* dptr, size_t* bytes, CUlibrary library, const char* name)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)dptr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)bytes, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&library, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)name, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    std::size_t name_len = std::strlen(name) + 1;
    if (rpc_write_start_request(conn, RPC_cuLibraryGetManaged) < 0 ||
        rpc_write(conn, &library, sizeof(CUlibrary)) < 0 ||
        rpc_write(conn, &name_len, sizeof(std::size_t)) < 0 ||
        rpc_write(conn, name, name_len) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, dptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, bytes, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dptr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)bytes, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&library, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)name, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuLibraryGetUnifiedFunction(void** fptr, CUlibrary library, const char* symbol)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)fptr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&library, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)symbol, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    std::size_t symbol_len = std::strlen(symbol) + 1;
    if (rpc_write_start_request(conn, RPC_cuLibraryGetUnifiedFunction) < 0 ||
        rpc_write(conn, &library, sizeof(CUlibrary)) < 0 ||
        rpc_write(conn, &symbol_len, sizeof(std::size_t)) < 0 ||
        rpc_write(conn, symbol, symbol_len) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, fptr, sizeof(void*)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)fptr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&library, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)symbol, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuKernelGetAttribute(int* pi, CUfunction_attribute attrib, CUkernel kernel, CUdevice dev)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pi, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&attrib, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&kernel, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuKernelGetAttribute) < 0 ||
        rpc_write(conn, pi, sizeof(int)) < 0 ||
        rpc_write(conn, &attrib, sizeof(CUfunction_attribute)) < 0 ||
        rpc_write(conn, &kernel, sizeof(CUkernel)) < 0 ||
        rpc_write(conn, &dev, sizeof(CUdevice)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pi, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pi, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&attrib, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&kernel, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuKernelSetAttribute(CUfunction_attribute attrib, int val, CUkernel kernel, CUdevice dev)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&attrib, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&val, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&kernel, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuKernelSetAttribute) < 0 ||
        rpc_write(conn, &attrib, sizeof(CUfunction_attribute)) < 0 ||
        rpc_write(conn, &val, sizeof(int)) < 0 ||
        rpc_write(conn, &kernel, sizeof(CUkernel)) < 0 ||
        rpc_write(conn, &dev, sizeof(CUdevice)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&attrib, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&val, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&kernel, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuKernelSetCacheConfig(CUkernel kernel, CUfunc_cache config, CUdevice dev)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&kernel, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&config, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuKernelSetCacheConfig) < 0 ||
        rpc_write(conn, &kernel, sizeof(CUkernel)) < 0 ||
        rpc_write(conn, &config, sizeof(CUfunc_cache)) < 0 ||
        rpc_write(conn, &dev, sizeof(CUdevice)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&kernel, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&config, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuKernelGetName(const char** name, CUkernel hfunc)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)name, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hfunc, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuKernelGetName) < 0 ||
        rpc_write(conn, name, sizeof(const char*)) < 0 ||
        rpc_write(conn, &hfunc, sizeof(CUkernel)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, name, sizeof(const char*)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)name, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hfunc, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuKernelGetParamInfo(CUkernel kernel, size_t paramIndex, size_t* paramOffset, size_t* paramSize)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&kernel, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&paramIndex, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)paramOffset, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)paramSize, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuKernelGetParamInfo) < 0 ||
        rpc_write(conn, &kernel, sizeof(CUkernel)) < 0 ||
        rpc_write(conn, &paramIndex, sizeof(size_t)) < 0 ||
        rpc_write(conn, paramOffset, sizeof(size_t)) < 0 ||
        rpc_write(conn, paramSize, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, paramOffset, sizeof(size_t)) < 0 ||
        rpc_read(conn, paramSize, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&kernel, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&paramIndex, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)paramOffset, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)paramSize, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemGetInfo_v2(size_t* free, size_t* total)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)free, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)total, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemGetInfo_v2) < 0 ||
        rpc_write(conn, free, sizeof(size_t)) < 0 ||
        rpc_write(conn, total, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, free, sizeof(size_t)) < 0 ||
        rpc_read(conn, total, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)free, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)total, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemAlloc_v2(CUdeviceptr* dptr, size_t bytesize)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)dptr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&bytesize, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemAlloc_v2) < 0 ||
        rpc_write(conn, dptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &bytesize, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, dptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dptr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&bytesize, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemAllocPitch_v2(CUdeviceptr* dptr, size_t* pPitch, size_t WidthInBytes, size_t Height, unsigned int ElementSizeBytes)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)dptr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pPitch, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&WidthInBytes, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Height, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ElementSizeBytes, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemAllocPitch_v2) < 0 ||
        rpc_write(conn, dptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, pPitch, sizeof(size_t)) < 0 ||
        rpc_write(conn, &WidthInBytes, sizeof(size_t)) < 0 ||
        rpc_write(conn, &Height, sizeof(size_t)) < 0 ||
        rpc_write(conn, &ElementSizeBytes, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, dptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, pPitch, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dptr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pPitch, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&WidthInBytes, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Height, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ElementSizeBytes, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemFree_v2(CUdeviceptr dptr)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&dptr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemFree_v2) < 0 ||
        rpc_write(conn, &dptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dptr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemGetAddressRange_v2(CUdeviceptr* pbase, size_t* psize, CUdeviceptr dptr)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pbase, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)psize, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dptr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemGetAddressRange_v2) < 0 ||
        rpc_write(conn, pbase, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, psize, sizeof(size_t)) < 0 ||
        rpc_write(conn, &dptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pbase, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, psize, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pbase, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)psize, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dptr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemAllocHost_v2(void** pp, size_t bytesize)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pp, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&bytesize, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemAllocHost_v2) < 0 ||
        rpc_write(conn, &bytesize, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pp, sizeof(void*)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pp, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&bytesize, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemFreeHost(void* p)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)p, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemFreeHost) < 0 ||
        rpc_write(conn, &p, sizeof(void*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)p, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemHostAlloc(void** pp, size_t bytesize, unsigned int Flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pp, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&bytesize, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemHostAlloc) < 0 ||
        rpc_write(conn, &bytesize, sizeof(size_t)) < 0 ||
        rpc_write(conn, &Flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pp, sizeof(void*)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pp, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&bytesize, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemHostGetDevicePointer_v2(CUdeviceptr* pdptr, void* p, unsigned int Flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pdptr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)p, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemHostGetDevicePointer_v2) < 0 ||
        rpc_write(conn, pdptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &p, sizeof(void*)) < 0 ||
        rpc_write(conn, &Flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pdptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pdptr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)p, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemHostGetFlags(unsigned int* pFlags, void* p)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pFlags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)p, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemHostGetFlags) < 0 ||
        rpc_write(conn, pFlags, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &p, sizeof(void*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pFlags, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pFlags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)p, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemAllocManaged(CUdeviceptr* dptr, size_t bytesize, unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)dptr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&bytesize, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemAllocManaged) < 0 ||
        rpc_write(conn, dptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &bytesize, sizeof(size_t)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, dptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dptr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&bytesize, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuDeviceUnregisterAsyncNotification(CUdevice device, CUasyncCallbackHandle callback)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&callback, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuDeviceUnregisterAsyncNotification) < 0 ||
        rpc_write(conn, &device, sizeof(CUdevice)) < 0 ||
        rpc_write(conn, &callback, sizeof(CUasyncCallbackHandle)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&callback, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuDeviceGetByPCIBusId(CUdevice* dev, const char* pciBusId)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)dev, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pciBusId, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    std::size_t pciBusId_len = std::strlen(pciBusId) + 1;
    if (rpc_write_start_request(conn, RPC_cuDeviceGetByPCIBusId) < 0 ||
        rpc_write(conn, dev, sizeof(CUdevice)) < 0 ||
        rpc_write(conn, &pciBusId_len, sizeof(std::size_t)) < 0 ||
        rpc_write(conn, pciBusId, pciBusId_len) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, dev, sizeof(CUdevice)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dev, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pciBusId, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuDeviceGetPCIBusId(char* pciBusId, int len, CUdevice dev)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&len, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pciBusId, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    for (int i = 0; i < static_cast<int>(len) && is_unified_pointer(conn, (void*)pciBusId); i++)
      if (maybe_copy_unified_arg(conn, (void*)&pciBusId[i], cudaMemcpyHostToDevice) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuDeviceGetPCIBusId) < 0 ||
        rpc_write(conn, &len, sizeof(int)) < 0 ||
        rpc_write(conn, &dev, sizeof(CUdevice)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pciBusId, len * sizeof(char)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&len, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pciBusId, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    for (int i = 0; i < static_cast<int>(len) && is_unified_pointer(conn, (void*)pciBusId); i++)
      if (maybe_copy_unified_arg(conn, (void*)&pciBusId[i], cudaMemcpyDeviceToHost) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuIpcGetEventHandle(CUipcEventHandle* pHandle, CUevent event)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pHandle, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&event, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuIpcGetEventHandle) < 0 ||
        rpc_write(conn, pHandle, sizeof(CUipcEventHandle)) < 0 ||
        rpc_write(conn, &event, sizeof(CUevent)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pHandle, sizeof(CUipcEventHandle)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pHandle, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&event, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuIpcOpenEventHandle(CUevent* phEvent, CUipcEventHandle handle)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)phEvent, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&handle, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuIpcOpenEventHandle) < 0 ||
        rpc_write(conn, phEvent, sizeof(CUevent)) < 0 ||
        rpc_write(conn, &handle, sizeof(CUipcEventHandle)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, phEvent, sizeof(CUevent)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)phEvent, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&handle, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuIpcGetMemHandle(CUipcMemHandle* pHandle, CUdeviceptr dptr)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pHandle, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dptr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuIpcGetMemHandle) < 0 ||
        rpc_write(conn, pHandle, sizeof(CUipcMemHandle)) < 0 ||
        rpc_write(conn, &dptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pHandle, sizeof(CUipcMemHandle)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pHandle, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dptr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuIpcOpenMemHandle_v2(CUdeviceptr* pdptr, CUipcMemHandle handle, unsigned int Flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pdptr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&handle, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuIpcOpenMemHandle_v2) < 0 ||
        rpc_write(conn, pdptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &handle, sizeof(CUipcMemHandle)) < 0 ||
        rpc_write(conn, &Flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pdptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pdptr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&handle, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuIpcCloseMemHandle(CUdeviceptr dptr)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&dptr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuIpcCloseMemHandle) < 0 ||
        rpc_write(conn, &dptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dptr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemcpy(CUdeviceptr dst, CUdeviceptr src, size_t ByteCount)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&dst, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&src, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ByteCount, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemcpy) < 0 ||
        rpc_write(conn, &dst, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &src, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &ByteCount, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dst, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&src, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ByteCount, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemcpyPeer(CUdeviceptr dstDevice, CUcontext dstContext, CUdeviceptr srcDevice, CUcontext srcContext, size_t ByteCount)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstContext, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&srcDevice, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&srcContext, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ByteCount, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemcpyPeer) < 0 ||
        rpc_write(conn, &dstDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &dstContext, sizeof(CUcontext)) < 0 ||
        rpc_write(conn, &srcDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &srcContext, sizeof(CUcontext)) < 0 ||
        rpc_write(conn, &ByteCount, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstContext, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&srcDevice, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&srcContext, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ByteCount, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemcpyHtoD_v2(CUdeviceptr dstDevice, const void* srcHost, size_t ByteCount)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)srcHost, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ByteCount, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemcpyHtoD_v2) < 0 ||
        rpc_write(conn, &dstDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &srcHost, sizeof(const void*)) < 0 ||
        rpc_write(conn, &ByteCount, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)srcHost, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ByteCount, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemcpyDtoD_v2(CUdeviceptr dstDevice, CUdeviceptr srcDevice, size_t ByteCount)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&srcDevice, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ByteCount, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemcpyDtoD_v2) < 0 ||
        rpc_write(conn, &dstDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &srcDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &ByteCount, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&srcDevice, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ByteCount, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemcpyDtoA_v2(CUarray dstArray, size_t dstOffset, CUdeviceptr srcDevice, size_t ByteCount)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&dstArray, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstOffset, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&srcDevice, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ByteCount, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemcpyDtoA_v2) < 0 ||
        rpc_write(conn, &dstArray, sizeof(CUarray)) < 0 ||
        rpc_write(conn, &dstOffset, sizeof(size_t)) < 0 ||
        rpc_write(conn, &srcDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &ByteCount, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstArray, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstOffset, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&srcDevice, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ByteCount, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemcpyAtoD_v2(CUdeviceptr dstDevice, CUarray srcArray, size_t srcOffset, size_t ByteCount)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&srcArray, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&srcOffset, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ByteCount, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemcpyAtoD_v2) < 0 ||
        rpc_write(conn, &dstDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &srcArray, sizeof(CUarray)) < 0 ||
        rpc_write(conn, &srcOffset, sizeof(size_t)) < 0 ||
        rpc_write(conn, &ByteCount, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&srcArray, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&srcOffset, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ByteCount, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemcpyAtoH_v2(void* dstHost, CUarray srcArray, size_t srcOffset, size_t ByteCount)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)dstHost, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&srcArray, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&srcOffset, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ByteCount, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemcpyAtoH_v2) < 0 ||
        rpc_write(conn, &dstHost, sizeof(void*)) < 0 ||
        rpc_write(conn, &srcArray, sizeof(CUarray)) < 0 ||
        rpc_write(conn, &srcOffset, sizeof(size_t)) < 0 ||
        rpc_write(conn, &ByteCount, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dstHost, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&srcArray, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&srcOffset, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ByteCount, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemcpyAtoA_v2(CUarray dstArray, size_t dstOffset, CUarray srcArray, size_t srcOffset, size_t ByteCount)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&dstArray, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstOffset, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&srcArray, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&srcOffset, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ByteCount, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemcpyAtoA_v2) < 0 ||
        rpc_write(conn, &dstArray, sizeof(CUarray)) < 0 ||
        rpc_write(conn, &dstOffset, sizeof(size_t)) < 0 ||
        rpc_write(conn, &srcArray, sizeof(CUarray)) < 0 ||
        rpc_write(conn, &srcOffset, sizeof(size_t)) < 0 ||
        rpc_write(conn, &ByteCount, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstArray, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstOffset, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&srcArray, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&srcOffset, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ByteCount, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemcpyAsync(CUdeviceptr dst, CUdeviceptr src, size_t ByteCount, CUstream hStream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&dst, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&src, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ByteCount, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemcpyAsync) < 0 ||
        rpc_write(conn, &dst, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &src, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &ByteCount, sizeof(size_t)) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dst, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&src, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ByteCount, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemcpyPeerAsync(CUdeviceptr dstDevice, CUcontext dstContext, CUdeviceptr srcDevice, CUcontext srcContext, size_t ByteCount, CUstream hStream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstContext, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&srcDevice, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&srcContext, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ByteCount, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemcpyPeerAsync) < 0 ||
        rpc_write(conn, &dstDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &dstContext, sizeof(CUcontext)) < 0 ||
        rpc_write(conn, &srcDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &srcContext, sizeof(CUcontext)) < 0 ||
        rpc_write(conn, &ByteCount, sizeof(size_t)) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstContext, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&srcDevice, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&srcContext, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ByteCount, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemcpyHtoDAsync_v2(CUdeviceptr dstDevice, const void* srcHost, size_t ByteCount, CUstream hStream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)srcHost, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ByteCount, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemcpyHtoDAsync_v2) < 0 ||
        rpc_write(conn, &dstDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &srcHost, sizeof(const void*)) < 0 ||
        rpc_write(conn, &ByteCount, sizeof(size_t)) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)srcHost, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ByteCount, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemcpyDtoDAsync_v2(CUdeviceptr dstDevice, CUdeviceptr srcDevice, size_t ByteCount, CUstream hStream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&srcDevice, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ByteCount, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemcpyDtoDAsync_v2) < 0 ||
        rpc_write(conn, &dstDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &srcDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &ByteCount, sizeof(size_t)) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&srcDevice, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ByteCount, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemsetD8_v2(CUdeviceptr dstDevice, unsigned char uc, size_t N)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&uc, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&N, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemsetD8_v2) < 0 ||
        rpc_write(conn, &dstDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &uc, sizeof(unsigned char)) < 0 ||
        rpc_write(conn, &N, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&uc, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&N, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemsetD16_v2(CUdeviceptr dstDevice, unsigned short us, size_t N)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&us, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&N, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemsetD16_v2) < 0 ||
        rpc_write(conn, &dstDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &us, sizeof(unsigned short)) < 0 ||
        rpc_write(conn, &N, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&us, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&N, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemsetD32_v2(CUdeviceptr dstDevice, unsigned int ui, size_t N)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ui, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&N, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemsetD32_v2) < 0 ||
        rpc_write(conn, &dstDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &ui, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &N, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ui, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&N, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemsetD2D8_v2(CUdeviceptr dstDevice, size_t dstPitch, unsigned char uc, size_t Width, size_t Height)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstPitch, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&uc, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Width, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Height, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemsetD2D8_v2) < 0 ||
        rpc_write(conn, &dstDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &dstPitch, sizeof(size_t)) < 0 ||
        rpc_write(conn, &uc, sizeof(unsigned char)) < 0 ||
        rpc_write(conn, &Width, sizeof(size_t)) < 0 ||
        rpc_write(conn, &Height, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstPitch, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&uc, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Width, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Height, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemsetD2D16_v2(CUdeviceptr dstDevice, size_t dstPitch, unsigned short us, size_t Width, size_t Height)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstPitch, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&us, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Width, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Height, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemsetD2D16_v2) < 0 ||
        rpc_write(conn, &dstDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &dstPitch, sizeof(size_t)) < 0 ||
        rpc_write(conn, &us, sizeof(unsigned short)) < 0 ||
        rpc_write(conn, &Width, sizeof(size_t)) < 0 ||
        rpc_write(conn, &Height, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstPitch, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&us, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Width, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Height, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemsetD2D32_v2(CUdeviceptr dstDevice, size_t dstPitch, unsigned int ui, size_t Width, size_t Height)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstPitch, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ui, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Width, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Height, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemsetD2D32_v2) < 0 ||
        rpc_write(conn, &dstDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &dstPitch, sizeof(size_t)) < 0 ||
        rpc_write(conn, &ui, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &Width, sizeof(size_t)) < 0 ||
        rpc_write(conn, &Height, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstPitch, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ui, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Width, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Height, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemsetD8Async(CUdeviceptr dstDevice, unsigned char uc, size_t N, CUstream hStream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&uc, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&N, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemsetD8Async) < 0 ||
        rpc_write(conn, &dstDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &uc, sizeof(unsigned char)) < 0 ||
        rpc_write(conn, &N, sizeof(size_t)) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&uc, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&N, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemsetD16Async(CUdeviceptr dstDevice, unsigned short us, size_t N, CUstream hStream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&us, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&N, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemsetD16Async) < 0 ||
        rpc_write(conn, &dstDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &us, sizeof(unsigned short)) < 0 ||
        rpc_write(conn, &N, sizeof(size_t)) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&us, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&N, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemsetD32Async(CUdeviceptr dstDevice, unsigned int ui, size_t N, CUstream hStream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ui, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&N, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemsetD32Async) < 0 ||
        rpc_write(conn, &dstDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &ui, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &N, sizeof(size_t)) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ui, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&N, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemsetD2D8Async(CUdeviceptr dstDevice, size_t dstPitch, unsigned char uc, size_t Width, size_t Height, CUstream hStream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstPitch, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&uc, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Width, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Height, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemsetD2D8Async) < 0 ||
        rpc_write(conn, &dstDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &dstPitch, sizeof(size_t)) < 0 ||
        rpc_write(conn, &uc, sizeof(unsigned char)) < 0 ||
        rpc_write(conn, &Width, sizeof(size_t)) < 0 ||
        rpc_write(conn, &Height, sizeof(size_t)) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstPitch, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&uc, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Width, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Height, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemsetD2D16Async(CUdeviceptr dstDevice, size_t dstPitch, unsigned short us, size_t Width, size_t Height, CUstream hStream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstPitch, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&us, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Width, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Height, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemsetD2D16Async) < 0 ||
        rpc_write(conn, &dstDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &dstPitch, sizeof(size_t)) < 0 ||
        rpc_write(conn, &us, sizeof(unsigned short)) < 0 ||
        rpc_write(conn, &Width, sizeof(size_t)) < 0 ||
        rpc_write(conn, &Height, sizeof(size_t)) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstPitch, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&us, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Width, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Height, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemsetD2D32Async(CUdeviceptr dstDevice, size_t dstPitch, unsigned int ui, size_t Width, size_t Height, CUstream hStream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstPitch, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ui, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Width, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Height, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemsetD2D32Async) < 0 ||
        rpc_write(conn, &dstDevice, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &dstPitch, sizeof(size_t)) < 0 ||
        rpc_write(conn, &ui, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &Width, sizeof(size_t)) < 0 ||
        rpc_write(conn, &Height, sizeof(size_t)) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstPitch, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ui, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Width, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Height, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuArrayCreate_v2(CUarray* pHandle, const CUDA_ARRAY_DESCRIPTOR* pAllocateArray)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pHandle, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pAllocateArray, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuArrayCreate_v2) < 0 ||
        rpc_write(conn, pHandle, sizeof(CUarray)) < 0 ||
        rpc_write(conn, &pAllocateArray, sizeof(const CUDA_ARRAY_DESCRIPTOR*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pHandle, sizeof(CUarray)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pHandle, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pAllocateArray, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuArrayGetDescriptor_v2(CUDA_ARRAY_DESCRIPTOR* pArrayDescriptor, CUarray hArray)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pArrayDescriptor, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hArray, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuArrayGetDescriptor_v2) < 0 ||
        rpc_write(conn, pArrayDescriptor, sizeof(CUDA_ARRAY_DESCRIPTOR)) < 0 ||
        rpc_write(conn, &hArray, sizeof(CUarray)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pArrayDescriptor, sizeof(CUDA_ARRAY_DESCRIPTOR)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pArrayDescriptor, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hArray, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuArrayGetSparseProperties(CUDA_ARRAY_SPARSE_PROPERTIES* sparseProperties, CUarray array)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)sparseProperties, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&array, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuArrayGetSparseProperties) < 0 ||
        rpc_write(conn, sparseProperties, sizeof(CUDA_ARRAY_SPARSE_PROPERTIES)) < 0 ||
        rpc_write(conn, &array, sizeof(CUarray)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, sparseProperties, sizeof(CUDA_ARRAY_SPARSE_PROPERTIES)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)sparseProperties, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&array, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMipmappedArrayGetSparseProperties(CUDA_ARRAY_SPARSE_PROPERTIES* sparseProperties, CUmipmappedArray mipmap)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)sparseProperties, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&mipmap, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMipmappedArrayGetSparseProperties) < 0 ||
        rpc_write(conn, sparseProperties, sizeof(CUDA_ARRAY_SPARSE_PROPERTIES)) < 0 ||
        rpc_write(conn, &mipmap, sizeof(CUmipmappedArray)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, sparseProperties, sizeof(CUDA_ARRAY_SPARSE_PROPERTIES)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)sparseProperties, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&mipmap, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuArrayGetMemoryRequirements(CUDA_ARRAY_MEMORY_REQUIREMENTS* memoryRequirements, CUarray array, CUdevice device)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)memoryRequirements, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&array, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuArrayGetMemoryRequirements) < 0 ||
        rpc_write(conn, memoryRequirements, sizeof(CUDA_ARRAY_MEMORY_REQUIREMENTS)) < 0 ||
        rpc_write(conn, &array, sizeof(CUarray)) < 0 ||
        rpc_write(conn, &device, sizeof(CUdevice)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, memoryRequirements, sizeof(CUDA_ARRAY_MEMORY_REQUIREMENTS)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)memoryRequirements, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&array, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMipmappedArrayGetMemoryRequirements(CUDA_ARRAY_MEMORY_REQUIREMENTS* memoryRequirements, CUmipmappedArray mipmap, CUdevice device)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)memoryRequirements, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&mipmap, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMipmappedArrayGetMemoryRequirements) < 0 ||
        rpc_write(conn, memoryRequirements, sizeof(CUDA_ARRAY_MEMORY_REQUIREMENTS)) < 0 ||
        rpc_write(conn, &mipmap, sizeof(CUmipmappedArray)) < 0 ||
        rpc_write(conn, &device, sizeof(CUdevice)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, memoryRequirements, sizeof(CUDA_ARRAY_MEMORY_REQUIREMENTS)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)memoryRequirements, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&mipmap, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuArrayGetPlane(CUarray* pPlaneArray, CUarray hArray, unsigned int planeIdx)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pPlaneArray, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hArray, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&planeIdx, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuArrayGetPlane) < 0 ||
        rpc_write(conn, pPlaneArray, sizeof(CUarray)) < 0 ||
        rpc_write(conn, &hArray, sizeof(CUarray)) < 0 ||
        rpc_write(conn, &planeIdx, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pPlaneArray, sizeof(CUarray)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pPlaneArray, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hArray, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&planeIdx, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuArrayDestroy(CUarray hArray)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hArray, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuArrayDestroy) < 0 ||
        rpc_write(conn, &hArray, sizeof(CUarray)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hArray, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuArray3DCreate_v2(CUarray* pHandle, const CUDA_ARRAY3D_DESCRIPTOR* pAllocateArray)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pHandle, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pAllocateArray, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuArray3DCreate_v2) < 0 ||
        rpc_write(conn, pHandle, sizeof(CUarray)) < 0 ||
        rpc_write(conn, &pAllocateArray, sizeof(const CUDA_ARRAY3D_DESCRIPTOR*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pHandle, sizeof(CUarray)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pHandle, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pAllocateArray, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuArray3DGetDescriptor_v2(CUDA_ARRAY3D_DESCRIPTOR* pArrayDescriptor, CUarray hArray)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pArrayDescriptor, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hArray, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuArray3DGetDescriptor_v2) < 0 ||
        rpc_write(conn, pArrayDescriptor, sizeof(CUDA_ARRAY3D_DESCRIPTOR)) < 0 ||
        rpc_write(conn, &hArray, sizeof(CUarray)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pArrayDescriptor, sizeof(CUDA_ARRAY3D_DESCRIPTOR)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pArrayDescriptor, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hArray, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMipmappedArrayCreate(CUmipmappedArray* pHandle, const CUDA_ARRAY3D_DESCRIPTOR* pMipmappedArrayDesc, unsigned int numMipmapLevels)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pHandle, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pMipmappedArrayDesc, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numMipmapLevels, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMipmappedArrayCreate) < 0 ||
        rpc_write(conn, pHandle, sizeof(CUmipmappedArray)) < 0 ||
        rpc_write(conn, &pMipmappedArrayDesc, sizeof(const CUDA_ARRAY3D_DESCRIPTOR*)) < 0 ||
        rpc_write(conn, &numMipmapLevels, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pHandle, sizeof(CUmipmappedArray)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pHandle, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pMipmappedArrayDesc, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numMipmapLevels, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMipmappedArrayGetLevel(CUarray* pLevelArray, CUmipmappedArray hMipmappedArray, unsigned int level)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pLevelArray, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hMipmappedArray, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&level, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMipmappedArrayGetLevel) < 0 ||
        rpc_write(conn, pLevelArray, sizeof(CUarray)) < 0 ||
        rpc_write(conn, &hMipmappedArray, sizeof(CUmipmappedArray)) < 0 ||
        rpc_write(conn, &level, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pLevelArray, sizeof(CUarray)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pLevelArray, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hMipmappedArray, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&level, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMipmappedArrayDestroy(CUmipmappedArray hMipmappedArray)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hMipmappedArray, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMipmappedArrayDestroy) < 0 ||
        rpc_write(conn, &hMipmappedArray, sizeof(CUmipmappedArray)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hMipmappedArray, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemAddressReserve(CUdeviceptr* ptr, size_t size, size_t alignment, CUdeviceptr addr, unsigned long long flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)ptr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&size, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&alignment, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&addr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemAddressReserve) < 0 ||
        rpc_write(conn, ptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &size, sizeof(size_t)) < 0 ||
        rpc_write(conn, &alignment, sizeof(size_t)) < 0 ||
        rpc_write(conn, &addr, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned long long)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, ptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)ptr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&size, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&alignment, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&addr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemAddressFree(CUdeviceptr ptr, size_t size)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&ptr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&size, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemAddressFree) < 0 ||
        rpc_write(conn, &ptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &size, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ptr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&size, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemCreate(CUmemGenericAllocationHandle* handle, size_t size, const CUmemAllocationProp* prop, unsigned long long flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)handle, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&size, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)prop, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemCreate) < 0 ||
        rpc_write(conn, handle, sizeof(CUmemGenericAllocationHandle)) < 0 ||
        rpc_write(conn, &size, sizeof(size_t)) < 0 ||
        rpc_write(conn, &prop, sizeof(const CUmemAllocationProp*)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned long long)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, handle, sizeof(CUmemGenericAllocationHandle)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)handle, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&size, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)prop, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemRelease(CUmemGenericAllocationHandle handle)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&handle, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemRelease) < 0 ||
        rpc_write(conn, &handle, sizeof(CUmemGenericAllocationHandle)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&handle, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemMap(CUdeviceptr ptr, size_t size, size_t offset, CUmemGenericAllocationHandle handle, unsigned long long flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&ptr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&size, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&offset, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&handle, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemMap) < 0 ||
        rpc_write(conn, &ptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &size, sizeof(size_t)) < 0 ||
        rpc_write(conn, &offset, sizeof(size_t)) < 0 ||
        rpc_write(conn, &handle, sizeof(CUmemGenericAllocationHandle)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned long long)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ptr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&size, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&offset, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&handle, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemMapArrayAsync(CUarrayMapInfo* mapInfoList, unsigned int count, CUstream hStream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)mapInfoList, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemMapArrayAsync) < 0 ||
        rpc_write(conn, mapInfoList, sizeof(CUarrayMapInfo)) < 0 ||
        rpc_write(conn, &count, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, mapInfoList, sizeof(CUarrayMapInfo)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)mapInfoList, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemUnmap(CUdeviceptr ptr, size_t size)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&ptr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&size, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemUnmap) < 0 ||
        rpc_write(conn, &ptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &size, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ptr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&size, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemSetAccess(CUdeviceptr ptr, size_t size, const CUmemAccessDesc* desc, size_t count)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&ptr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&size, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)desc, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemSetAccess) < 0 ||
        rpc_write(conn, &ptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &size, sizeof(size_t)) < 0 ||
        rpc_write(conn, &desc, sizeof(const CUmemAccessDesc*)) < 0 ||
        rpc_write(conn, &count, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ptr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&size, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)desc, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemGetAccess(unsigned long long* flags, const CUmemLocation* location, CUdeviceptr ptr)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)location, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ptr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemGetAccess) < 0 ||
        rpc_write(conn, flags, sizeof(unsigned long long)) < 0 ||
        rpc_write(conn, &location, sizeof(const CUmemLocation*)) < 0 ||
        rpc_write(conn, &ptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, flags, sizeof(unsigned long long)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)location, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ptr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemGetAllocationGranularity(size_t* granularity, const CUmemAllocationProp* prop, CUmemAllocationGranularity_flags option)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)granularity, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)prop, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&option, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemGetAllocationGranularity) < 0 ||
        rpc_write(conn, granularity, sizeof(size_t)) < 0 ||
        rpc_write(conn, &prop, sizeof(const CUmemAllocationProp*)) < 0 ||
        rpc_write(conn, &option, sizeof(CUmemAllocationGranularity_flags)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, granularity, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)granularity, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)prop, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&option, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemGetAllocationPropertiesFromHandle(CUmemAllocationProp* prop, CUmemGenericAllocationHandle handle)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)prop, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&handle, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemGetAllocationPropertiesFromHandle) < 0 ||
        rpc_write(conn, prop, sizeof(CUmemAllocationProp)) < 0 ||
        rpc_write(conn, &handle, sizeof(CUmemGenericAllocationHandle)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, prop, sizeof(CUmemAllocationProp)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)prop, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&handle, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemFreeAsync(CUdeviceptr dptr, CUstream hStream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&dptr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemFreeAsync) < 0 ||
        rpc_write(conn, &dptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dptr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemAllocAsync(CUdeviceptr* dptr, size_t bytesize, CUstream hStream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)dptr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&bytesize, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemAllocAsync) < 0 ||
        rpc_write(conn, dptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &bytesize, sizeof(size_t)) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, dptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dptr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&bytesize, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemPoolTrimTo(CUmemoryPool pool, size_t minBytesToKeep)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&pool, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&minBytesToKeep, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemPoolTrimTo) < 0 ||
        rpc_write(conn, &pool, sizeof(CUmemoryPool)) < 0 ||
        rpc_write(conn, &minBytesToKeep, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&pool, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&minBytesToKeep, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemPoolSetAccess(CUmemoryPool pool, const CUmemAccessDesc* map, size_t count)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&pool, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)map, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemPoolSetAccess) < 0 ||
        rpc_write(conn, &pool, sizeof(CUmemoryPool)) < 0 ||
        rpc_write(conn, &map, sizeof(const CUmemAccessDesc*)) < 0 ||
        rpc_write(conn, &count, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&pool, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)map, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemPoolGetAccess(CUmemAccess_flags* flags, CUmemoryPool memPool, CUmemLocation* location)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&memPool, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)location, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemPoolGetAccess) < 0 ||
        rpc_write(conn, flags, sizeof(CUmemAccess_flags)) < 0 ||
        rpc_write(conn, &memPool, sizeof(CUmemoryPool)) < 0 ||
        rpc_write(conn, location, sizeof(CUmemLocation)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, flags, sizeof(CUmemAccess_flags)) < 0 ||
        rpc_read(conn, location, sizeof(CUmemLocation)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&memPool, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)location, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemPoolCreate(CUmemoryPool* pool, const CUmemPoolProps* poolProps)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pool, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)poolProps, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemPoolCreate) < 0 ||
        rpc_write(conn, pool, sizeof(CUmemoryPool)) < 0 ||
        rpc_write(conn, &poolProps, sizeof(const CUmemPoolProps*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pool, sizeof(CUmemoryPool)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pool, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)poolProps, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemPoolDestroy(CUmemoryPool pool)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&pool, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemPoolDestroy) < 0 ||
        rpc_write(conn, &pool, sizeof(CUmemoryPool)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&pool, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemAllocFromPoolAsync(CUdeviceptr* dptr, size_t bytesize, CUmemoryPool pool, CUstream hStream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)dptr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&bytesize, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&pool, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemAllocFromPoolAsync) < 0 ||
        rpc_write(conn, dptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &bytesize, sizeof(size_t)) < 0 ||
        rpc_write(conn, &pool, sizeof(CUmemoryPool)) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, dptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dptr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&bytesize, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&pool, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemPoolExportPointer(CUmemPoolPtrExportData* shareData_out, CUdeviceptr ptr)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)shareData_out, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ptr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemPoolExportPointer) < 0 ||
        rpc_write(conn, shareData_out, sizeof(CUmemPoolPtrExportData)) < 0 ||
        rpc_write(conn, &ptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, shareData_out, sizeof(CUmemPoolPtrExportData)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)shareData_out, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ptr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemPoolImportPointer(CUdeviceptr* ptr_out, CUmemoryPool pool, CUmemPoolPtrExportData* shareData)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)ptr_out, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&pool, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)shareData, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemPoolImportPointer) < 0 ||
        rpc_write(conn, ptr_out, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &pool, sizeof(CUmemoryPool)) < 0 ||
        rpc_write(conn, shareData, sizeof(CUmemPoolPtrExportData)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, ptr_out, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, shareData, sizeof(CUmemPoolPtrExportData)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)ptr_out, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&pool, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)shareData, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMulticastCreate(CUmemGenericAllocationHandle* mcHandle, const CUmulticastObjectProp* prop)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)mcHandle, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)prop, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMulticastCreate) < 0 ||
        rpc_write(conn, mcHandle, sizeof(CUmemGenericAllocationHandle)) < 0 ||
        rpc_write(conn, &prop, sizeof(const CUmulticastObjectProp*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, mcHandle, sizeof(CUmemGenericAllocationHandle)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)mcHandle, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)prop, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMulticastAddDevice(CUmemGenericAllocationHandle mcHandle, CUdevice dev)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&mcHandle, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMulticastAddDevice) < 0 ||
        rpc_write(conn, &mcHandle, sizeof(CUmemGenericAllocationHandle)) < 0 ||
        rpc_write(conn, &dev, sizeof(CUdevice)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&mcHandle, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMulticastBindMem(CUmemGenericAllocationHandle mcHandle, size_t mcOffset, CUmemGenericAllocationHandle memHandle, size_t memOffset, size_t size, unsigned long long flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&mcHandle, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&mcOffset, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&memHandle, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&memOffset, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&size, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMulticastBindMem) < 0 ||
        rpc_write(conn, &mcHandle, sizeof(CUmemGenericAllocationHandle)) < 0 ||
        rpc_write(conn, &mcOffset, sizeof(size_t)) < 0 ||
        rpc_write(conn, &memHandle, sizeof(CUmemGenericAllocationHandle)) < 0 ||
        rpc_write(conn, &memOffset, sizeof(size_t)) < 0 ||
        rpc_write(conn, &size, sizeof(size_t)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned long long)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&mcHandle, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&mcOffset, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&memHandle, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&memOffset, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&size, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMulticastBindAddr(CUmemGenericAllocationHandle mcHandle, size_t mcOffset, CUdeviceptr memptr, size_t size, unsigned long long flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&mcHandle, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&mcOffset, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&memptr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&size, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMulticastBindAddr) < 0 ||
        rpc_write(conn, &mcHandle, sizeof(CUmemGenericAllocationHandle)) < 0 ||
        rpc_write(conn, &mcOffset, sizeof(size_t)) < 0 ||
        rpc_write(conn, &memptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &size, sizeof(size_t)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned long long)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&mcHandle, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&mcOffset, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&memptr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&size, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMulticastUnbind(CUmemGenericAllocationHandle mcHandle, CUdevice dev, size_t mcOffset, size_t size)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&mcHandle, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&mcOffset, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&size, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMulticastUnbind) < 0 ||
        rpc_write(conn, &mcHandle, sizeof(CUmemGenericAllocationHandle)) < 0 ||
        rpc_write(conn, &dev, sizeof(CUdevice)) < 0 ||
        rpc_write(conn, &mcOffset, sizeof(size_t)) < 0 ||
        rpc_write(conn, &size, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&mcHandle, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&mcOffset, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&size, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMulticastGetGranularity(size_t* granularity, const CUmulticastObjectProp* prop, CUmulticastGranularity_flags option)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)granularity, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)prop, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&option, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMulticastGetGranularity) < 0 ||
        rpc_write(conn, granularity, sizeof(size_t)) < 0 ||
        rpc_write(conn, &prop, sizeof(const CUmulticastObjectProp*)) < 0 ||
        rpc_write(conn, &option, sizeof(CUmulticastGranularity_flags)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, granularity, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)granularity, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)prop, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&option, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemPrefetchAsync(CUdeviceptr devPtr, size_t count, CUdevice dstDevice, CUstream hStream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&devPtr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemPrefetchAsync) < 0 ||
        rpc_write(conn, &devPtr, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &count, sizeof(size_t)) < 0 ||
        rpc_write(conn, &dstDevice, sizeof(CUdevice)) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&devPtr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemPrefetchAsync_v2(CUdeviceptr devPtr, size_t count, CUmemLocation location, unsigned int flags, CUstream hStream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&devPtr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&location, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemPrefetchAsync_v2) < 0 ||
        rpc_write(conn, &devPtr, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &count, sizeof(size_t)) < 0 ||
        rpc_write(conn, &location, sizeof(CUmemLocation)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&devPtr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&location, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemAdvise(CUdeviceptr devPtr, size_t count, CUmem_advise advice, CUdevice device)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&devPtr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&advice, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemAdvise) < 0 ||
        rpc_write(conn, &devPtr, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &count, sizeof(size_t)) < 0 ||
        rpc_write(conn, &advice, sizeof(CUmem_advise)) < 0 ||
        rpc_write(conn, &device, sizeof(CUdevice)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&devPtr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&advice, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemAdvise_v2(CUdeviceptr devPtr, size_t count, CUmem_advise advice, CUmemLocation location)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&devPtr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&advice, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&location, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemAdvise_v2) < 0 ||
        rpc_write(conn, &devPtr, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &count, sizeof(size_t)) < 0 ||
        rpc_write(conn, &advice, sizeof(CUmem_advise)) < 0 ||
        rpc_write(conn, &location, sizeof(CUmemLocation)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&devPtr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&advice, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&location, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuMemRangeGetAttributes(void** data, size_t* dataSizes, CUmem_range_attribute* attributes, size_t numAttributes, CUdeviceptr devPtr, size_t count)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)data, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dataSizes, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)attributes, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numAttributes, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&devPtr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuMemRangeGetAttributes) < 0 ||
        rpc_write(conn, data, sizeof(void*)) < 0 ||
        rpc_write(conn, dataSizes, sizeof(size_t)) < 0 ||
        rpc_write(conn, attributes, sizeof(CUmem_range_attribute)) < 0 ||
        rpc_write(conn, &numAttributes, sizeof(size_t)) < 0 ||
        rpc_write(conn, &devPtr, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &count, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, data, sizeof(void*)) < 0 ||
        rpc_read(conn, dataSizes, sizeof(size_t)) < 0 ||
        rpc_read(conn, attributes, sizeof(CUmem_range_attribute)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)data, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dataSizes, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)attributes, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numAttributes, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&devPtr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuPointerSetAttribute(const void* value, CUpointer_attribute attribute, CUdeviceptr ptr)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)value, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&attribute, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ptr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuPointerSetAttribute) < 0 ||
        rpc_write(conn, &value, sizeof(const void*)) < 0 ||
        rpc_write(conn, &attribute, sizeof(CUpointer_attribute)) < 0 ||
        rpc_write(conn, &ptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)value, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&attribute, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ptr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuPointerGetAttributes(unsigned int numAttributes, CUpointer_attribute* attributes, void** data, CUdeviceptr ptr)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&numAttributes, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)attributes, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)data, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ptr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuPointerGetAttributes) < 0 ||
        rpc_write(conn, &numAttributes, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, attributes, sizeof(CUpointer_attribute)) < 0 ||
        rpc_write(conn, data, sizeof(void*)) < 0 ||
        rpc_write(conn, &ptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, attributes, sizeof(CUpointer_attribute)) < 0 ||
        rpc_read(conn, data, sizeof(void*)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numAttributes, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)attributes, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)data, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ptr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuStreamCreate(CUstream* phStream, unsigned int Flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)phStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuStreamCreate) < 0 ||
        rpc_write(conn, phStream, sizeof(CUstream)) < 0 ||
        rpc_write(conn, &Flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, phStream, sizeof(CUstream)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)phStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuStreamCreateWithPriority(CUstream* phStream, unsigned int flags, int priority)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)phStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&priority, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuStreamCreateWithPriority) < 0 ||
        rpc_write(conn, phStream, sizeof(CUstream)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &priority, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, phStream, sizeof(CUstream)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)phStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&priority, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuStreamGetPriority(CUstream hStream, int* priority)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)priority, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuStreamGetPriority) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_write(conn, priority, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, priority, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)priority, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuStreamGetFlags(CUstream hStream, unsigned int* flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuStreamGetFlags) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_write(conn, flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, flags, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuStreamGetId(CUstream hStream, unsigned long long* streamId)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)streamId, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuStreamGetId) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_write(conn, streamId, sizeof(unsigned long long)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, streamId, sizeof(unsigned long long)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)streamId, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuStreamGetCtx(CUstream hStream, CUcontext* pctx)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pctx, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuStreamGetCtx) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_write(conn, pctx, sizeof(CUcontext)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pctx, sizeof(CUcontext)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pctx, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuStreamWaitEvent(CUstream hStream, CUevent hEvent, unsigned int Flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hEvent, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuStreamWaitEvent) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_write(conn, &hEvent, sizeof(CUevent)) < 0 ||
        rpc_write(conn, &Flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hEvent, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuStreamBeginCapture_v2(CUstream hStream, CUstreamCaptureMode mode)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&mode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuStreamBeginCapture_v2) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_write(conn, &mode, sizeof(CUstreamCaptureMode)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&mode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuStreamBeginCaptureToGraph(CUstream hStream, CUgraph hGraph, const CUgraphNode* dependencies, const CUgraphEdgeData* dependencyData, size_t numDependencies, CUstreamCaptureMode mode)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencyData, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&mode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuStreamBeginCaptureToGraph) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_write(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_write(conn, &dependencies, sizeof(const CUgraphNode*)) < 0 ||
        rpc_write(conn, &dependencyData, sizeof(const CUgraphEdgeData*)) < 0 ||
        rpc_write(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_write(conn, &mode, sizeof(CUstreamCaptureMode)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencyData, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&mode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuThreadExchangeStreamCaptureMode(CUstreamCaptureMode* mode)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)mode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuThreadExchangeStreamCaptureMode) < 0 ||
        rpc_write(conn, mode, sizeof(CUstreamCaptureMode)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, mode, sizeof(CUstreamCaptureMode)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)mode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuStreamEndCapture(CUstream hStream, CUgraph* phGraph)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)phGraph, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuStreamEndCapture) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_write(conn, phGraph, sizeof(CUgraph)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, phGraph, sizeof(CUgraph)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)phGraph, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuStreamIsCapturing(CUstream hStream, CUstreamCaptureStatus* captureStatus)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)captureStatus, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuStreamIsCapturing) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_write(conn, captureStatus, sizeof(CUstreamCaptureStatus)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, captureStatus, sizeof(CUstreamCaptureStatus)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)captureStatus, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuStreamGetCaptureInfo_v3(CUstream hStream, CUstreamCaptureStatus* captureStatus_out, cuuint64_t* id_out, CUgraph* graph_out, const CUgraphNode** dependencies_out, const CUgraphEdgeData** edgeData_out, size_t* numDependencies_out)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)captureStatus_out, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)id_out, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)graph_out, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies_out, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)edgeData_out, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)numDependencies_out, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuStreamGetCaptureInfo_v3) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_write(conn, captureStatus_out, sizeof(CUstreamCaptureStatus)) < 0 ||
        rpc_write(conn, id_out, sizeof(cuuint64_t)) < 0 ||
        rpc_write(conn, graph_out, sizeof(CUgraph)) < 0 ||
        rpc_write(conn, dependencies_out, sizeof(const CUgraphNode*)) < 0 ||
        rpc_write(conn, edgeData_out, sizeof(const CUgraphEdgeData*)) < 0 ||
        rpc_write(conn, numDependencies_out, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, captureStatus_out, sizeof(CUstreamCaptureStatus)) < 0 ||
        rpc_read(conn, id_out, sizeof(cuuint64_t)) < 0 ||
        rpc_read(conn, graph_out, sizeof(CUgraph)) < 0 ||
        rpc_read(conn, dependencies_out, sizeof(const CUgraphNode*)) < 0 ||
        rpc_read(conn, edgeData_out, sizeof(const CUgraphEdgeData*)) < 0 ||
        rpc_read(conn, numDependencies_out, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)captureStatus_out, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)id_out, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)graph_out, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies_out, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)edgeData_out, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)numDependencies_out, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuStreamUpdateCaptureDependencies(CUstream hStream, CUgraphNode* dependencies, size_t numDependencies, unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuStreamUpdateCaptureDependencies) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_write(conn, dependencies, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, dependencies, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuStreamUpdateCaptureDependencies_v2(CUstream hStream, CUgraphNode* dependencies, const CUgraphEdgeData* dependencyData, size_t numDependencies, unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencyData, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuStreamUpdateCaptureDependencies_v2) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_write(conn, dependencies, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &dependencyData, sizeof(const CUgraphEdgeData*)) < 0 ||
        rpc_write(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, dependencies, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencyData, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuStreamAttachMemAsync(CUstream hStream, CUdeviceptr dptr, size_t length, unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dptr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&length, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuStreamAttachMemAsync) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_write(conn, &dptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &length, sizeof(size_t)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dptr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&length, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuStreamQuery(CUstream hStream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuStreamQuery) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuStreamSynchronize(CUstream hStream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuStreamSynchronize) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuStreamDestroy_v2(CUstream hStream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuStreamDestroy_v2) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuStreamCopyAttributes(CUstream dst, CUstream src)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&dst, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&src, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuStreamCopyAttributes) < 0 ||
        rpc_write(conn, &dst, sizeof(CUstream)) < 0 ||
        rpc_write(conn, &src, sizeof(CUstream)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dst, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&src, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuStreamGetAttribute(CUstream hStream, CUstreamAttrID attr, CUstreamAttrValue* value_out)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&attr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)value_out, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuStreamGetAttribute) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_write(conn, &attr, sizeof(CUstreamAttrID)) < 0 ||
        rpc_write(conn, value_out, sizeof(CUstreamAttrValue)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, value_out, sizeof(CUstreamAttrValue)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&attr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)value_out, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuStreamSetAttribute(CUstream hStream, CUstreamAttrID attr, const CUstreamAttrValue* value)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&attr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)value, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuStreamSetAttribute) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_write(conn, &attr, sizeof(CUstreamAttrID)) < 0 ||
        rpc_write(conn, &value, sizeof(const CUstreamAttrValue*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&attr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)value, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuEventCreate(CUevent* phEvent, unsigned int Flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)phEvent, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuEventCreate) < 0 ||
        rpc_write(conn, phEvent, sizeof(CUevent)) < 0 ||
        rpc_write(conn, &Flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, phEvent, sizeof(CUevent)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)phEvent, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuEventRecord(CUevent hEvent, CUstream hStream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hEvent, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuEventRecord) < 0 ||
        rpc_write(conn, &hEvent, sizeof(CUevent)) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hEvent, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuEventRecordWithFlags(CUevent hEvent, CUstream hStream, unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hEvent, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuEventRecordWithFlags) < 0 ||
        rpc_write(conn, &hEvent, sizeof(CUevent)) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hEvent, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuEventQuery(CUevent hEvent)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hEvent, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuEventQuery) < 0 ||
        rpc_write(conn, &hEvent, sizeof(CUevent)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hEvent, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuEventSynchronize(CUevent hEvent)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hEvent, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuEventSynchronize) < 0 ||
        rpc_write(conn, &hEvent, sizeof(CUevent)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hEvent, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuEventDestroy_v2(CUevent hEvent)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hEvent, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuEventDestroy_v2) < 0 ||
        rpc_write(conn, &hEvent, sizeof(CUevent)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hEvent, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuEventElapsedTime(float* pMilliseconds, CUevent hStart, CUevent hEnd)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pMilliseconds, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStart, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hEnd, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuEventElapsedTime) < 0 ||
        rpc_write(conn, pMilliseconds, sizeof(float)) < 0 ||
        rpc_write(conn, &hStart, sizeof(CUevent)) < 0 ||
        rpc_write(conn, &hEnd, sizeof(CUevent)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pMilliseconds, sizeof(float)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pMilliseconds, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStart, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hEnd, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuImportExternalMemory(CUexternalMemory* extMem_out, const CUDA_EXTERNAL_MEMORY_HANDLE_DESC* memHandleDesc)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)extMem_out, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)memHandleDesc, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuImportExternalMemory) < 0 ||
        rpc_write(conn, extMem_out, sizeof(CUexternalMemory)) < 0 ||
        rpc_write(conn, &memHandleDesc, sizeof(const CUDA_EXTERNAL_MEMORY_HANDLE_DESC*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, extMem_out, sizeof(CUexternalMemory)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)extMem_out, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)memHandleDesc, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuExternalMemoryGetMappedBuffer(CUdeviceptr* devPtr, CUexternalMemory extMem, const CUDA_EXTERNAL_MEMORY_BUFFER_DESC* bufferDesc)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)devPtr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&extMem, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)bufferDesc, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuExternalMemoryGetMappedBuffer) < 0 ||
        rpc_write(conn, devPtr, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &extMem, sizeof(CUexternalMemory)) < 0 ||
        rpc_write(conn, &bufferDesc, sizeof(const CUDA_EXTERNAL_MEMORY_BUFFER_DESC*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, devPtr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)devPtr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&extMem, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)bufferDesc, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuExternalMemoryGetMappedMipmappedArray(CUmipmappedArray* mipmap, CUexternalMemory extMem, const CUDA_EXTERNAL_MEMORY_MIPMAPPED_ARRAY_DESC* mipmapDesc)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)mipmap, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&extMem, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)mipmapDesc, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuExternalMemoryGetMappedMipmappedArray) < 0 ||
        rpc_write(conn, mipmap, sizeof(CUmipmappedArray)) < 0 ||
        rpc_write(conn, &extMem, sizeof(CUexternalMemory)) < 0 ||
        rpc_write(conn, &mipmapDesc, sizeof(const CUDA_EXTERNAL_MEMORY_MIPMAPPED_ARRAY_DESC*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, mipmap, sizeof(CUmipmappedArray)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)mipmap, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&extMem, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)mipmapDesc, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuDestroyExternalMemory(CUexternalMemory extMem)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&extMem, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuDestroyExternalMemory) < 0 ||
        rpc_write(conn, &extMem, sizeof(CUexternalMemory)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&extMem, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuImportExternalSemaphore(CUexternalSemaphore* extSem_out, const CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC* semHandleDesc)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)extSem_out, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)semHandleDesc, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuImportExternalSemaphore) < 0 ||
        rpc_write(conn, extSem_out, sizeof(CUexternalSemaphore)) < 0 ||
        rpc_write(conn, &semHandleDesc, sizeof(const CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, extSem_out, sizeof(CUexternalSemaphore)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)extSem_out, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)semHandleDesc, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuSignalExternalSemaphoresAsync(const CUexternalSemaphore* extSemArray, const CUDA_EXTERNAL_SEMAPHORE_SIGNAL_PARAMS* paramsArray, unsigned int numExtSems, CUstream stream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)extSemArray, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)paramsArray, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numExtSems, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuSignalExternalSemaphoresAsync) < 0 ||
        rpc_write(conn, &extSemArray, sizeof(const CUexternalSemaphore*)) < 0 ||
        rpc_write(conn, &paramsArray, sizeof(const CUDA_EXTERNAL_SEMAPHORE_SIGNAL_PARAMS*)) < 0 ||
        rpc_write(conn, &numExtSems, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &stream, sizeof(CUstream)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)extSemArray, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)paramsArray, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numExtSems, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuWaitExternalSemaphoresAsync(const CUexternalSemaphore* extSemArray, const CUDA_EXTERNAL_SEMAPHORE_WAIT_PARAMS* paramsArray, unsigned int numExtSems, CUstream stream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)extSemArray, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)paramsArray, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numExtSems, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuWaitExternalSemaphoresAsync) < 0 ||
        rpc_write(conn, &extSemArray, sizeof(const CUexternalSemaphore*)) < 0 ||
        rpc_write(conn, &paramsArray, sizeof(const CUDA_EXTERNAL_SEMAPHORE_WAIT_PARAMS*)) < 0 ||
        rpc_write(conn, &numExtSems, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &stream, sizeof(CUstream)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)extSemArray, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)paramsArray, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numExtSems, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuDestroyExternalSemaphore(CUexternalSemaphore extSem)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&extSem, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuDestroyExternalSemaphore) < 0 ||
        rpc_write(conn, &extSem, sizeof(CUexternalSemaphore)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&extSem, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuStreamWaitValue32_v2(CUstream stream, CUdeviceptr addr, cuuint32_t value, unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&addr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&value, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuStreamWaitValue32_v2) < 0 ||
        rpc_write(conn, &stream, sizeof(CUstream)) < 0 ||
        rpc_write(conn, &addr, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &value, sizeof(cuuint32_t)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&addr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&value, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuStreamWaitValue64_v2(CUstream stream, CUdeviceptr addr, cuuint64_t value, unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&addr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&value, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuStreamWaitValue64_v2) < 0 ||
        rpc_write(conn, &stream, sizeof(CUstream)) < 0 ||
        rpc_write(conn, &addr, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &value, sizeof(cuuint64_t)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&addr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&value, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuStreamWriteValue32_v2(CUstream stream, CUdeviceptr addr, cuuint32_t value, unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&addr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&value, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuStreamWriteValue32_v2) < 0 ||
        rpc_write(conn, &stream, sizeof(CUstream)) < 0 ||
        rpc_write(conn, &addr, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &value, sizeof(cuuint32_t)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&addr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&value, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuStreamWriteValue64_v2(CUstream stream, CUdeviceptr addr, cuuint64_t value, unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&addr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&value, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuStreamWriteValue64_v2) < 0 ||
        rpc_write(conn, &stream, sizeof(CUstream)) < 0 ||
        rpc_write(conn, &addr, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &value, sizeof(cuuint64_t)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&addr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&value, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuStreamBatchMemOp_v2(CUstream stream, unsigned int count, CUstreamBatchMemOpParams* paramArray, unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)paramArray, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuStreamBatchMemOp_v2) < 0 ||
        rpc_write(conn, &stream, sizeof(CUstream)) < 0 ||
        rpc_write(conn, &count, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, paramArray, sizeof(CUstreamBatchMemOpParams)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, paramArray, sizeof(CUstreamBatchMemOpParams)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)paramArray, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuFuncGetAttribute(int* pi, CUfunction_attribute attrib, CUfunction hfunc)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pi, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&attrib, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hfunc, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuFuncGetAttribute) < 0 ||
        rpc_write(conn, pi, sizeof(int)) < 0 ||
        rpc_write(conn, &attrib, sizeof(CUfunction_attribute)) < 0 ||
        rpc_write(conn, &hfunc, sizeof(CUfunction)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pi, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pi, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&attrib, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hfunc, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuFuncSetAttribute(CUfunction hfunc, CUfunction_attribute attrib, int value)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hfunc, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&attrib, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&value, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuFuncSetAttribute) < 0 ||
        rpc_write(conn, &hfunc, sizeof(CUfunction)) < 0 ||
        rpc_write(conn, &attrib, sizeof(CUfunction_attribute)) < 0 ||
        rpc_write(conn, &value, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hfunc, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&attrib, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&value, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuFuncSetCacheConfig(CUfunction hfunc, CUfunc_cache config)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hfunc, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&config, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuFuncSetCacheConfig) < 0 ||
        rpc_write(conn, &hfunc, sizeof(CUfunction)) < 0 ||
        rpc_write(conn, &config, sizeof(CUfunc_cache)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hfunc, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&config, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuFuncGetModule(CUmodule* hmod, CUfunction hfunc)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)hmod, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hfunc, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuFuncGetModule) < 0 ||
        rpc_write(conn, hmod, sizeof(CUmodule)) < 0 ||
        rpc_write(conn, &hfunc, sizeof(CUfunction)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, hmod, sizeof(CUmodule)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)hmod, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hfunc, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuFuncGetName(const char** name, CUfunction hfunc)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)name, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hfunc, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuFuncGetName) < 0 ||
        rpc_write(conn, name, sizeof(const char*)) < 0 ||
        rpc_write(conn, &hfunc, sizeof(CUfunction)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, name, sizeof(const char*)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)name, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hfunc, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuFuncGetParamInfo(CUfunction func, size_t paramIndex, size_t* paramOffset, size_t* paramSize)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&func, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&paramIndex, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)paramOffset, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)paramSize, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuFuncGetParamInfo) < 0 ||
        rpc_write(conn, &func, sizeof(CUfunction)) < 0 ||
        rpc_write(conn, &paramIndex, sizeof(size_t)) < 0 ||
        rpc_write(conn, paramOffset, sizeof(size_t)) < 0 ||
        rpc_write(conn, paramSize, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, paramOffset, sizeof(size_t)) < 0 ||
        rpc_read(conn, paramSize, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&func, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&paramIndex, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)paramOffset, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)paramSize, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuFuncIsLoaded(CUfunctionLoadingState* state, CUfunction function)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)state, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&function, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuFuncIsLoaded) < 0 ||
        rpc_write(conn, state, sizeof(CUfunctionLoadingState)) < 0 ||
        rpc_write(conn, &function, sizeof(CUfunction)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, state, sizeof(CUfunctionLoadingState)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)state, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&function, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuFuncLoad(CUfunction function)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&function, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuFuncLoad) < 0 ||
        rpc_write(conn, &function, sizeof(CUfunction)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&function, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuLaunchKernel(CUfunction f, unsigned int gridDimX, unsigned int gridDimY, unsigned int gridDimZ, unsigned int blockDimX, unsigned int blockDimY, unsigned int blockDimZ, unsigned int sharedMemBytes, CUstream hStream, void** kernelParams, void** extra)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&f, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&gridDimX, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&gridDimY, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&gridDimZ, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&blockDimX, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&blockDimY, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&blockDimZ, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&sharedMemBytes, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)kernelParams, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)extra, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuLaunchKernel) < 0 ||
        rpc_write(conn, &f, sizeof(CUfunction)) < 0 ||
        rpc_write(conn, &gridDimX, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &gridDimY, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &gridDimZ, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &blockDimX, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &blockDimY, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &blockDimZ, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &sharedMemBytes, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_write(conn, &kernelParams, sizeof(void**)) < 0 ||
        rpc_write(conn, &extra, sizeof(void**)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&f, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&gridDimX, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&gridDimY, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&gridDimZ, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&blockDimX, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&blockDimY, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&blockDimZ, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&sharedMemBytes, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)kernelParams, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)extra, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuLaunchCooperativeKernel(CUfunction f, unsigned int gridDimX, unsigned int gridDimY, unsigned int gridDimZ, unsigned int blockDimX, unsigned int blockDimY, unsigned int blockDimZ, unsigned int sharedMemBytes, CUstream hStream, void** kernelParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&f, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&gridDimX, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&gridDimY, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&gridDimZ, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&blockDimX, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&blockDimY, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&blockDimZ, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&sharedMemBytes, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)kernelParams, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuLaunchCooperativeKernel) < 0 ||
        rpc_write(conn, &f, sizeof(CUfunction)) < 0 ||
        rpc_write(conn, &gridDimX, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &gridDimY, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &gridDimZ, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &blockDimX, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &blockDimY, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &blockDimZ, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &sharedMemBytes, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_write(conn, kernelParams, sizeof(void*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, kernelParams, sizeof(void*)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&f, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&gridDimX, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&gridDimY, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&gridDimZ, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&blockDimX, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&blockDimY, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&blockDimZ, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&sharedMemBytes, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)kernelParams, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuLaunchCooperativeKernelMultiDevice(CUDA_LAUNCH_PARAMS* launchParamsList, unsigned int numDevices, unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)launchParamsList, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDevices, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuLaunchCooperativeKernelMultiDevice) < 0 ||
        rpc_write(conn, launchParamsList, sizeof(CUDA_LAUNCH_PARAMS)) < 0 ||
        rpc_write(conn, &numDevices, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, launchParamsList, sizeof(CUDA_LAUNCH_PARAMS)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)launchParamsList, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDevices, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuFuncSetBlockShape(CUfunction hfunc, int x, int y, int z)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hfunc, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&x, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&y, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&z, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuFuncSetBlockShape) < 0 ||
        rpc_write(conn, &hfunc, sizeof(CUfunction)) < 0 ||
        rpc_write(conn, &x, sizeof(int)) < 0 ||
        rpc_write(conn, &y, sizeof(int)) < 0 ||
        rpc_write(conn, &z, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hfunc, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&x, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&y, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&z, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuFuncSetSharedSize(CUfunction hfunc, unsigned int bytes)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hfunc, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&bytes, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuFuncSetSharedSize) < 0 ||
        rpc_write(conn, &hfunc, sizeof(CUfunction)) < 0 ||
        rpc_write(conn, &bytes, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hfunc, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&bytes, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuParamSetSize(CUfunction hfunc, unsigned int numbytes)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hfunc, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numbytes, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuParamSetSize) < 0 ||
        rpc_write(conn, &hfunc, sizeof(CUfunction)) < 0 ||
        rpc_write(conn, &numbytes, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hfunc, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numbytes, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuParamSeti(CUfunction hfunc, int offset, unsigned int value)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hfunc, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&offset, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&value, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuParamSeti) < 0 ||
        rpc_write(conn, &hfunc, sizeof(CUfunction)) < 0 ||
        rpc_write(conn, &offset, sizeof(int)) < 0 ||
        rpc_write(conn, &value, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hfunc, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&offset, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&value, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuParamSetf(CUfunction hfunc, int offset, float value)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hfunc, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&offset, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&value, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuParamSetf) < 0 ||
        rpc_write(conn, &hfunc, sizeof(CUfunction)) < 0 ||
        rpc_write(conn, &offset, sizeof(int)) < 0 ||
        rpc_write(conn, &value, sizeof(float)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hfunc, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&offset, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&value, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuLaunch(CUfunction f)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&f, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuLaunch) < 0 ||
        rpc_write(conn, &f, sizeof(CUfunction)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&f, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuLaunchGrid(CUfunction f, int grid_width, int grid_height)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&f, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&grid_width, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&grid_height, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuLaunchGrid) < 0 ||
        rpc_write(conn, &f, sizeof(CUfunction)) < 0 ||
        rpc_write(conn, &grid_width, sizeof(int)) < 0 ||
        rpc_write(conn, &grid_height, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&f, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&grid_width, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&grid_height, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuLaunchGridAsync(CUfunction f, int grid_width, int grid_height, CUstream hStream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&f, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&grid_width, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&grid_height, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuLaunchGridAsync) < 0 ||
        rpc_write(conn, &f, sizeof(CUfunction)) < 0 ||
        rpc_write(conn, &grid_width, sizeof(int)) < 0 ||
        rpc_write(conn, &grid_height, sizeof(int)) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&f, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&grid_width, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&grid_height, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuParamSetTexRef(CUfunction hfunc, int texunit, CUtexref hTexRef)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hfunc, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&texunit, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuParamSetTexRef) < 0 ||
        rpc_write(conn, &hfunc, sizeof(CUfunction)) < 0 ||
        rpc_write(conn, &texunit, sizeof(int)) < 0 ||
        rpc_write(conn, &hTexRef, sizeof(CUtexref)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hfunc, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&texunit, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuFuncSetSharedMemConfig(CUfunction hfunc, CUsharedconfig config)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hfunc, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&config, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuFuncSetSharedMemConfig) < 0 ||
        rpc_write(conn, &hfunc, sizeof(CUfunction)) < 0 ||
        rpc_write(conn, &config, sizeof(CUsharedconfig)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hfunc, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&config, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphCreate(CUgraph* phGraph, unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)phGraph, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphCreate) < 0 ||
        rpc_write(conn, phGraph, sizeof(CUgraph)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, phGraph, sizeof(CUgraph)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)phGraph, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphAddKernelNode_v2(CUgraphNode* phGraphNode, CUgraph hGraph, const CUgraphNode* dependencies, size_t numDependencies, const CUDA_KERNEL_NODE_PARAMS* nodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)phGraphNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphAddKernelNode_v2) < 0 ||
        rpc_write(conn, phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_write(conn, &dependencies, sizeof(const CUgraphNode*)) < 0 ||
        rpc_write(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_write(conn, &nodeParams, sizeof(const CUDA_KERNEL_NODE_PARAMS*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)phGraphNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphKernelNodeGetParams_v2(CUgraphNode hNode, CUDA_KERNEL_NODE_PARAMS* nodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphKernelNodeGetParams_v2) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, nodeParams, sizeof(CUDA_KERNEL_NODE_PARAMS)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, nodeParams, sizeof(CUDA_KERNEL_NODE_PARAMS)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphKernelNodeSetParams_v2(CUgraphNode hNode, const CUDA_KERNEL_NODE_PARAMS* nodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphKernelNodeSetParams_v2) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &nodeParams, sizeof(const CUDA_KERNEL_NODE_PARAMS*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphAddMemcpyNode(CUgraphNode* phGraphNode, CUgraph hGraph, const CUgraphNode* dependencies, size_t numDependencies, const CUDA_MEMCPY3D* copyParams, CUcontext ctx)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)phGraphNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)copyParams, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ctx, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphAddMemcpyNode) < 0 ||
        rpc_write(conn, phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_write(conn, &dependencies, sizeof(const CUgraphNode*)) < 0 ||
        rpc_write(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_write(conn, &copyParams, sizeof(const CUDA_MEMCPY3D*)) < 0 ||
        rpc_write(conn, &ctx, sizeof(CUcontext)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)phGraphNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)copyParams, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ctx, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphMemcpyNodeGetParams(CUgraphNode hNode, CUDA_MEMCPY3D* nodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphMemcpyNodeGetParams) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, nodeParams, sizeof(CUDA_MEMCPY3D)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, nodeParams, sizeof(CUDA_MEMCPY3D)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphMemcpyNodeSetParams(CUgraphNode hNode, const CUDA_MEMCPY3D* nodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphMemcpyNodeSetParams) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &nodeParams, sizeof(const CUDA_MEMCPY3D*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphAddMemsetNode(CUgraphNode* phGraphNode, CUgraph hGraph, const CUgraphNode* dependencies, size_t numDependencies, const CUDA_MEMSET_NODE_PARAMS* memsetParams, CUcontext ctx)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)phGraphNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)memsetParams, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ctx, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphAddMemsetNode) < 0 ||
        rpc_write(conn, phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_write(conn, &dependencies, sizeof(const CUgraphNode*)) < 0 ||
        rpc_write(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_write(conn, &memsetParams, sizeof(const CUDA_MEMSET_NODE_PARAMS*)) < 0 ||
        rpc_write(conn, &ctx, sizeof(CUcontext)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)phGraphNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)memsetParams, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ctx, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphMemsetNodeGetParams(CUgraphNode hNode, CUDA_MEMSET_NODE_PARAMS* nodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphMemsetNodeGetParams) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, nodeParams, sizeof(CUDA_MEMSET_NODE_PARAMS)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, nodeParams, sizeof(CUDA_MEMSET_NODE_PARAMS)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphMemsetNodeSetParams(CUgraphNode hNode, const CUDA_MEMSET_NODE_PARAMS* nodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphMemsetNodeSetParams) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &nodeParams, sizeof(const CUDA_MEMSET_NODE_PARAMS*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphAddHostNode(CUgraphNode* phGraphNode, CUgraph hGraph, const CUgraphNode* dependencies, size_t numDependencies, const CUDA_HOST_NODE_PARAMS* nodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)phGraphNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphAddHostNode) < 0 ||
        rpc_write(conn, phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_write(conn, &dependencies, sizeof(const CUgraphNode*)) < 0 ||
        rpc_write(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_write(conn, &nodeParams, sizeof(const CUDA_HOST_NODE_PARAMS*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)phGraphNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphHostNodeGetParams(CUgraphNode hNode, CUDA_HOST_NODE_PARAMS* nodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphHostNodeGetParams) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, nodeParams, sizeof(CUDA_HOST_NODE_PARAMS)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, nodeParams, sizeof(CUDA_HOST_NODE_PARAMS)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphHostNodeSetParams(CUgraphNode hNode, const CUDA_HOST_NODE_PARAMS* nodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphHostNodeSetParams) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &nodeParams, sizeof(const CUDA_HOST_NODE_PARAMS*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphAddChildGraphNode(CUgraphNode* phGraphNode, CUgraph hGraph, const CUgraphNode* dependencies, size_t numDependencies, CUgraph childGraph)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)phGraphNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&childGraph, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphAddChildGraphNode) < 0 ||
        rpc_write(conn, phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_write(conn, &dependencies, sizeof(const CUgraphNode*)) < 0 ||
        rpc_write(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_write(conn, &childGraph, sizeof(CUgraph)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)phGraphNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&childGraph, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphChildGraphNodeGetGraph(CUgraphNode hNode, CUgraph* phGraph)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)phGraph, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphChildGraphNodeGetGraph) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, phGraph, sizeof(CUgraph)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, phGraph, sizeof(CUgraph)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)phGraph, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphAddEmptyNode(CUgraphNode* phGraphNode, CUgraph hGraph, const CUgraphNode* dependencies, size_t numDependencies)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)phGraphNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphAddEmptyNode) < 0 ||
        rpc_write(conn, phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_write(conn, &dependencies, sizeof(const CUgraphNode*)) < 0 ||
        rpc_write(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)phGraphNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphAddEventRecordNode(CUgraphNode* phGraphNode, CUgraph hGraph, const CUgraphNode* dependencies, size_t numDependencies, CUevent event)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)phGraphNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&event, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphAddEventRecordNode) < 0 ||
        rpc_write(conn, phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_write(conn, &dependencies, sizeof(const CUgraphNode*)) < 0 ||
        rpc_write(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_write(conn, &event, sizeof(CUevent)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)phGraphNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&event, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphEventRecordNodeGetEvent(CUgraphNode hNode, CUevent* event_out)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)event_out, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphEventRecordNodeGetEvent) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, event_out, sizeof(CUevent)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, event_out, sizeof(CUevent)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)event_out, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphEventRecordNodeSetEvent(CUgraphNode hNode, CUevent event)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&event, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphEventRecordNodeSetEvent) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &event, sizeof(CUevent)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&event, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphAddEventWaitNode(CUgraphNode* phGraphNode, CUgraph hGraph, const CUgraphNode* dependencies, size_t numDependencies, CUevent event)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)phGraphNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&event, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphAddEventWaitNode) < 0 ||
        rpc_write(conn, phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_write(conn, &dependencies, sizeof(const CUgraphNode*)) < 0 ||
        rpc_write(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_write(conn, &event, sizeof(CUevent)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)phGraphNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&event, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphEventWaitNodeGetEvent(CUgraphNode hNode, CUevent* event_out)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)event_out, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphEventWaitNodeGetEvent) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, event_out, sizeof(CUevent)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, event_out, sizeof(CUevent)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)event_out, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphEventWaitNodeSetEvent(CUgraphNode hNode, CUevent event)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&event, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphEventWaitNodeSetEvent) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &event, sizeof(CUevent)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&event, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphAddExternalSemaphoresSignalNode(CUgraphNode* phGraphNode, CUgraph hGraph, const CUgraphNode* dependencies, size_t numDependencies, const CUDA_EXT_SEM_SIGNAL_NODE_PARAMS* nodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)phGraphNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphAddExternalSemaphoresSignalNode) < 0 ||
        rpc_write(conn, phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_write(conn, &dependencies, sizeof(const CUgraphNode*)) < 0 ||
        rpc_write(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_write(conn, &nodeParams, sizeof(const CUDA_EXT_SEM_SIGNAL_NODE_PARAMS*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)phGraphNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphExternalSemaphoresSignalNodeGetParams(CUgraphNode hNode, CUDA_EXT_SEM_SIGNAL_NODE_PARAMS* params_out)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)params_out, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphExternalSemaphoresSignalNodeGetParams) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, params_out, sizeof(CUDA_EXT_SEM_SIGNAL_NODE_PARAMS)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, params_out, sizeof(CUDA_EXT_SEM_SIGNAL_NODE_PARAMS)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)params_out, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphExternalSemaphoresSignalNodeSetParams(CUgraphNode hNode, const CUDA_EXT_SEM_SIGNAL_NODE_PARAMS* nodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphExternalSemaphoresSignalNodeSetParams) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &nodeParams, sizeof(const CUDA_EXT_SEM_SIGNAL_NODE_PARAMS*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphAddExternalSemaphoresWaitNode(CUgraphNode* phGraphNode, CUgraph hGraph, const CUgraphNode* dependencies, size_t numDependencies, const CUDA_EXT_SEM_WAIT_NODE_PARAMS* nodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)phGraphNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphAddExternalSemaphoresWaitNode) < 0 ||
        rpc_write(conn, phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_write(conn, &dependencies, sizeof(const CUgraphNode*)) < 0 ||
        rpc_write(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_write(conn, &nodeParams, sizeof(const CUDA_EXT_SEM_WAIT_NODE_PARAMS*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)phGraphNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphExternalSemaphoresWaitNodeGetParams(CUgraphNode hNode, CUDA_EXT_SEM_WAIT_NODE_PARAMS* params_out)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)params_out, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphExternalSemaphoresWaitNodeGetParams) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, params_out, sizeof(CUDA_EXT_SEM_WAIT_NODE_PARAMS)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, params_out, sizeof(CUDA_EXT_SEM_WAIT_NODE_PARAMS)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)params_out, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphExternalSemaphoresWaitNodeSetParams(CUgraphNode hNode, const CUDA_EXT_SEM_WAIT_NODE_PARAMS* nodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphExternalSemaphoresWaitNodeSetParams) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &nodeParams, sizeof(const CUDA_EXT_SEM_WAIT_NODE_PARAMS*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphAddBatchMemOpNode(CUgraphNode* phGraphNode, CUgraph hGraph, const CUgraphNode* dependencies, size_t numDependencies, const CUDA_BATCH_MEM_OP_NODE_PARAMS* nodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)phGraphNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphAddBatchMemOpNode) < 0 ||
        rpc_write(conn, phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_write(conn, &dependencies, sizeof(const CUgraphNode*)) < 0 ||
        rpc_write(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_write(conn, &nodeParams, sizeof(const CUDA_BATCH_MEM_OP_NODE_PARAMS*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)phGraphNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphBatchMemOpNodeGetParams(CUgraphNode hNode, CUDA_BATCH_MEM_OP_NODE_PARAMS* nodeParams_out)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams_out, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphBatchMemOpNodeGetParams) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, nodeParams_out, sizeof(CUDA_BATCH_MEM_OP_NODE_PARAMS)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, nodeParams_out, sizeof(CUDA_BATCH_MEM_OP_NODE_PARAMS)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams_out, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphBatchMemOpNodeSetParams(CUgraphNode hNode, const CUDA_BATCH_MEM_OP_NODE_PARAMS* nodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphBatchMemOpNodeSetParams) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &nodeParams, sizeof(const CUDA_BATCH_MEM_OP_NODE_PARAMS*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphExecBatchMemOpNodeSetParams(CUgraphExec hGraphExec, CUgraphNode hNode, const CUDA_BATCH_MEM_OP_NODE_PARAMS* nodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphExecBatchMemOpNodeSetParams) < 0 ||
        rpc_write(conn, &hGraphExec, sizeof(CUgraphExec)) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &nodeParams, sizeof(const CUDA_BATCH_MEM_OP_NODE_PARAMS*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphAddMemAllocNode(CUgraphNode* phGraphNode, CUgraph hGraph, const CUgraphNode* dependencies, size_t numDependencies, CUDA_MEM_ALLOC_NODE_PARAMS* nodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)phGraphNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphAddMemAllocNode) < 0 ||
        rpc_write(conn, phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_write(conn, &dependencies, sizeof(const CUgraphNode*)) < 0 ||
        rpc_write(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_write(conn, nodeParams, sizeof(CUDA_MEM_ALLOC_NODE_PARAMS)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, nodeParams, sizeof(CUDA_MEM_ALLOC_NODE_PARAMS)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)phGraphNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphMemAllocNodeGetParams(CUgraphNode hNode, CUDA_MEM_ALLOC_NODE_PARAMS* params_out)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)params_out, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphMemAllocNodeGetParams) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, params_out, sizeof(CUDA_MEM_ALLOC_NODE_PARAMS)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, params_out, sizeof(CUDA_MEM_ALLOC_NODE_PARAMS)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)params_out, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphAddMemFreeNode(CUgraphNode* phGraphNode, CUgraph hGraph, const CUgraphNode* dependencies, size_t numDependencies, CUdeviceptr dptr)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)phGraphNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    for (int i = 0; i < static_cast<int>(numDependencies) && is_unified_pointer(conn, (void*)dependencies); i++)
      if (maybe_copy_unified_arg(conn, (void*)&dependencies[i], cudaMemcpyHostToDevice) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dptr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphAddMemFreeNode) < 0 ||
        rpc_write(conn, phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_write(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_write(conn, dependencies, numDependencies * sizeof(const CUgraphNode)) < 0 ||
        rpc_write(conn, &dptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)phGraphNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    for (int i = 0; i < static_cast<int>(numDependencies) && is_unified_pointer(conn, (void*)dependencies); i++)
      if (maybe_copy_unified_arg(conn, (void*)&dependencies[i], cudaMemcpyDeviceToHost) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dptr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphMemFreeNodeGetParams(CUgraphNode hNode, CUdeviceptr* dptr_out)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dptr_out, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphMemFreeNodeGetParams) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, dptr_out, sizeof(CUdeviceptr)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, dptr_out, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dptr_out, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuDeviceGraphMemTrim(CUdevice device)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuDeviceGraphMemTrim) < 0 ||
        rpc_write(conn, &device, sizeof(CUdevice)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphClone(CUgraph* phGraphClone, CUgraph originalGraph)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)phGraphClone, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&originalGraph, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphClone) < 0 ||
        rpc_write(conn, phGraphClone, sizeof(CUgraph)) < 0 ||
        rpc_write(conn, &originalGraph, sizeof(CUgraph)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, phGraphClone, sizeof(CUgraph)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)phGraphClone, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&originalGraph, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphNodeFindInClone(CUgraphNode* phNode, CUgraphNode hOriginalNode, CUgraph hClonedGraph)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)phNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hOriginalNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hClonedGraph, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphNodeFindInClone) < 0 ||
        rpc_write(conn, phNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &hOriginalNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &hClonedGraph, sizeof(CUgraph)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, phNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)phNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hOriginalNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hClonedGraph, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphNodeGetType(CUgraphNode hNode, CUgraphNodeType* type)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)type, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphNodeGetType) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, type, sizeof(CUgraphNodeType)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, type, sizeof(CUgraphNodeType)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)type, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphGetNodes(CUgraph hGraph, CUgraphNode* nodes, size_t* numNodes)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodes, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)numNodes, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphGetNodes) < 0 ||
        rpc_write(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_write(conn, nodes, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, numNodes, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, nodes, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, numNodes, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodes, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)numNodes, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphGetRootNodes(CUgraph hGraph, CUgraphNode* rootNodes, size_t* numRootNodes)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)rootNodes, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)numRootNodes, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphGetRootNodes) < 0 ||
        rpc_write(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_write(conn, rootNodes, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, numRootNodes, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, rootNodes, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, numRootNodes, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)rootNodes, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)numRootNodes, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphGetEdges(CUgraph hGraph, CUgraphNode* from, CUgraphNode* to, size_t* numEdges)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)from, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)to, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)numEdges, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphGetEdges) < 0 ||
        rpc_write(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_write(conn, from, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, to, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, numEdges, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, from, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, to, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, numEdges, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)from, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)to, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)numEdges, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphGetEdges_v2(CUgraph hGraph, CUgraphNode* from, CUgraphNode* to, CUgraphEdgeData* edgeData, size_t* numEdges)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)from, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)to, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)edgeData, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)numEdges, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphGetEdges_v2) < 0 ||
        rpc_write(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_write(conn, from, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, to, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, edgeData, sizeof(CUgraphEdgeData)) < 0 ||
        rpc_write(conn, numEdges, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, from, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, to, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, edgeData, sizeof(CUgraphEdgeData)) < 0 ||
        rpc_read(conn, numEdges, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)from, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)to, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)edgeData, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)numEdges, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphNodeGetDependencies(CUgraphNode hNode, CUgraphNode* dependencies, size_t* numDependencies)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)numDependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphNodeGetDependencies) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, dependencies, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, numDependencies, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, dependencies, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, numDependencies, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)numDependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphNodeGetDependencies_v2(CUgraphNode hNode, CUgraphNode* dependencies, CUgraphEdgeData* edgeData, size_t* numDependencies)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)edgeData, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)numDependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphNodeGetDependencies_v2) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, dependencies, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, edgeData, sizeof(CUgraphEdgeData)) < 0 ||
        rpc_write(conn, numDependencies, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, dependencies, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, edgeData, sizeof(CUgraphEdgeData)) < 0 ||
        rpc_read(conn, numDependencies, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)edgeData, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)numDependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphNodeGetDependentNodes(CUgraphNode hNode, CUgraphNode* dependentNodes, size_t* numDependentNodes)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependentNodes, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)numDependentNodes, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphNodeGetDependentNodes) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, dependentNodes, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, numDependentNodes, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, dependentNodes, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, numDependentNodes, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependentNodes, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)numDependentNodes, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphNodeGetDependentNodes_v2(CUgraphNode hNode, CUgraphNode* dependentNodes, CUgraphEdgeData* edgeData, size_t* numDependentNodes)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependentNodes, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)edgeData, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)numDependentNodes, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphNodeGetDependentNodes_v2) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, dependentNodes, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, edgeData, sizeof(CUgraphEdgeData)) < 0 ||
        rpc_write(conn, numDependentNodes, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, dependentNodes, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, edgeData, sizeof(CUgraphEdgeData)) < 0 ||
        rpc_read(conn, numDependentNodes, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependentNodes, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)edgeData, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)numDependentNodes, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphAddDependencies(CUgraph hGraph, const CUgraphNode* from, const CUgraphNode* to, size_t numDependencies)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)from, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)to, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphAddDependencies) < 0 ||
        rpc_write(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_write(conn, &from, sizeof(const CUgraphNode*)) < 0 ||
        rpc_write(conn, &to, sizeof(const CUgraphNode*)) < 0 ||
        rpc_write(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)from, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)to, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphAddDependencies_v2(CUgraph hGraph, const CUgraphNode* from, const CUgraphNode* to, const CUgraphEdgeData* edgeData, size_t numDependencies)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)from, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)to, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)edgeData, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphAddDependencies_v2) < 0 ||
        rpc_write(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_write(conn, &from, sizeof(const CUgraphNode*)) < 0 ||
        rpc_write(conn, &to, sizeof(const CUgraphNode*)) < 0 ||
        rpc_write(conn, &edgeData, sizeof(const CUgraphEdgeData*)) < 0 ||
        rpc_write(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)from, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)to, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)edgeData, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphRemoveDependencies(CUgraph hGraph, const CUgraphNode* from, const CUgraphNode* to, size_t numDependencies)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)from, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)to, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphRemoveDependencies) < 0 ||
        rpc_write(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_write(conn, &from, sizeof(const CUgraphNode*)) < 0 ||
        rpc_write(conn, &to, sizeof(const CUgraphNode*)) < 0 ||
        rpc_write(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)from, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)to, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphRemoveDependencies_v2(CUgraph hGraph, const CUgraphNode* from, const CUgraphNode* to, const CUgraphEdgeData* edgeData, size_t numDependencies)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)from, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)to, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)edgeData, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphRemoveDependencies_v2) < 0 ||
        rpc_write(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_write(conn, &from, sizeof(const CUgraphNode*)) < 0 ||
        rpc_write(conn, &to, sizeof(const CUgraphNode*)) < 0 ||
        rpc_write(conn, &edgeData, sizeof(const CUgraphEdgeData*)) < 0 ||
        rpc_write(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)from, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)to, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)edgeData, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphDestroyNode(CUgraphNode hNode)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphDestroyNode) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphInstantiateWithFlags(CUgraphExec* phGraphExec, CUgraph hGraph, unsigned long long flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)phGraphExec, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphInstantiateWithFlags) < 0 ||
        rpc_write(conn, phGraphExec, sizeof(CUgraphExec)) < 0 ||
        rpc_write(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned long long)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, phGraphExec, sizeof(CUgraphExec)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)phGraphExec, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphInstantiateWithParams(CUgraphExec* phGraphExec, CUgraph hGraph, CUDA_GRAPH_INSTANTIATE_PARAMS* instantiateParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)phGraphExec, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)instantiateParams, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphInstantiateWithParams) < 0 ||
        rpc_write(conn, phGraphExec, sizeof(CUgraphExec)) < 0 ||
        rpc_write(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_write(conn, instantiateParams, sizeof(CUDA_GRAPH_INSTANTIATE_PARAMS)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, phGraphExec, sizeof(CUgraphExec)) < 0 ||
        rpc_read(conn, instantiateParams, sizeof(CUDA_GRAPH_INSTANTIATE_PARAMS)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)phGraphExec, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)instantiateParams, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphExecGetFlags(CUgraphExec hGraphExec, cuuint64_t* flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphExecGetFlags) < 0 ||
        rpc_write(conn, &hGraphExec, sizeof(CUgraphExec)) < 0 ||
        rpc_write(conn, flags, sizeof(cuuint64_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, flags, sizeof(cuuint64_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphExecKernelNodeSetParams_v2(CUgraphExec hGraphExec, CUgraphNode hNode, const CUDA_KERNEL_NODE_PARAMS* nodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphExecKernelNodeSetParams_v2) < 0 ||
        rpc_write(conn, &hGraphExec, sizeof(CUgraphExec)) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &nodeParams, sizeof(const CUDA_KERNEL_NODE_PARAMS*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphExecMemcpyNodeSetParams(CUgraphExec hGraphExec, CUgraphNode hNode, const CUDA_MEMCPY3D* copyParams, CUcontext ctx)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)copyParams, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ctx, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphExecMemcpyNodeSetParams) < 0 ||
        rpc_write(conn, &hGraphExec, sizeof(CUgraphExec)) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &copyParams, sizeof(const CUDA_MEMCPY3D*)) < 0 ||
        rpc_write(conn, &ctx, sizeof(CUcontext)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)copyParams, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ctx, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphExecMemsetNodeSetParams(CUgraphExec hGraphExec, CUgraphNode hNode, const CUDA_MEMSET_NODE_PARAMS* memsetParams, CUcontext ctx)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)memsetParams, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ctx, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphExecMemsetNodeSetParams) < 0 ||
        rpc_write(conn, &hGraphExec, sizeof(CUgraphExec)) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &memsetParams, sizeof(const CUDA_MEMSET_NODE_PARAMS*)) < 0 ||
        rpc_write(conn, &ctx, sizeof(CUcontext)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)memsetParams, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ctx, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphExecHostNodeSetParams(CUgraphExec hGraphExec, CUgraphNode hNode, const CUDA_HOST_NODE_PARAMS* nodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphExecHostNodeSetParams) < 0 ||
        rpc_write(conn, &hGraphExec, sizeof(CUgraphExec)) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &nodeParams, sizeof(const CUDA_HOST_NODE_PARAMS*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphExecChildGraphNodeSetParams(CUgraphExec hGraphExec, CUgraphNode hNode, CUgraph childGraph)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&childGraph, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphExecChildGraphNodeSetParams) < 0 ||
        rpc_write(conn, &hGraphExec, sizeof(CUgraphExec)) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &childGraph, sizeof(CUgraph)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&childGraph, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphExecEventRecordNodeSetEvent(CUgraphExec hGraphExec, CUgraphNode hNode, CUevent event)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&event, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphExecEventRecordNodeSetEvent) < 0 ||
        rpc_write(conn, &hGraphExec, sizeof(CUgraphExec)) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &event, sizeof(CUevent)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&event, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphExecEventWaitNodeSetEvent(CUgraphExec hGraphExec, CUgraphNode hNode, CUevent event)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&event, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphExecEventWaitNodeSetEvent) < 0 ||
        rpc_write(conn, &hGraphExec, sizeof(CUgraphExec)) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &event, sizeof(CUevent)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&event, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphExecExternalSemaphoresSignalNodeSetParams(CUgraphExec hGraphExec, CUgraphNode hNode, const CUDA_EXT_SEM_SIGNAL_NODE_PARAMS* nodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphExecExternalSemaphoresSignalNodeSetParams) < 0 ||
        rpc_write(conn, &hGraphExec, sizeof(CUgraphExec)) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &nodeParams, sizeof(const CUDA_EXT_SEM_SIGNAL_NODE_PARAMS*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphExecExternalSemaphoresWaitNodeSetParams(CUgraphExec hGraphExec, CUgraphNode hNode, const CUDA_EXT_SEM_WAIT_NODE_PARAMS* nodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphExecExternalSemaphoresWaitNodeSetParams) < 0 ||
        rpc_write(conn, &hGraphExec, sizeof(CUgraphExec)) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &nodeParams, sizeof(const CUDA_EXT_SEM_WAIT_NODE_PARAMS*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphNodeSetEnabled(CUgraphExec hGraphExec, CUgraphNode hNode, unsigned int isEnabled)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&isEnabled, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphNodeSetEnabled) < 0 ||
        rpc_write(conn, &hGraphExec, sizeof(CUgraphExec)) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &isEnabled, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&isEnabled, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphNodeGetEnabled(CUgraphExec hGraphExec, CUgraphNode hNode, unsigned int* isEnabled)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)isEnabled, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphNodeGetEnabled) < 0 ||
        rpc_write(conn, &hGraphExec, sizeof(CUgraphExec)) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, isEnabled, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, isEnabled, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)isEnabled, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphUpload(CUgraphExec hGraphExec, CUstream hStream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphUpload) < 0 ||
        rpc_write(conn, &hGraphExec, sizeof(CUgraphExec)) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphLaunch(CUgraphExec hGraphExec, CUstream hStream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphLaunch) < 0 ||
        rpc_write(conn, &hGraphExec, sizeof(CUgraphExec)) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphExecDestroy(CUgraphExec hGraphExec)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphExecDestroy) < 0 ||
        rpc_write(conn, &hGraphExec, sizeof(CUgraphExec)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphDestroy(CUgraph hGraph)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphDestroy) < 0 ||
        rpc_write(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphExecUpdate_v2(CUgraphExec hGraphExec, CUgraph hGraph, CUgraphExecUpdateResultInfo* resultInfo)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)resultInfo, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphExecUpdate_v2) < 0 ||
        rpc_write(conn, &hGraphExec, sizeof(CUgraphExec)) < 0 ||
        rpc_write(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_write(conn, resultInfo, sizeof(CUgraphExecUpdateResultInfo)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, resultInfo, sizeof(CUgraphExecUpdateResultInfo)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)resultInfo, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphKernelNodeCopyAttributes(CUgraphNode dst, CUgraphNode src)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&dst, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&src, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphKernelNodeCopyAttributes) < 0 ||
        rpc_write(conn, &dst, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &src, sizeof(CUgraphNode)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dst, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&src, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphKernelNodeGetAttribute(CUgraphNode hNode, CUkernelNodeAttrID attr, CUkernelNodeAttrValue* value_out)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&attr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)value_out, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphKernelNodeGetAttribute) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &attr, sizeof(CUkernelNodeAttrID)) < 0 ||
        rpc_write(conn, value_out, sizeof(CUkernelNodeAttrValue)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, value_out, sizeof(CUkernelNodeAttrValue)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&attr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)value_out, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphKernelNodeSetAttribute(CUgraphNode hNode, CUkernelNodeAttrID attr, const CUkernelNodeAttrValue* value)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&attr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)value, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphKernelNodeSetAttribute) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &attr, sizeof(CUkernelNodeAttrID)) < 0 ||
        rpc_write(conn, &value, sizeof(const CUkernelNodeAttrValue*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&attr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)value, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphDebugDotPrint(CUgraph hGraph, const char* path, unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)path, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphDebugDotPrint) < 0 ||
        rpc_write(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_write(conn, &path, sizeof(const char*)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)path, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuUserObjectRetain(CUuserObject object, unsigned int count)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&object, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuUserObjectRetain) < 0 ||
        rpc_write(conn, &object, sizeof(CUuserObject)) < 0 ||
        rpc_write(conn, &count, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&object, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuUserObjectRelease(CUuserObject object, unsigned int count)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&object, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuUserObjectRelease) < 0 ||
        rpc_write(conn, &object, sizeof(CUuserObject)) < 0 ||
        rpc_write(conn, &count, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&object, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphRetainUserObject(CUgraph graph, CUuserObject object, unsigned int count, unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&object, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphRetainUserObject) < 0 ||
        rpc_write(conn, &graph, sizeof(CUgraph)) < 0 ||
        rpc_write(conn, &object, sizeof(CUuserObject)) < 0 ||
        rpc_write(conn, &count, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&object, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphReleaseUserObject(CUgraph graph, CUuserObject object, unsigned int count)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&object, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphReleaseUserObject) < 0 ||
        rpc_write(conn, &graph, sizeof(CUgraph)) < 0 ||
        rpc_write(conn, &object, sizeof(CUuserObject)) < 0 ||
        rpc_write(conn, &count, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&object, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphAddNode(CUgraphNode* phGraphNode, CUgraph hGraph, const CUgraphNode* dependencies, size_t numDependencies, CUgraphNodeParams* nodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)phGraphNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphAddNode) < 0 ||
        rpc_write(conn, phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_write(conn, &dependencies, sizeof(const CUgraphNode*)) < 0 ||
        rpc_write(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_write(conn, nodeParams, sizeof(CUgraphNodeParams)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, nodeParams, sizeof(CUgraphNodeParams)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)phGraphNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphAddNode_v2(CUgraphNode* phGraphNode, CUgraph hGraph, const CUgraphNode* dependencies, const CUgraphEdgeData* dependencyData, size_t numDependencies, CUgraphNodeParams* nodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)phGraphNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencyData, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphAddNode_v2) < 0 ||
        rpc_write(conn, phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_write(conn, &dependencies, sizeof(const CUgraphNode*)) < 0 ||
        rpc_write(conn, &dependencyData, sizeof(const CUgraphEdgeData*)) < 0 ||
        rpc_write(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_write(conn, nodeParams, sizeof(CUgraphNodeParams)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, phGraphNode, sizeof(CUgraphNode)) < 0 ||
        rpc_read(conn, nodeParams, sizeof(CUgraphNodeParams)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)phGraphNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dependencyData, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphNodeSetParams(CUgraphNode hNode, CUgraphNodeParams* nodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphNodeSetParams) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, nodeParams, sizeof(CUgraphNodeParams)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, nodeParams, sizeof(CUgraphNodeParams)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphExecNodeSetParams(CUgraphExec hGraphExec, CUgraphNode hNode, CUgraphNodeParams* nodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphExecNodeSetParams) < 0 ||
        rpc_write(conn, &hGraphExec, sizeof(CUgraphExec)) < 0 ||
        rpc_write(conn, &hNode, sizeof(CUgraphNode)) < 0 ||
        rpc_write(conn, nodeParams, sizeof(CUgraphNodeParams)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, nodeParams, sizeof(CUgraphNodeParams)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphConditionalHandleCreate(CUgraphConditionalHandle* pHandle_out, CUgraph hGraph, CUcontext ctx, unsigned int defaultLaunchValue, unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pHandle_out, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ctx, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&defaultLaunchValue, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphConditionalHandleCreate) < 0 ||
        rpc_write(conn, pHandle_out, sizeof(CUgraphConditionalHandle)) < 0 ||
        rpc_write(conn, &hGraph, sizeof(CUgraph)) < 0 ||
        rpc_write(conn, &ctx, sizeof(CUcontext)) < 0 ||
        rpc_write(conn, &defaultLaunchValue, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pHandle_out, sizeof(CUgraphConditionalHandle)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pHandle_out, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&ctx, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&defaultLaunchValue, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuOccupancyMaxActiveBlocksPerMultiprocessor(int* numBlocks, CUfunction func, int blockSize, size_t dynamicSMemSize)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)numBlocks, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&func, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&blockSize, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dynamicSMemSize, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuOccupancyMaxActiveBlocksPerMultiprocessor) < 0 ||
        rpc_write(conn, numBlocks, sizeof(int)) < 0 ||
        rpc_write(conn, &func, sizeof(CUfunction)) < 0 ||
        rpc_write(conn, &blockSize, sizeof(int)) < 0 ||
        rpc_write(conn, &dynamicSMemSize, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, numBlocks, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)numBlocks, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&func, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&blockSize, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dynamicSMemSize, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuOccupancyMaxActiveBlocksPerMultiprocessorWithFlags(int* numBlocks, CUfunction func, int blockSize, size_t dynamicSMemSize, unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)numBlocks, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&func, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&blockSize, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dynamicSMemSize, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuOccupancyMaxActiveBlocksPerMultiprocessorWithFlags) < 0 ||
        rpc_write(conn, numBlocks, sizeof(int)) < 0 ||
        rpc_write(conn, &func, sizeof(CUfunction)) < 0 ||
        rpc_write(conn, &blockSize, sizeof(int)) < 0 ||
        rpc_write(conn, &dynamicSMemSize, sizeof(size_t)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, numBlocks, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)numBlocks, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&func, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&blockSize, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dynamicSMemSize, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuOccupancyAvailableDynamicSMemPerBlock(size_t* dynamicSmemSize, CUfunction func, int numBlocks, int blockSize)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)dynamicSmemSize, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&func, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numBlocks, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&blockSize, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuOccupancyAvailableDynamicSMemPerBlock) < 0 ||
        rpc_write(conn, dynamicSmemSize, sizeof(size_t)) < 0 ||
        rpc_write(conn, &func, sizeof(CUfunction)) < 0 ||
        rpc_write(conn, &numBlocks, sizeof(int)) < 0 ||
        rpc_write(conn, &blockSize, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, dynamicSmemSize, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)dynamicSmemSize, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&func, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&numBlocks, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&blockSize, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuOccupancyMaxPotentialClusterSize(int* clusterSize, CUfunction func, const CUlaunchConfig* config)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)clusterSize, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&func, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)config, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuOccupancyMaxPotentialClusterSize) < 0 ||
        rpc_write(conn, clusterSize, sizeof(int)) < 0 ||
        rpc_write(conn, &func, sizeof(CUfunction)) < 0 ||
        rpc_write(conn, &config, sizeof(const CUlaunchConfig*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, clusterSize, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)clusterSize, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&func, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)config, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuOccupancyMaxActiveClusters(int* numClusters, CUfunction func, const CUlaunchConfig* config)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)numClusters, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&func, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)config, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuOccupancyMaxActiveClusters) < 0 ||
        rpc_write(conn, numClusters, sizeof(int)) < 0 ||
        rpc_write(conn, &func, sizeof(CUfunction)) < 0 ||
        rpc_write(conn, &config, sizeof(const CUlaunchConfig*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, numClusters, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)numClusters, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&func, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)config, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuTexRefSetArray(CUtexref hTexRef, CUarray hArray, unsigned int Flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hArray, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuTexRefSetArray) < 0 ||
        rpc_write(conn, &hTexRef, sizeof(CUtexref)) < 0 ||
        rpc_write(conn, &hArray, sizeof(CUarray)) < 0 ||
        rpc_write(conn, &Flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hArray, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuTexRefSetMipmappedArray(CUtexref hTexRef, CUmipmappedArray hMipmappedArray, unsigned int Flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hMipmappedArray, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuTexRefSetMipmappedArray) < 0 ||
        rpc_write(conn, &hTexRef, sizeof(CUtexref)) < 0 ||
        rpc_write(conn, &hMipmappedArray, sizeof(CUmipmappedArray)) < 0 ||
        rpc_write(conn, &Flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hMipmappedArray, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuTexRefSetAddress_v2(size_t* ByteOffset, CUtexref hTexRef, CUdeviceptr dptr, size_t bytes)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)ByteOffset, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dptr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&bytes, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuTexRefSetAddress_v2) < 0 ||
        rpc_write(conn, ByteOffset, sizeof(size_t)) < 0 ||
        rpc_write(conn, &hTexRef, sizeof(CUtexref)) < 0 ||
        rpc_write(conn, &dptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &bytes, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, ByteOffset, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)ByteOffset, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dptr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&bytes, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuTexRefSetAddress2D_v3(CUtexref hTexRef, const CUDA_ARRAY_DESCRIPTOR* desc, CUdeviceptr dptr, size_t Pitch)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)desc, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dptr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Pitch, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuTexRefSetAddress2D_v3) < 0 ||
        rpc_write(conn, &hTexRef, sizeof(CUtexref)) < 0 ||
        rpc_write(conn, &desc, sizeof(const CUDA_ARRAY_DESCRIPTOR*)) < 0 ||
        rpc_write(conn, &dptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &Pitch, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)desc, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dptr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Pitch, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuTexRefSetFormat(CUtexref hTexRef, CUarray_format fmt, int NumPackedComponents)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&fmt, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&NumPackedComponents, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuTexRefSetFormat) < 0 ||
        rpc_write(conn, &hTexRef, sizeof(CUtexref)) < 0 ||
        rpc_write(conn, &fmt, sizeof(CUarray_format)) < 0 ||
        rpc_write(conn, &NumPackedComponents, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&fmt, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&NumPackedComponents, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuTexRefSetAddressMode(CUtexref hTexRef, int dim, CUaddress_mode am)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dim, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&am, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuTexRefSetAddressMode) < 0 ||
        rpc_write(conn, &hTexRef, sizeof(CUtexref)) < 0 ||
        rpc_write(conn, &dim, sizeof(int)) < 0 ||
        rpc_write(conn, &am, sizeof(CUaddress_mode)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dim, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&am, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuTexRefSetFilterMode(CUtexref hTexRef, CUfilter_mode fm)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&fm, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuTexRefSetFilterMode) < 0 ||
        rpc_write(conn, &hTexRef, sizeof(CUtexref)) < 0 ||
        rpc_write(conn, &fm, sizeof(CUfilter_mode)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&fm, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuTexRefSetMipmapFilterMode(CUtexref hTexRef, CUfilter_mode fm)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&fm, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuTexRefSetMipmapFilterMode) < 0 ||
        rpc_write(conn, &hTexRef, sizeof(CUtexref)) < 0 ||
        rpc_write(conn, &fm, sizeof(CUfilter_mode)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&fm, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuTexRefSetMipmapLevelBias(CUtexref hTexRef, float bias)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&bias, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuTexRefSetMipmapLevelBias) < 0 ||
        rpc_write(conn, &hTexRef, sizeof(CUtexref)) < 0 ||
        rpc_write(conn, &bias, sizeof(float)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&bias, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuTexRefSetMipmapLevelClamp(CUtexref hTexRef, float minMipmapLevelClamp, float maxMipmapLevelClamp)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&minMipmapLevelClamp, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&maxMipmapLevelClamp, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuTexRefSetMipmapLevelClamp) < 0 ||
        rpc_write(conn, &hTexRef, sizeof(CUtexref)) < 0 ||
        rpc_write(conn, &minMipmapLevelClamp, sizeof(float)) < 0 ||
        rpc_write(conn, &maxMipmapLevelClamp, sizeof(float)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&minMipmapLevelClamp, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&maxMipmapLevelClamp, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuTexRefSetMaxAnisotropy(CUtexref hTexRef, unsigned int maxAniso)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&maxAniso, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuTexRefSetMaxAnisotropy) < 0 ||
        rpc_write(conn, &hTexRef, sizeof(CUtexref)) < 0 ||
        rpc_write(conn, &maxAniso, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&maxAniso, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuTexRefSetBorderColor(CUtexref hTexRef, float* pBorderColor)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pBorderColor, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuTexRefSetBorderColor) < 0 ||
        rpc_write(conn, &hTexRef, sizeof(CUtexref)) < 0 ||
        rpc_write(conn, pBorderColor, sizeof(float)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pBorderColor, sizeof(float)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pBorderColor, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuTexRefSetFlags(CUtexref hTexRef, unsigned int Flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuTexRefSetFlags) < 0 ||
        rpc_write(conn, &hTexRef, sizeof(CUtexref)) < 0 ||
        rpc_write(conn, &Flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuTexRefGetAddress_v2(CUdeviceptr* pdptr, CUtexref hTexRef)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pdptr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuTexRefGetAddress_v2) < 0 ||
        rpc_write(conn, pdptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, &hTexRef, sizeof(CUtexref)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pdptr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pdptr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuTexRefGetArray(CUarray* phArray, CUtexref hTexRef)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)phArray, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuTexRefGetArray) < 0 ||
        rpc_write(conn, phArray, sizeof(CUarray)) < 0 ||
        rpc_write(conn, &hTexRef, sizeof(CUtexref)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, phArray, sizeof(CUarray)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)phArray, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuTexRefGetMipmappedArray(CUmipmappedArray* phMipmappedArray, CUtexref hTexRef)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)phMipmappedArray, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuTexRefGetMipmappedArray) < 0 ||
        rpc_write(conn, phMipmappedArray, sizeof(CUmipmappedArray)) < 0 ||
        rpc_write(conn, &hTexRef, sizeof(CUtexref)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, phMipmappedArray, sizeof(CUmipmappedArray)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)phMipmappedArray, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuTexRefGetAddressMode(CUaddress_mode* pam, CUtexref hTexRef, int dim)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pam, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dim, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuTexRefGetAddressMode) < 0 ||
        rpc_write(conn, pam, sizeof(CUaddress_mode)) < 0 ||
        rpc_write(conn, &hTexRef, sizeof(CUtexref)) < 0 ||
        rpc_write(conn, &dim, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pam, sizeof(CUaddress_mode)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pam, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dim, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuTexRefGetFilterMode(CUfilter_mode* pfm, CUtexref hTexRef)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pfm, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuTexRefGetFilterMode) < 0 ||
        rpc_write(conn, pfm, sizeof(CUfilter_mode)) < 0 ||
        rpc_write(conn, &hTexRef, sizeof(CUtexref)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pfm, sizeof(CUfilter_mode)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pfm, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuTexRefGetFormat(CUarray_format* pFormat, int* pNumChannels, CUtexref hTexRef)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pFormat, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pNumChannels, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuTexRefGetFormat) < 0 ||
        rpc_write(conn, pFormat, sizeof(CUarray_format)) < 0 ||
        rpc_write(conn, pNumChannels, sizeof(int)) < 0 ||
        rpc_write(conn, &hTexRef, sizeof(CUtexref)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pFormat, sizeof(CUarray_format)) < 0 ||
        rpc_read(conn, pNumChannels, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pFormat, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pNumChannels, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuTexRefGetMipmapFilterMode(CUfilter_mode* pfm, CUtexref hTexRef)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pfm, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuTexRefGetMipmapFilterMode) < 0 ||
        rpc_write(conn, pfm, sizeof(CUfilter_mode)) < 0 ||
        rpc_write(conn, &hTexRef, sizeof(CUtexref)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pfm, sizeof(CUfilter_mode)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pfm, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuTexRefGetMipmapLevelBias(float* pbias, CUtexref hTexRef)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pbias, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuTexRefGetMipmapLevelBias) < 0 ||
        rpc_write(conn, pbias, sizeof(float)) < 0 ||
        rpc_write(conn, &hTexRef, sizeof(CUtexref)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pbias, sizeof(float)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pbias, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuTexRefGetMipmapLevelClamp(float* pminMipmapLevelClamp, float* pmaxMipmapLevelClamp, CUtexref hTexRef)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pminMipmapLevelClamp, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pmaxMipmapLevelClamp, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuTexRefGetMipmapLevelClamp) < 0 ||
        rpc_write(conn, pminMipmapLevelClamp, sizeof(float)) < 0 ||
        rpc_write(conn, pmaxMipmapLevelClamp, sizeof(float)) < 0 ||
        rpc_write(conn, &hTexRef, sizeof(CUtexref)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pminMipmapLevelClamp, sizeof(float)) < 0 ||
        rpc_read(conn, pmaxMipmapLevelClamp, sizeof(float)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pminMipmapLevelClamp, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pmaxMipmapLevelClamp, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuTexRefGetMaxAnisotropy(int* pmaxAniso, CUtexref hTexRef)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pmaxAniso, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuTexRefGetMaxAnisotropy) < 0 ||
        rpc_write(conn, pmaxAniso, sizeof(int)) < 0 ||
        rpc_write(conn, &hTexRef, sizeof(CUtexref)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pmaxAniso, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pmaxAniso, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuTexRefGetBorderColor(float* pBorderColor, CUtexref hTexRef)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pBorderColor, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuTexRefGetBorderColor) < 0 ||
        rpc_write(conn, pBorderColor, sizeof(float)) < 0 ||
        rpc_write(conn, &hTexRef, sizeof(CUtexref)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pBorderColor, sizeof(float)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pBorderColor, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuTexRefGetFlags(unsigned int* pFlags, CUtexref hTexRef)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pFlags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuTexRefGetFlags) < 0 ||
        rpc_write(conn, pFlags, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &hTexRef, sizeof(CUtexref)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pFlags, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pFlags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuTexRefCreate(CUtexref* pTexRef)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pTexRef, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuTexRefCreate) < 0 ||
        rpc_write(conn, pTexRef, sizeof(CUtexref)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pTexRef, sizeof(CUtexref)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pTexRef, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuTexRefDestroy(CUtexref hTexRef)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuTexRefDestroy) < 0 ||
        rpc_write(conn, &hTexRef, sizeof(CUtexref)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hTexRef, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuSurfRefSetArray(CUsurfref hSurfRef, CUarray hArray, unsigned int Flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hSurfRef, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hArray, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuSurfRefSetArray) < 0 ||
        rpc_write(conn, &hSurfRef, sizeof(CUsurfref)) < 0 ||
        rpc_write(conn, &hArray, sizeof(CUarray)) < 0 ||
        rpc_write(conn, &Flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hSurfRef, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hArray, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuSurfRefGetArray(CUarray* phArray, CUsurfref hSurfRef)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)phArray, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hSurfRef, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuSurfRefGetArray) < 0 ||
        rpc_write(conn, phArray, sizeof(CUarray)) < 0 ||
        rpc_write(conn, &hSurfRef, sizeof(CUsurfref)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, phArray, sizeof(CUarray)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)phArray, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hSurfRef, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuTexObjectCreate(CUtexObject* pTexObject, const CUDA_RESOURCE_DESC* pResDesc, const CUDA_TEXTURE_DESC* pTexDesc, const CUDA_RESOURCE_VIEW_DESC* pResViewDesc)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pTexObject, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pResDesc, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pTexDesc, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pResViewDesc, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuTexObjectCreate) < 0 ||
        rpc_write(conn, pTexObject, sizeof(CUtexObject)) < 0 ||
        rpc_write(conn, &pResDesc, sizeof(const CUDA_RESOURCE_DESC*)) < 0 ||
        rpc_write(conn, &pTexDesc, sizeof(const CUDA_TEXTURE_DESC*)) < 0 ||
        rpc_write(conn, &pResViewDesc, sizeof(const CUDA_RESOURCE_VIEW_DESC*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pTexObject, sizeof(CUtexObject)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pTexObject, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pResDesc, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pTexDesc, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pResViewDesc, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuTexObjectDestroy(CUtexObject texObject)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&texObject, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuTexObjectDestroy) < 0 ||
        rpc_write(conn, &texObject, sizeof(CUtexObject)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&texObject, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuTexObjectGetResourceDesc(CUDA_RESOURCE_DESC* pResDesc, CUtexObject texObject)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pResDesc, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&texObject, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuTexObjectGetResourceDesc) < 0 ||
        rpc_write(conn, pResDesc, sizeof(CUDA_RESOURCE_DESC)) < 0 ||
        rpc_write(conn, &texObject, sizeof(CUtexObject)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pResDesc, sizeof(CUDA_RESOURCE_DESC)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pResDesc, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&texObject, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuTexObjectGetTextureDesc(CUDA_TEXTURE_DESC* pTexDesc, CUtexObject texObject)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pTexDesc, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&texObject, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuTexObjectGetTextureDesc) < 0 ||
        rpc_write(conn, pTexDesc, sizeof(CUDA_TEXTURE_DESC)) < 0 ||
        rpc_write(conn, &texObject, sizeof(CUtexObject)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pTexDesc, sizeof(CUDA_TEXTURE_DESC)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pTexDesc, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&texObject, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuTexObjectGetResourceViewDesc(CUDA_RESOURCE_VIEW_DESC* pResViewDesc, CUtexObject texObject)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pResViewDesc, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&texObject, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuTexObjectGetResourceViewDesc) < 0 ||
        rpc_write(conn, pResViewDesc, sizeof(CUDA_RESOURCE_VIEW_DESC)) < 0 ||
        rpc_write(conn, &texObject, sizeof(CUtexObject)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pResViewDesc, sizeof(CUDA_RESOURCE_VIEW_DESC)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pResViewDesc, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&texObject, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuSurfObjectCreate(CUsurfObject* pSurfObject, const CUDA_RESOURCE_DESC* pResDesc)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pSurfObject, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pResDesc, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuSurfObjectCreate) < 0 ||
        rpc_write(conn, pSurfObject, sizeof(CUsurfObject)) < 0 ||
        rpc_write(conn, &pResDesc, sizeof(const CUDA_RESOURCE_DESC*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pSurfObject, sizeof(CUsurfObject)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pSurfObject, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pResDesc, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuSurfObjectDestroy(CUsurfObject surfObject)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&surfObject, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuSurfObjectDestroy) < 0 ||
        rpc_write(conn, &surfObject, sizeof(CUsurfObject)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&surfObject, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuSurfObjectGetResourceDesc(CUDA_RESOURCE_DESC* pResDesc, CUsurfObject surfObject)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pResDesc, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&surfObject, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuSurfObjectGetResourceDesc) < 0 ||
        rpc_write(conn, pResDesc, sizeof(CUDA_RESOURCE_DESC)) < 0 ||
        rpc_write(conn, &surfObject, sizeof(CUsurfObject)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pResDesc, sizeof(CUDA_RESOURCE_DESC)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pResDesc, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&surfObject, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuDeviceCanAccessPeer(int* canAccessPeer, CUdevice dev, CUdevice peerDev)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)canAccessPeer, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&peerDev, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuDeviceCanAccessPeer) < 0 ||
        rpc_write(conn, canAccessPeer, sizeof(int)) < 0 ||
        rpc_write(conn, &dev, sizeof(CUdevice)) < 0 ||
        rpc_write(conn, &peerDev, sizeof(CUdevice)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, canAccessPeer, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)canAccessPeer, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&peerDev, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuCtxEnablePeerAccess(CUcontext peerContext, unsigned int Flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&peerContext, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuCtxEnablePeerAccess) < 0 ||
        rpc_write(conn, &peerContext, sizeof(CUcontext)) < 0 ||
        rpc_write(conn, &Flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&peerContext, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&Flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuCtxDisablePeerAccess(CUcontext peerContext)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&peerContext, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuCtxDisablePeerAccess) < 0 ||
        rpc_write(conn, &peerContext, sizeof(CUcontext)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&peerContext, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuDeviceGetP2PAttribute(int* value, CUdevice_P2PAttribute attrib, CUdevice srcDevice, CUdevice dstDevice)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)value, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&attrib, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&srcDevice, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuDeviceGetP2PAttribute) < 0 ||
        rpc_write(conn, value, sizeof(int)) < 0 ||
        rpc_write(conn, &attrib, sizeof(CUdevice_P2PAttribute)) < 0 ||
        rpc_write(conn, &srcDevice, sizeof(CUdevice)) < 0 ||
        rpc_write(conn, &dstDevice, sizeof(CUdevice)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, value, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)value, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&attrib, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&srcDevice, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphicsUnregisterResource(CUgraphicsResource resource)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&resource, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphicsUnregisterResource) < 0 ||
        rpc_write(conn, &resource, sizeof(CUgraphicsResource)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&resource, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphicsSubResourceGetMappedArray(CUarray* pArray, CUgraphicsResource resource, unsigned int arrayIndex, unsigned int mipLevel)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pArray, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&resource, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&arrayIndex, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&mipLevel, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphicsSubResourceGetMappedArray) < 0 ||
        rpc_write(conn, pArray, sizeof(CUarray)) < 0 ||
        rpc_write(conn, &resource, sizeof(CUgraphicsResource)) < 0 ||
        rpc_write(conn, &arrayIndex, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &mipLevel, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pArray, sizeof(CUarray)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pArray, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&resource, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&arrayIndex, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&mipLevel, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphicsResourceGetMappedMipmappedArray(CUmipmappedArray* pMipmappedArray, CUgraphicsResource resource)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pMipmappedArray, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&resource, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphicsResourceGetMappedMipmappedArray) < 0 ||
        rpc_write(conn, pMipmappedArray, sizeof(CUmipmappedArray)) < 0 ||
        rpc_write(conn, &resource, sizeof(CUgraphicsResource)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pMipmappedArray, sizeof(CUmipmappedArray)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pMipmappedArray, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&resource, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphicsResourceGetMappedPointer_v2(CUdeviceptr* pDevPtr, size_t* pSize, CUgraphicsResource resource)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pDevPtr, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pSize, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&resource, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphicsResourceGetMappedPointer_v2) < 0 ||
        rpc_write(conn, pDevPtr, sizeof(CUdeviceptr)) < 0 ||
        rpc_write(conn, pSize, sizeof(size_t)) < 0 ||
        rpc_write(conn, &resource, sizeof(CUgraphicsResource)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pDevPtr, sizeof(CUdeviceptr)) < 0 ||
        rpc_read(conn, pSize, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pDevPtr, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pSize, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&resource, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphicsResourceSetMapFlags_v2(CUgraphicsResource resource, unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&resource, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphicsResourceSetMapFlags_v2) < 0 ||
        rpc_write(conn, &resource, sizeof(CUgraphicsResource)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&resource, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphicsMapResources(unsigned int count, CUgraphicsResource* resources, CUstream hStream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)resources, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphicsMapResources) < 0 ||
        rpc_write(conn, &count, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, resources, sizeof(CUgraphicsResource)) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, resources, sizeof(CUgraphicsResource)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)resources, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGraphicsUnmapResources(unsigned int count, CUgraphicsResource* resources, CUstream hStream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)resources, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGraphicsUnmapResources) < 0 ||
        rpc_write(conn, &count, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, resources, sizeof(CUgraphicsResource)) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, resources, sizeof(CUgraphicsResource)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)resources, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGreenCtxCreate(CUgreenCtx* phCtx, CUdevResourceDesc desc, CUdevice dev, unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)phCtx, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&desc, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGreenCtxCreate) < 0 ||
        rpc_write(conn, phCtx, sizeof(CUgreenCtx)) < 0 ||
        rpc_write(conn, &desc, sizeof(CUdevResourceDesc)) < 0 ||
        rpc_write(conn, &dev, sizeof(CUdevice)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, phCtx, sizeof(CUgreenCtx)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)phCtx, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&desc, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&dev, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGreenCtxDestroy(CUgreenCtx hCtx)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hCtx, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGreenCtxDestroy) < 0 ||
        rpc_write(conn, &hCtx, sizeof(CUgreenCtx)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hCtx, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuCtxFromGreenCtx(CUcontext* pContext, CUgreenCtx hCtx)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pContext, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hCtx, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuCtxFromGreenCtx) < 0 ||
        rpc_write(conn, pContext, sizeof(CUcontext)) < 0 ||
        rpc_write(conn, &hCtx, sizeof(CUgreenCtx)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pContext, sizeof(CUcontext)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)pContext, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hCtx, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuDeviceGetDevResource(CUdevice device, CUdevResource* resource, CUdevResourceType type)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)resource, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&type, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuDeviceGetDevResource) < 0 ||
        rpc_write(conn, &device, sizeof(CUdevice)) < 0 ||
        rpc_write(conn, resource, sizeof(CUdevResource)) < 0 ||
        rpc_write(conn, &type, sizeof(CUdevResourceType)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, resource, sizeof(CUdevResource)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)resource, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&type, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuCtxGetDevResource(CUcontext hCtx, CUdevResource* resource, CUdevResourceType type)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hCtx, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)resource, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&type, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuCtxGetDevResource) < 0 ||
        rpc_write(conn, &hCtx, sizeof(CUcontext)) < 0 ||
        rpc_write(conn, resource, sizeof(CUdevResource)) < 0 ||
        rpc_write(conn, &type, sizeof(CUdevResourceType)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, resource, sizeof(CUdevResource)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hCtx, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)resource, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&type, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGreenCtxGetDevResource(CUgreenCtx hCtx, CUdevResource* resource, CUdevResourceType type)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hCtx, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)resource, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&type, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGreenCtxGetDevResource) < 0 ||
        rpc_write(conn, &hCtx, sizeof(CUgreenCtx)) < 0 ||
        rpc_write(conn, resource, sizeof(CUdevResource)) < 0 ||
        rpc_write(conn, &type, sizeof(CUdevResourceType)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, resource, sizeof(CUdevResource)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hCtx, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)resource, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&type, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuDevSmResourceSplitByCount(CUdevResource* result, unsigned int* nbGroups, const CUdevResource* input, CUdevResource* remaining, unsigned int useFlags, unsigned int minCount)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)result, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nbGroups, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)input, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)remaining, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&useFlags, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&minCount, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuDevSmResourceSplitByCount) < 0 ||
        rpc_write(conn, result, sizeof(CUdevResource)) < 0 ||
        rpc_write(conn, nbGroups, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &input, sizeof(const CUdevResource*)) < 0 ||
        rpc_write(conn, remaining, sizeof(CUdevResource)) < 0 ||
        rpc_write(conn, &useFlags, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &minCount, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, result, sizeof(CUdevResource)) < 0 ||
        rpc_read(conn, nbGroups, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, remaining, sizeof(CUdevResource)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)result, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)nbGroups, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)input, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)remaining, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&useFlags, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&minCount, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuDevResourceGenerateDesc(CUdevResourceDesc* phDesc, CUdevResource* resources, unsigned int nbResources)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)phDesc, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)resources, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&nbResources, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuDevResourceGenerateDesc) < 0 ||
        rpc_write(conn, phDesc, sizeof(CUdevResourceDesc)) < 0 ||
        rpc_write(conn, resources, sizeof(CUdevResource)) < 0 ||
        rpc_write(conn, &nbResources, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, phDesc, sizeof(CUdevResourceDesc)) < 0 ||
        rpc_read(conn, resources, sizeof(CUdevResource)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)phDesc, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)resources, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&nbResources, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGreenCtxRecordEvent(CUgreenCtx hCtx, CUevent hEvent)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hCtx, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hEvent, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGreenCtxRecordEvent) < 0 ||
        rpc_write(conn, &hCtx, sizeof(CUgreenCtx)) < 0 ||
        rpc_write(conn, &hEvent, sizeof(CUevent)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hCtx, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hEvent, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuGreenCtxWaitEvent(CUgreenCtx hCtx, CUevent hEvent)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hCtx, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hEvent, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuGreenCtxWaitEvent) < 0 ||
        rpc_write(conn, &hCtx, sizeof(CUgreenCtx)) < 0 ||
        rpc_write(conn, &hEvent, sizeof(CUevent)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hCtx, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hEvent, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

CUresult cuStreamGetGreenCtx(CUstream hStream, CUgreenCtx* phCtx)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)phCtx, cudaMemcpyHostToDevice) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    CUresult return_value;
    if (rpc_write_start_request(conn, RPC_cuStreamGetGreenCtx) < 0 ||
        rpc_write(conn, &hStream, sizeof(CUstream)) < 0 ||
        rpc_write(conn, phCtx, sizeof(CUgreenCtx)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, phCtx, sizeof(CUgreenCtx)) < 0 ||
        rpc_read(conn, &return_value, sizeof(CUresult)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    if (maybe_copy_unified_arg(conn, (void*)phCtx, cudaMemcpyDeviceToHost) < 0)
      return CUDA_ERROR_DEVICE_UNAVAILABLE;
    return return_value;
}

cudaError_t cudaDeviceReset()
{
    conn_t *conn = rpc_client_get_connection(0);
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaDeviceReset) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaDeviceSynchronize()
{
    conn_t *conn = rpc_client_get_connection(0);
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaDeviceSynchronize) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaDeviceSetLimit(enum cudaLimit limit, size_t value)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&limit, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&value, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaDeviceSetLimit) < 0 ||
        rpc_write(conn, &limit, sizeof(enum cudaLimit)) < 0 ||
        rpc_write(conn, &value, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&limit, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&value, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaDeviceGetLimit(size_t* pValue, enum cudaLimit limit)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pValue, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&limit, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaDeviceGetLimit) < 0 ||
        rpc_write(conn, pValue, sizeof(size_t)) < 0 ||
        rpc_write(conn, &limit, sizeof(enum cudaLimit)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pValue, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pValue, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&limit, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaDeviceGetTexture1DLinearMaxWidth(size_t* maxWidthInElements, const struct cudaChannelFormatDesc* fmtDesc, int device)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)maxWidthInElements, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)fmtDesc, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaDeviceGetTexture1DLinearMaxWidth) < 0 ||
        rpc_write(conn, maxWidthInElements, sizeof(size_t)) < 0 ||
        rpc_write(conn, &fmtDesc, sizeof(const struct cudaChannelFormatDesc*)) < 0 ||
        rpc_write(conn, &device, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, maxWidthInElements, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)maxWidthInElements, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)fmtDesc, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaDeviceGetCacheConfig(enum cudaFuncCache* pCacheConfig)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pCacheConfig, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaDeviceGetCacheConfig) < 0 ||
        rpc_write(conn, pCacheConfig, sizeof(enum cudaFuncCache)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pCacheConfig, sizeof(enum cudaFuncCache)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pCacheConfig, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaDeviceGetStreamPriorityRange(int* leastPriority, int* greatestPriority)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)leastPriority, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)greatestPriority, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaDeviceGetStreamPriorityRange) < 0 ||
        rpc_write(conn, leastPriority, sizeof(int)) < 0 ||
        rpc_write(conn, greatestPriority, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, leastPriority, sizeof(int)) < 0 ||
        rpc_read(conn, greatestPriority, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)leastPriority, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)greatestPriority, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaDeviceSetCacheConfig(enum cudaFuncCache cacheConfig)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&cacheConfig, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaDeviceSetCacheConfig) < 0 ||
        rpc_write(conn, &cacheConfig, sizeof(enum cudaFuncCache)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&cacheConfig, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaDeviceGetByPCIBusId(int* device, const char* pciBusId)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)device, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pciBusId, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaDeviceGetByPCIBusId) < 0 ||
        rpc_write(conn, device, sizeof(int)) < 0 ||
        rpc_write(conn, &pciBusId, sizeof(const char*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, device, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)device, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pciBusId, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaDeviceGetPCIBusId(char* pciBusId, int len, int device)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pciBusId, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&len, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaDeviceGetPCIBusId) < 0 ||
        rpc_write(conn, pciBusId, sizeof(char)) < 0 ||
        rpc_write(conn, &len, sizeof(int)) < 0 ||
        rpc_write(conn, &device, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pciBusId, sizeof(char)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pciBusId, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&len, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaIpcGetEventHandle(cudaIpcEventHandle_t* handle, cudaEvent_t event)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)handle, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&event, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaIpcGetEventHandle) < 0 ||
        rpc_write(conn, handle, sizeof(cudaIpcEventHandle_t)) < 0 ||
        rpc_write(conn, &event, sizeof(cudaEvent_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, handle, sizeof(cudaIpcEventHandle_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)handle, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&event, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaIpcOpenEventHandle(cudaEvent_t* event, cudaIpcEventHandle_t handle)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)event, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&handle, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaIpcOpenEventHandle) < 0 ||
        rpc_write(conn, event, sizeof(cudaEvent_t)) < 0 ||
        rpc_write(conn, &handle, sizeof(cudaIpcEventHandle_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, event, sizeof(cudaEvent_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)event, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&handle, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaIpcOpenMemHandle(void** devPtr, cudaIpcMemHandle_t handle, unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)devPtr, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&handle, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaIpcOpenMemHandle) < 0 ||
        rpc_write(conn, devPtr, sizeof(void*)) < 0 ||
        rpc_write(conn, &handle, sizeof(cudaIpcMemHandle_t)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, devPtr, sizeof(void*)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)devPtr, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&handle, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaDeviceFlushGPUDirectRDMAWrites(enum cudaFlushGPUDirectRDMAWritesTarget target, enum cudaFlushGPUDirectRDMAWritesScope scope)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&target, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&scope, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaDeviceFlushGPUDirectRDMAWrites) < 0 ||
        rpc_write(conn, &target, sizeof(enum cudaFlushGPUDirectRDMAWritesTarget)) < 0 ||
        rpc_write(conn, &scope, sizeof(enum cudaFlushGPUDirectRDMAWritesScope)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&target, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&scope, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaDeviceUnregisterAsyncNotification(int device, cudaAsyncCallbackHandle_t callback)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&callback, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaDeviceUnregisterAsyncNotification) < 0 ||
        rpc_write(conn, &device, sizeof(int)) < 0 ||
        rpc_write(conn, &callback, sizeof(cudaAsyncCallbackHandle_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&callback, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaDeviceGetSharedMemConfig(enum cudaSharedMemConfig* pConfig)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pConfig, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaDeviceGetSharedMemConfig) < 0 ||
        rpc_write(conn, pConfig, sizeof(enum cudaSharedMemConfig)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pConfig, sizeof(enum cudaSharedMemConfig)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pConfig, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaDeviceSetSharedMemConfig(enum cudaSharedMemConfig config)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&config, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaDeviceSetSharedMemConfig) < 0 ||
        rpc_write(conn, &config, sizeof(enum cudaSharedMemConfig)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&config, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaThreadExit()
{
    conn_t *conn = rpc_client_get_connection(0);
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaThreadExit) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaThreadSynchronize()
{
    conn_t *conn = rpc_client_get_connection(0);
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaThreadSynchronize) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaThreadSetLimit(enum cudaLimit limit, size_t value)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&limit, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&value, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaThreadSetLimit) < 0 ||
        rpc_write(conn, &limit, sizeof(enum cudaLimit)) < 0 ||
        rpc_write(conn, &value, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&limit, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&value, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaThreadGetLimit(size_t* pValue, enum cudaLimit limit)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pValue, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&limit, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaThreadGetLimit) < 0 ||
        rpc_write(conn, pValue, sizeof(size_t)) < 0 ||
        rpc_write(conn, &limit, sizeof(enum cudaLimit)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pValue, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pValue, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&limit, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaThreadGetCacheConfig(enum cudaFuncCache* pCacheConfig)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pCacheConfig, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaThreadGetCacheConfig) < 0 ||
        rpc_write(conn, pCacheConfig, sizeof(enum cudaFuncCache)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pCacheConfig, sizeof(enum cudaFuncCache)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pCacheConfig, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaThreadSetCacheConfig(enum cudaFuncCache cacheConfig)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&cacheConfig, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaThreadSetCacheConfig) < 0 ||
        rpc_write(conn, &cacheConfig, sizeof(enum cudaFuncCache)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&cacheConfig, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGetLastError()
{
    conn_t *conn = rpc_client_get_connection(0);
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGetLastError) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaPeekAtLastError()
{
    conn_t *conn = rpc_client_get_connection(0);
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaPeekAtLastError) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGetDeviceCount(int* count)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)count, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGetDeviceCount) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, count, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)count, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGetDeviceProperties_v2(struct cudaDeviceProp* prop, int device)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)prop, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGetDeviceProperties_v2) < 0 ||
        rpc_write(conn, &device, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, prop, sizeof(struct cudaDeviceProp)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)prop, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaDeviceGetAttribute(int* value, enum cudaDeviceAttr attr, int device)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)value, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&attr, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaDeviceGetAttribute) < 0 ||
        rpc_write(conn, value, sizeof(int)) < 0 ||
        rpc_write(conn, &attr, sizeof(enum cudaDeviceAttr)) < 0 ||
        rpc_write(conn, &device, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, value, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)value, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&attr, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaDeviceGetDefaultMemPool(cudaMemPool_t* memPool, int device)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)memPool, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaDeviceGetDefaultMemPool) < 0 ||
        rpc_write(conn, memPool, sizeof(cudaMemPool_t)) < 0 ||
        rpc_write(conn, &device, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, memPool, sizeof(cudaMemPool_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)memPool, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaDeviceSetMemPool(int device, cudaMemPool_t memPool)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&memPool, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaDeviceSetMemPool) < 0 ||
        rpc_write(conn, &device, sizeof(int)) < 0 ||
        rpc_write(conn, &memPool, sizeof(cudaMemPool_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&memPool, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaDeviceGetMemPool(cudaMemPool_t* memPool, int device)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)memPool, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaDeviceGetMemPool) < 0 ||
        rpc_write(conn, memPool, sizeof(cudaMemPool_t)) < 0 ||
        rpc_write(conn, &device, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, memPool, sizeof(cudaMemPool_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)memPool, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaDeviceGetP2PAttribute(int* value, enum cudaDeviceP2PAttr attr, int srcDevice, int dstDevice)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)value, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&attr, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&srcDevice, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaDeviceGetP2PAttribute) < 0 ||
        rpc_write(conn, value, sizeof(int)) < 0 ||
        rpc_write(conn, &attr, sizeof(enum cudaDeviceP2PAttr)) < 0 ||
        rpc_write(conn, &srcDevice, sizeof(int)) < 0 ||
        rpc_write(conn, &dstDevice, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, value, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)value, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&attr, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&srcDevice, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaChooseDevice(int* device, const struct cudaDeviceProp* prop)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)device, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)prop, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaChooseDevice) < 0 ||
        rpc_write(conn, device, sizeof(int)) < 0 ||
        rpc_write(conn, &prop, sizeof(const struct cudaDeviceProp*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, device, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)device, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)prop, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaInitDevice(int device, unsigned int deviceFlags, unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&deviceFlags, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaInitDevice) < 0 ||
        rpc_write(conn, &device, sizeof(int)) < 0 ||
        rpc_write(conn, &deviceFlags, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&deviceFlags, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaSetDevice(int device)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaSetDevice) < 0 ||
        rpc_write(conn, &device, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGetDevice(int* device)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)device, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGetDevice) < 0 ||
        rpc_write(conn, device, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, device, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)device, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaSetValidDevices(int* device_arr, int len)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)device_arr, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&len, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaSetValidDevices) < 0 ||
        rpc_write(conn, device_arr, sizeof(int)) < 0 ||
        rpc_write(conn, &len, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, device_arr, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)device_arr, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&len, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaSetDeviceFlags(unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaSetDeviceFlags) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGetDeviceFlags(unsigned int* flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)flags, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGetDeviceFlags) < 0 ||
        rpc_write(conn, flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, flags, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)flags, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaStreamCreate(cudaStream_t* pStream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pStream, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaStreamCreate) < 0 ||
        rpc_write(conn, pStream, sizeof(cudaStream_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pStream, sizeof(cudaStream_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pStream, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaStreamCreateWithFlags(cudaStream_t* pStream, unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pStream, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaStreamCreateWithFlags) < 0 ||
        rpc_write(conn, pStream, sizeof(cudaStream_t)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pStream, sizeof(cudaStream_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pStream, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaStreamCreateWithPriority(cudaStream_t* pStream, unsigned int flags, int priority)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pStream, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&priority, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaStreamCreateWithPriority) < 0 ||
        rpc_write(conn, pStream, sizeof(cudaStream_t)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &priority, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pStream, sizeof(cudaStream_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pStream, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&priority, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaStreamGetPriority(cudaStream_t hStream, int* priority)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)priority, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaStreamGetPriority) < 0 ||
        rpc_write(conn, &hStream, sizeof(cudaStream_t)) < 0 ||
        rpc_write(conn, priority, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, priority, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)priority, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaStreamGetFlags(cudaStream_t hStream, unsigned int* flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)flags, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaStreamGetFlags) < 0 ||
        rpc_write(conn, &hStream, sizeof(cudaStream_t)) < 0 ||
        rpc_write(conn, flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, flags, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)flags, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaStreamGetId(cudaStream_t hStream, unsigned long long* streamId)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)streamId, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaStreamGetId) < 0 ||
        rpc_write(conn, &hStream, sizeof(cudaStream_t)) < 0 ||
        rpc_write(conn, streamId, sizeof(unsigned long long)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, streamId, sizeof(unsigned long long)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)streamId, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaCtxResetPersistingL2Cache()
{
    conn_t *conn = rpc_client_get_connection(0);
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaCtxResetPersistingL2Cache) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaStreamCopyAttributes(cudaStream_t dst, cudaStream_t src)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&dst, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&src, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaStreamCopyAttributes) < 0 ||
        rpc_write(conn, &dst, sizeof(cudaStream_t)) < 0 ||
        rpc_write(conn, &src, sizeof(cudaStream_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&dst, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&src, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaStreamGetAttribute(cudaStream_t hStream, cudaLaunchAttributeID attr, cudaLaunchAttributeValue* value_out)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&attr, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)value_out, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaStreamGetAttribute) < 0 ||
        rpc_write(conn, &hStream, sizeof(cudaStream_t)) < 0 ||
        rpc_write(conn, &attr, sizeof(cudaLaunchAttributeID)) < 0 ||
        rpc_write(conn, value_out, sizeof(cudaLaunchAttributeValue)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, value_out, sizeof(cudaLaunchAttributeValue)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&attr, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)value_out, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaStreamSetAttribute(cudaStream_t hStream, cudaLaunchAttributeID attr, const cudaLaunchAttributeValue* value)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&attr, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)value, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaStreamSetAttribute) < 0 ||
        rpc_write(conn, &hStream, sizeof(cudaStream_t)) < 0 ||
        rpc_write(conn, &attr, sizeof(cudaLaunchAttributeID)) < 0 ||
        rpc_write(conn, &value, sizeof(const cudaLaunchAttributeValue*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&attr, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)value, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaStreamDestroy(cudaStream_t stream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaStreamDestroy) < 0 ||
        rpc_write(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaStreamWaitEvent(cudaStream_t stream, cudaEvent_t event, unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&event, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaStreamWaitEvent) < 0 ||
        rpc_write(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_write(conn, &event, sizeof(cudaEvent_t)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&event, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaStreamSynchronize(cudaStream_t stream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaStreamSynchronize) < 0 ||
        rpc_write(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaStreamQuery(cudaStream_t stream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaStreamQuery) < 0 ||
        rpc_write(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaStreamBeginCapture(cudaStream_t stream, enum cudaStreamCaptureMode mode)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&mode, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaStreamBeginCapture) < 0 ||
        rpc_write(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_write(conn, &mode, sizeof(enum cudaStreamCaptureMode)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&mode, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaStreamBeginCaptureToGraph(cudaStream_t stream, cudaGraph_t graph, const cudaGraphNode_t* dependencies, const cudaGraphEdgeData* dependencyData, size_t numDependencies, enum cudaStreamCaptureMode mode)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)dependencyData, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&mode, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaStreamBeginCaptureToGraph) < 0 ||
        rpc_write(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_write(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
        rpc_write(conn, &dependencies, sizeof(const cudaGraphNode_t*)) < 0 ||
        rpc_write(conn, &dependencyData, sizeof(const cudaGraphEdgeData*)) < 0 ||
        rpc_write(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_write(conn, &mode, sizeof(enum cudaStreamCaptureMode)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)dependencyData, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&mode, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaThreadExchangeStreamCaptureMode(enum cudaStreamCaptureMode* mode)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)mode, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaThreadExchangeStreamCaptureMode) < 0 ||
        rpc_write(conn, mode, sizeof(enum cudaStreamCaptureMode)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, mode, sizeof(enum cudaStreamCaptureMode)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)mode, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaStreamEndCapture(cudaStream_t stream, cudaGraph_t* pGraph)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pGraph, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaStreamEndCapture) < 0 ||
        rpc_write(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_write(conn, pGraph, sizeof(cudaGraph_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pGraph, sizeof(cudaGraph_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pGraph, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaStreamIsCapturing(cudaStream_t stream, enum cudaStreamCaptureStatus* pCaptureStatus)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pCaptureStatus, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaStreamIsCapturing) < 0 ||
        rpc_write(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_write(conn, pCaptureStatus, sizeof(enum cudaStreamCaptureStatus)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pCaptureStatus, sizeof(enum cudaStreamCaptureStatus)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pCaptureStatus, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaStreamGetCaptureInfo_v2(cudaStream_t stream, enum cudaStreamCaptureStatus* captureStatus_out, unsigned long long* id_out, cudaGraph_t* graph_out, const cudaGraphNode_t** dependencies_out, size_t* numDependencies_out)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)captureStatus_out, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)id_out, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)graph_out, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)numDependencies_out, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)dependencies_out, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    for (int i = 0; i < static_cast<int>(*numDependencies_out) && is_unified_pointer(conn, (void*)dependencies_out); i++)
      if (maybe_copy_unified_arg(conn, (void*)&dependencies_out[i], cudaMemcpyHostToDevice) < 0)
        return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaStreamGetCaptureInfo_v2) < 0 ||
        rpc_write(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, captureStatus_out, sizeof(enum cudaStreamCaptureStatus)) < 0 ||
        rpc_read(conn, id_out, sizeof(unsigned long long)) < 0 ||
        rpc_read(conn, graph_out, sizeof(cudaGraph_t)) < 0 ||
        rpc_read(conn, numDependencies_out, sizeof(size_t)) < 0 ||
        rpc_read(conn, dependencies_out, *numDependencies_out * sizeof(const cudaGraphNode_t*)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)captureStatus_out, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)id_out, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)graph_out, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)numDependencies_out, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)dependencies_out, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    for (int i = 0; i < static_cast<int>(*numDependencies_out) && is_unified_pointer(conn, (void*)dependencies_out); i++)
      if (maybe_copy_unified_arg(conn, (void*)&dependencies_out[i], cudaMemcpyDeviceToHost) < 0)
        return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaStreamGetCaptureInfo_v3(cudaStream_t stream, enum cudaStreamCaptureStatus* captureStatus_out, unsigned long long* id_out, cudaGraph_t* graph_out, const cudaGraphNode_t** dependencies_out, const cudaGraphEdgeData** edgeData_out, size_t* numDependencies_out)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)captureStatus_out, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)id_out, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)graph_out, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)dependencies_out, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)edgeData_out, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)numDependencies_out, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaStreamGetCaptureInfo_v3) < 0 ||
        rpc_write(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_write(conn, captureStatus_out, sizeof(enum cudaStreamCaptureStatus)) < 0 ||
        rpc_write(conn, id_out, sizeof(unsigned long long)) < 0 ||
        rpc_write(conn, graph_out, sizeof(cudaGraph_t)) < 0 ||
        rpc_write(conn, dependencies_out, sizeof(const cudaGraphNode_t*)) < 0 ||
        rpc_write(conn, edgeData_out, sizeof(const cudaGraphEdgeData*)) < 0 ||
        rpc_write(conn, numDependencies_out, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, captureStatus_out, sizeof(enum cudaStreamCaptureStatus)) < 0 ||
        rpc_read(conn, id_out, sizeof(unsigned long long)) < 0 ||
        rpc_read(conn, graph_out, sizeof(cudaGraph_t)) < 0 ||
        rpc_read(conn, dependencies_out, sizeof(const cudaGraphNode_t*)) < 0 ||
        rpc_read(conn, edgeData_out, sizeof(const cudaGraphEdgeData*)) < 0 ||
        rpc_read(conn, numDependencies_out, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)captureStatus_out, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)id_out, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)graph_out, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)dependencies_out, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)edgeData_out, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)numDependencies_out, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaStreamUpdateCaptureDependencies(cudaStream_t stream, cudaGraphNode_t* dependencies, size_t numDependencies, unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    for (int i = 0; i < static_cast<int>(numDependencies) && is_unified_pointer(conn, (void*)dependencies); i++)
      if (maybe_copy_unified_arg(conn, (void*)&dependencies[i], cudaMemcpyHostToDevice) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaStreamUpdateCaptureDependencies) < 0 ||
        rpc_write(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_write(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_write(conn, dependencies, numDependencies * sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    for (int i = 0; i < static_cast<int>(numDependencies) && is_unified_pointer(conn, (void*)dependencies); i++)
      if (maybe_copy_unified_arg(conn, (void*)&dependencies[i], cudaMemcpyDeviceToHost) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaStreamUpdateCaptureDependencies_v2(cudaStream_t stream, cudaGraphNode_t* dependencies, const cudaGraphEdgeData* dependencyData, size_t numDependencies, unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)dependencyData, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaStreamUpdateCaptureDependencies_v2) < 0 ||
        rpc_write(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_write(conn, dependencies, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &dependencyData, sizeof(const cudaGraphEdgeData*)) < 0 ||
        rpc_write(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, dependencies, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)dependencies, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)dependencyData, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaEventCreate(cudaEvent_t* event)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)event, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaEventCreate) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, event, sizeof(cudaEvent_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)event, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaEventCreateWithFlags(cudaEvent_t* event, unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)event, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaEventCreateWithFlags) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, event, sizeof(cudaEvent_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)event, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaEventRecord(cudaEvent_t event, cudaStream_t stream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&event, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaEventRecord) < 0 ||
        rpc_write(conn, &event, sizeof(cudaEvent_t)) < 0 ||
        rpc_write(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&event, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaEventRecordWithFlags(cudaEvent_t event, cudaStream_t stream, unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&event, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaEventRecordWithFlags) < 0 ||
        rpc_write(conn, &event, sizeof(cudaEvent_t)) < 0 ||
        rpc_write(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&event, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaEventQuery(cudaEvent_t event)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&event, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaEventQuery) < 0 ||
        rpc_write(conn, &event, sizeof(cudaEvent_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&event, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaEventSynchronize(cudaEvent_t event)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&event, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaEventSynchronize) < 0 ||
        rpc_write(conn, &event, sizeof(cudaEvent_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&event, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaEventDestroy(cudaEvent_t event)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&event, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaEventDestroy) < 0 ||
        rpc_write(conn, &event, sizeof(cudaEvent_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&event, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaEventElapsedTime(float* ms, cudaEvent_t start, cudaEvent_t end)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)ms, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&start, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&end, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaEventElapsedTime) < 0 ||
        rpc_write(conn, &start, sizeof(cudaEvent_t)) < 0 ||
        rpc_write(conn, &end, sizeof(cudaEvent_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, ms, sizeof(float)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)ms, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&start, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&end, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaExternalMemoryGetMappedBuffer(void** devPtr, cudaExternalMemory_t extMem, const struct cudaExternalMemoryBufferDesc* bufferDesc)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)devPtr, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&extMem, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)bufferDesc, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaExternalMemoryGetMappedBuffer) < 0 ||
        rpc_write(conn, devPtr, sizeof(void*)) < 0 ||
        rpc_write(conn, &extMem, sizeof(cudaExternalMemory_t)) < 0 ||
        rpc_write(conn, &bufferDesc, sizeof(const struct cudaExternalMemoryBufferDesc*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, devPtr, sizeof(void*)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)devPtr, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&extMem, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)bufferDesc, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaExternalMemoryGetMappedMipmappedArray(cudaMipmappedArray_t* mipmap, cudaExternalMemory_t extMem, const struct cudaExternalMemoryMipmappedArrayDesc* mipmapDesc)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)mipmap, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&extMem, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)mipmapDesc, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaExternalMemoryGetMappedMipmappedArray) < 0 ||
        rpc_write(conn, mipmap, sizeof(cudaMipmappedArray_t)) < 0 ||
        rpc_write(conn, &extMem, sizeof(cudaExternalMemory_t)) < 0 ||
        rpc_write(conn, &mipmapDesc, sizeof(const struct cudaExternalMemoryMipmappedArrayDesc*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, mipmap, sizeof(cudaMipmappedArray_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)mipmap, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&extMem, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)mipmapDesc, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaDestroyExternalMemory(cudaExternalMemory_t extMem)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&extMem, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaDestroyExternalMemory) < 0 ||
        rpc_write(conn, &extMem, sizeof(cudaExternalMemory_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&extMem, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaImportExternalSemaphore(cudaExternalSemaphore_t* extSem_out, const struct cudaExternalSemaphoreHandleDesc* semHandleDesc)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)extSem_out, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)semHandleDesc, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaImportExternalSemaphore) < 0 ||
        rpc_write(conn, extSem_out, sizeof(cudaExternalSemaphore_t)) < 0 ||
        rpc_write(conn, &semHandleDesc, sizeof(const struct cudaExternalSemaphoreHandleDesc*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, extSem_out, sizeof(cudaExternalSemaphore_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)extSem_out, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)semHandleDesc, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaSignalExternalSemaphoresAsync_v2(const cudaExternalSemaphore_t* extSemArray, const struct cudaExternalSemaphoreSignalParams* paramsArray, unsigned int numExtSems, cudaStream_t stream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)extSemArray, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)paramsArray, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&numExtSems, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaSignalExternalSemaphoresAsync_v2) < 0 ||
        rpc_write(conn, &extSemArray, sizeof(const cudaExternalSemaphore_t*)) < 0 ||
        rpc_write(conn, &paramsArray, sizeof(const struct cudaExternalSemaphoreSignalParams*)) < 0 ||
        rpc_write(conn, &numExtSems, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)extSemArray, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)paramsArray, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&numExtSems, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaWaitExternalSemaphoresAsync_v2(const cudaExternalSemaphore_t* extSemArray, const struct cudaExternalSemaphoreWaitParams* paramsArray, unsigned int numExtSems, cudaStream_t stream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)extSemArray, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)paramsArray, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&numExtSems, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaWaitExternalSemaphoresAsync_v2) < 0 ||
        rpc_write(conn, &extSemArray, sizeof(const cudaExternalSemaphore_t*)) < 0 ||
        rpc_write(conn, &paramsArray, sizeof(const struct cudaExternalSemaphoreWaitParams*)) < 0 ||
        rpc_write(conn, &numExtSems, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)extSemArray, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)paramsArray, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&numExtSems, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaDestroyExternalSemaphore(cudaExternalSemaphore_t extSem)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&extSem, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaDestroyExternalSemaphore) < 0 ||
        rpc_write(conn, &extSem, sizeof(cudaExternalSemaphore_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&extSem, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaLaunchKernelExC(const cudaLaunchConfig_t* config, const void* func, void** args)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)config, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)func, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)args, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaLaunchKernelExC) < 0 ||
        rpc_write(conn, &config, sizeof(const cudaLaunchConfig_t*)) < 0 ||
        rpc_write(conn, &func, sizeof(const void*)) < 0 ||
        rpc_write(conn, args, sizeof(void*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, args, sizeof(void*)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)config, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)func, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)args, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaLaunchCooperativeKernel(const void* func, dim3 gridDim, dim3 blockDim, void** args, size_t sharedMem, cudaStream_t stream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)func, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&gridDim, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&blockDim, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)args, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&sharedMem, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaLaunchCooperativeKernel) < 0 ||
        rpc_write(conn, &func, sizeof(const void*)) < 0 ||
        rpc_write(conn, &gridDim, sizeof(dim3)) < 0 ||
        rpc_write(conn, &blockDim, sizeof(dim3)) < 0 ||
        rpc_write(conn, args, sizeof(void*)) < 0 ||
        rpc_write(conn, &sharedMem, sizeof(size_t)) < 0 ||
        rpc_write(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, args, sizeof(void*)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)func, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&gridDim, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&blockDim, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)args, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&sharedMem, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaLaunchCooperativeKernelMultiDevice(struct cudaLaunchParams* launchParamsList, unsigned int numDevices, unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)launchParamsList, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&numDevices, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaLaunchCooperativeKernelMultiDevice) < 0 ||
        rpc_write(conn, launchParamsList, sizeof(struct cudaLaunchParams)) < 0 ||
        rpc_write(conn, &numDevices, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, launchParamsList, sizeof(struct cudaLaunchParams)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)launchParamsList, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&numDevices, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaFuncSetCacheConfig(const void* func, enum cudaFuncCache cacheConfig)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)func, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&cacheConfig, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaFuncSetCacheConfig) < 0 ||
        rpc_write(conn, &func, sizeof(const void*)) < 0 ||
        rpc_write(conn, &cacheConfig, sizeof(enum cudaFuncCache)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)func, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&cacheConfig, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaFuncGetAttributes(struct cudaFuncAttributes* attr, const void* func)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)attr, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)func, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaFuncGetAttributes) < 0 ||
        rpc_write(conn, attr, sizeof(struct cudaFuncAttributes)) < 0 ||
        rpc_write(conn, &func, sizeof(const void*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, attr, sizeof(struct cudaFuncAttributes)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)attr, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)func, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaFuncSetAttribute(const void* func, enum cudaFuncAttribute attr, int value)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)func, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&attr, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&value, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaFuncSetAttribute) < 0 ||
        rpc_write(conn, &func, sizeof(const void*)) < 0 ||
        rpc_write(conn, &attr, sizeof(enum cudaFuncAttribute)) < 0 ||
        rpc_write(conn, &value, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)func, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&attr, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&value, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaFuncGetName(const char** name, const void* func)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)name, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)func, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaFuncGetName) < 0 ||
        rpc_write(conn, name, sizeof(const char*)) < 0 ||
        rpc_write(conn, &func, sizeof(const void*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, name, sizeof(const char*)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)name, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)func, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaFuncGetParamInfo(const void* func, size_t paramIndex, size_t* paramOffset, size_t* paramSize)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)func, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&paramIndex, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)paramOffset, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)paramSize, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaFuncGetParamInfo) < 0 ||
        rpc_write(conn, &func, sizeof(const void*)) < 0 ||
        rpc_write(conn, &paramIndex, sizeof(size_t)) < 0 ||
        rpc_write(conn, paramOffset, sizeof(size_t)) < 0 ||
        rpc_write(conn, paramSize, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, paramOffset, sizeof(size_t)) < 0 ||
        rpc_read(conn, paramSize, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)func, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&paramIndex, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)paramOffset, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)paramSize, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaSetDoubleForDevice(double* d)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)d, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaSetDoubleForDevice) < 0 ||
        rpc_write(conn, d, sizeof(double)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, d, sizeof(double)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)d, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaSetDoubleForHost(double* d)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)d, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaSetDoubleForHost) < 0 ||
        rpc_write(conn, d, sizeof(double)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, d, sizeof(double)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)d, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaFuncSetSharedMemConfig(const void* func, enum cudaSharedMemConfig config)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)func, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&config, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaFuncSetSharedMemConfig) < 0 ||
        rpc_write(conn, &func, sizeof(const void*)) < 0 ||
        rpc_write(conn, &config, sizeof(enum cudaSharedMemConfig)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)func, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&config, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaOccupancyMaxActiveBlocksPerMultiprocessor(int* numBlocks, const void* func, int blockSize, size_t dynamicSMemSize)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)numBlocks, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)func, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&blockSize, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&dynamicSMemSize, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaOccupancyMaxActiveBlocksPerMultiprocessor) < 0 ||
        rpc_write(conn, numBlocks, sizeof(int)) < 0 ||
        rpc_write(conn, &func, sizeof(const void*)) < 0 ||
        rpc_write(conn, &blockSize, sizeof(int)) < 0 ||
        rpc_write(conn, &dynamicSMemSize, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, numBlocks, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)numBlocks, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)func, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&blockSize, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&dynamicSMemSize, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaOccupancyAvailableDynamicSMemPerBlock(size_t* dynamicSmemSize, const void* func, int numBlocks, int blockSize)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)dynamicSmemSize, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)func, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&numBlocks, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&blockSize, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaOccupancyAvailableDynamicSMemPerBlock) < 0 ||
        rpc_write(conn, dynamicSmemSize, sizeof(size_t)) < 0 ||
        rpc_write(conn, &func, sizeof(const void*)) < 0 ||
        rpc_write(conn, &numBlocks, sizeof(int)) < 0 ||
        rpc_write(conn, &blockSize, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, dynamicSmemSize, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)dynamicSmemSize, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)func, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&numBlocks, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&blockSize, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags(int* numBlocks, const void* func, int blockSize, size_t dynamicSMemSize, unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)numBlocks, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)func, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&blockSize, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&dynamicSMemSize, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags) < 0 ||
        rpc_write(conn, numBlocks, sizeof(int)) < 0 ||
        rpc_write(conn, &func, sizeof(const void*)) < 0 ||
        rpc_write(conn, &blockSize, sizeof(int)) < 0 ||
        rpc_write(conn, &dynamicSMemSize, sizeof(size_t)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, numBlocks, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)numBlocks, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)func, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&blockSize, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&dynamicSMemSize, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaOccupancyMaxPotentialClusterSize(int* clusterSize, const void* func, const cudaLaunchConfig_t* launchConfig)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)clusterSize, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)func, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)launchConfig, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaOccupancyMaxPotentialClusterSize) < 0 ||
        rpc_write(conn, clusterSize, sizeof(int)) < 0 ||
        rpc_write(conn, &func, sizeof(const void*)) < 0 ||
        rpc_write(conn, &launchConfig, sizeof(const cudaLaunchConfig_t*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, clusterSize, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)clusterSize, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)func, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)launchConfig, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaOccupancyMaxActiveClusters(int* numClusters, const void* func, const cudaLaunchConfig_t* launchConfig)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)numClusters, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)func, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)launchConfig, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaOccupancyMaxActiveClusters) < 0 ||
        rpc_write(conn, numClusters, sizeof(int)) < 0 ||
        rpc_write(conn, &func, sizeof(const void*)) < 0 ||
        rpc_write(conn, &launchConfig, sizeof(const cudaLaunchConfig_t*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, numClusters, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)numClusters, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)func, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)launchConfig, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaMalloc(void** devPtr, size_t size)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)devPtr, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&size, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaMalloc) < 0 ||
        rpc_write(conn, &size, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, devPtr, sizeof(void*)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)devPtr, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&size, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaMallocPitch(void** devPtr, size_t* pitch, size_t width, size_t height)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)devPtr, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pitch, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&width, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&height, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaMallocPitch) < 0 ||
        rpc_write(conn, devPtr, sizeof(void*)) < 0 ||
        rpc_write(conn, pitch, sizeof(size_t)) < 0 ||
        rpc_write(conn, &width, sizeof(size_t)) < 0 ||
        rpc_write(conn, &height, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, devPtr, sizeof(void*)) < 0 ||
        rpc_read(conn, pitch, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)devPtr, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pitch, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&width, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&height, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaMallocArray(cudaArray_t* array, const struct cudaChannelFormatDesc* desc, size_t width, size_t height, unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)array, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)desc, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&width, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&height, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaMallocArray) < 0 ||
        rpc_write(conn, array, sizeof(cudaArray_t)) < 0 ||
        rpc_write(conn, &desc, sizeof(const struct cudaChannelFormatDesc*)) < 0 ||
        rpc_write(conn, &width, sizeof(size_t)) < 0 ||
        rpc_write(conn, &height, sizeof(size_t)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, array, sizeof(cudaArray_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)array, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)desc, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&width, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&height, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaFreeHost(void* ptr)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)ptr, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaFreeHost) < 0 ||
        rpc_write(conn, &ptr, sizeof(void*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)ptr, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaFreeArray(cudaArray_t array)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&array, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaFreeArray) < 0 ||
        rpc_write(conn, &array, sizeof(cudaArray_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&array, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaFreeMipmappedArray(cudaMipmappedArray_t mipmappedArray)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&mipmappedArray, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaFreeMipmappedArray) < 0 ||
        rpc_write(conn, &mipmappedArray, sizeof(cudaMipmappedArray_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&mipmappedArray, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaMalloc3D(struct cudaPitchedPtr* pitchedDevPtr, struct cudaExtent extent)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pitchedDevPtr, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&extent, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaMalloc3D) < 0 ||
        rpc_write(conn, pitchedDevPtr, sizeof(struct cudaPitchedPtr)) < 0 ||
        rpc_write(conn, &extent, sizeof(struct cudaExtent)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pitchedDevPtr, sizeof(struct cudaPitchedPtr)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pitchedDevPtr, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&extent, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaMalloc3DArray(cudaArray_t* array, const struct cudaChannelFormatDesc* desc, struct cudaExtent extent, unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)array, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)desc, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&extent, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaMalloc3DArray) < 0 ||
        rpc_write(conn, array, sizeof(cudaArray_t)) < 0 ||
        rpc_write(conn, &desc, sizeof(const struct cudaChannelFormatDesc*)) < 0 ||
        rpc_write(conn, &extent, sizeof(struct cudaExtent)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, array, sizeof(cudaArray_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)array, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)desc, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&extent, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaMallocMipmappedArray(cudaMipmappedArray_t* mipmappedArray, const struct cudaChannelFormatDesc* desc, struct cudaExtent extent, unsigned int numLevels, unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)mipmappedArray, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)desc, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&extent, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&numLevels, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaMallocMipmappedArray) < 0 ||
        rpc_write(conn, mipmappedArray, sizeof(cudaMipmappedArray_t)) < 0 ||
        rpc_write(conn, &desc, sizeof(const struct cudaChannelFormatDesc*)) < 0 ||
        rpc_write(conn, &extent, sizeof(struct cudaExtent)) < 0 ||
        rpc_write(conn, &numLevels, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, mipmappedArray, sizeof(cudaMipmappedArray_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)mipmappedArray, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)desc, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&extent, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&numLevels, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGetMipmappedArrayLevel(cudaArray_t* levelArray, cudaMipmappedArray_const_t mipmappedArray, unsigned int level)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)levelArray, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&mipmappedArray, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&level, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGetMipmappedArrayLevel) < 0 ||
        rpc_write(conn, levelArray, sizeof(cudaArray_t)) < 0 ||
        rpc_write(conn, &mipmappedArray, sizeof(cudaMipmappedArray_const_t)) < 0 ||
        rpc_write(conn, &level, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, levelArray, sizeof(cudaArray_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)levelArray, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&mipmappedArray, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&level, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaMemcpy3D(const struct cudaMemcpy3DParms* p)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)p, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaMemcpy3D) < 0 ||
        rpc_write(conn, &p, sizeof(const struct cudaMemcpy3DParms*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)p, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaMemcpy3DPeer(const struct cudaMemcpy3DPeerParms* p)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)p, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaMemcpy3DPeer) < 0 ||
        rpc_write(conn, &p, sizeof(const struct cudaMemcpy3DPeerParms*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)p, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaMemcpy3DAsync(const struct cudaMemcpy3DParms* p, cudaStream_t stream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)p, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaMemcpy3DAsync) < 0 ||
        rpc_write(conn, &p, sizeof(const struct cudaMemcpy3DParms*)) < 0 ||
        rpc_write(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)p, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaMemcpy3DPeerAsync(const struct cudaMemcpy3DPeerParms* p, cudaStream_t stream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)p, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaMemcpy3DPeerAsync) < 0 ||
        rpc_write(conn, &p, sizeof(const struct cudaMemcpy3DPeerParms*)) < 0 ||
        rpc_write(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)p, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaMemGetInfo(size_t* free, size_t* total)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)free, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)total, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaMemGetInfo) < 0 ||
        rpc_write(conn, free, sizeof(size_t)) < 0 ||
        rpc_write(conn, total, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, free, sizeof(size_t)) < 0 ||
        rpc_read(conn, total, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)free, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)total, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaArrayGetInfo(struct cudaChannelFormatDesc* desc, struct cudaExtent* extent, unsigned int* flags, cudaArray_t array)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)desc, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)extent, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)flags, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&array, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaArrayGetInfo) < 0 ||
        rpc_write(conn, desc, sizeof(struct cudaChannelFormatDesc)) < 0 ||
        rpc_write(conn, extent, sizeof(struct cudaExtent)) < 0 ||
        rpc_write(conn, flags, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &array, sizeof(cudaArray_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, desc, sizeof(struct cudaChannelFormatDesc)) < 0 ||
        rpc_read(conn, extent, sizeof(struct cudaExtent)) < 0 ||
        rpc_read(conn, flags, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)desc, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)extent, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)flags, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&array, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaArrayGetPlane(cudaArray_t* pPlaneArray, cudaArray_t hArray, unsigned int planeIdx)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pPlaneArray, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hArray, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&planeIdx, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaArrayGetPlane) < 0 ||
        rpc_write(conn, pPlaneArray, sizeof(cudaArray_t)) < 0 ||
        rpc_write(conn, &hArray, sizeof(cudaArray_t)) < 0 ||
        rpc_write(conn, &planeIdx, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pPlaneArray, sizeof(cudaArray_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pPlaneArray, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hArray, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&planeIdx, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaArrayGetMemoryRequirements(struct cudaArrayMemoryRequirements* memoryRequirements, cudaArray_t array, int device)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)memoryRequirements, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&array, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaArrayGetMemoryRequirements) < 0 ||
        rpc_write(conn, memoryRequirements, sizeof(struct cudaArrayMemoryRequirements)) < 0 ||
        rpc_write(conn, &array, sizeof(cudaArray_t)) < 0 ||
        rpc_write(conn, &device, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, memoryRequirements, sizeof(struct cudaArrayMemoryRequirements)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)memoryRequirements, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&array, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaMipmappedArrayGetMemoryRequirements(struct cudaArrayMemoryRequirements* memoryRequirements, cudaMipmappedArray_t mipmap, int device)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)memoryRequirements, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&mipmap, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaMipmappedArrayGetMemoryRequirements) < 0 ||
        rpc_write(conn, memoryRequirements, sizeof(struct cudaArrayMemoryRequirements)) < 0 ||
        rpc_write(conn, &mipmap, sizeof(cudaMipmappedArray_t)) < 0 ||
        rpc_write(conn, &device, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, memoryRequirements, sizeof(struct cudaArrayMemoryRequirements)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)memoryRequirements, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&mipmap, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaArrayGetSparseProperties(struct cudaArraySparseProperties* sparseProperties, cudaArray_t array)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)sparseProperties, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&array, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaArrayGetSparseProperties) < 0 ||
        rpc_write(conn, sparseProperties, sizeof(struct cudaArraySparseProperties)) < 0 ||
        rpc_write(conn, &array, sizeof(cudaArray_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, sparseProperties, sizeof(struct cudaArraySparseProperties)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)sparseProperties, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&array, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaMipmappedArrayGetSparseProperties(struct cudaArraySparseProperties* sparseProperties, cudaMipmappedArray_t mipmap)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)sparseProperties, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&mipmap, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaMipmappedArrayGetSparseProperties) < 0 ||
        rpc_write(conn, sparseProperties, sizeof(struct cudaArraySparseProperties)) < 0 ||
        rpc_write(conn, &mipmap, sizeof(cudaMipmappedArray_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, sparseProperties, sizeof(struct cudaArraySparseProperties)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)sparseProperties, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&mipmap, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaMemcpy2DToArray(cudaArray_t dst, size_t wOffset, size_t hOffset, const void* src, size_t spitch, size_t width, size_t height, enum cudaMemcpyKind kind)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&dst, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&wOffset, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hOffset, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)src, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&spitch, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&width, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&height, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&kind, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaMemcpy2DToArray) < 0 ||
        rpc_write(conn, &dst, sizeof(cudaArray_t)) < 0 ||
        rpc_write(conn, &wOffset, sizeof(size_t)) < 0 ||
        rpc_write(conn, &hOffset, sizeof(size_t)) < 0 ||
        rpc_write(conn, &src, sizeof(const void*)) < 0 ||
        rpc_write(conn, &spitch, sizeof(size_t)) < 0 ||
        rpc_write(conn, &width, sizeof(size_t)) < 0 ||
        rpc_write(conn, &height, sizeof(size_t)) < 0 ||
        rpc_write(conn, &kind, sizeof(enum cudaMemcpyKind)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&dst, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&wOffset, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hOffset, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)src, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&spitch, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&width, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&height, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&kind, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaMemcpy2DArrayToArray(cudaArray_t dst, size_t wOffsetDst, size_t hOffsetDst, cudaArray_const_t src, size_t wOffsetSrc, size_t hOffsetSrc, size_t width, size_t height, enum cudaMemcpyKind kind)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&dst, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&wOffsetDst, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hOffsetDst, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&src, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&wOffsetSrc, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hOffsetSrc, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&width, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&height, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&kind, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaMemcpy2DArrayToArray) < 0 ||
        rpc_write(conn, &dst, sizeof(cudaArray_t)) < 0 ||
        rpc_write(conn, &wOffsetDst, sizeof(size_t)) < 0 ||
        rpc_write(conn, &hOffsetDst, sizeof(size_t)) < 0 ||
        rpc_write(conn, &src, sizeof(cudaArray_const_t)) < 0 ||
        rpc_write(conn, &wOffsetSrc, sizeof(size_t)) < 0 ||
        rpc_write(conn, &hOffsetSrc, sizeof(size_t)) < 0 ||
        rpc_write(conn, &width, sizeof(size_t)) < 0 ||
        rpc_write(conn, &height, sizeof(size_t)) < 0 ||
        rpc_write(conn, &kind, sizeof(enum cudaMemcpyKind)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&dst, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&wOffsetDst, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hOffsetDst, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&src, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&wOffsetSrc, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hOffsetSrc, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&width, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&height, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&kind, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaMemcpyToSymbol(const void* symbol, const void* src, size_t count, size_t offset, enum cudaMemcpyKind kind)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)symbol, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)src, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&offset, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&kind, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaMemcpyToSymbol) < 0 ||
        rpc_write(conn, &symbol, sizeof(const void*)) < 0 ||
        rpc_write(conn, &src, sizeof(const void*)) < 0 ||
        rpc_write(conn, &count, sizeof(size_t)) < 0 ||
        rpc_write(conn, &offset, sizeof(size_t)) < 0 ||
        rpc_write(conn, &kind, sizeof(enum cudaMemcpyKind)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)symbol, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)src, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&offset, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&kind, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaMemcpy2DToArrayAsync(cudaArray_t dst, size_t wOffset, size_t hOffset, const void* src, size_t spitch, size_t width, size_t height, enum cudaMemcpyKind kind, cudaStream_t stream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&dst, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&wOffset, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hOffset, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)src, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&spitch, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&width, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&height, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&kind, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaMemcpy2DToArrayAsync) < 0 ||
        rpc_write(conn, &dst, sizeof(cudaArray_t)) < 0 ||
        rpc_write(conn, &wOffset, sizeof(size_t)) < 0 ||
        rpc_write(conn, &hOffset, sizeof(size_t)) < 0 ||
        rpc_write(conn, &src, sizeof(const void*)) < 0 ||
        rpc_write(conn, &spitch, sizeof(size_t)) < 0 ||
        rpc_write(conn, &width, sizeof(size_t)) < 0 ||
        rpc_write(conn, &height, sizeof(size_t)) < 0 ||
        rpc_write(conn, &kind, sizeof(enum cudaMemcpyKind)) < 0 ||
        rpc_write(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&dst, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&wOffset, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hOffset, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)src, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&spitch, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&width, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&height, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&kind, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaMemcpyToSymbolAsync(const void* symbol, const void* src, size_t count, size_t offset, enum cudaMemcpyKind kind, cudaStream_t stream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)symbol, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)src, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&offset, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&kind, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaMemcpyToSymbolAsync) < 0 ||
        rpc_write(conn, &symbol, sizeof(const void*)) < 0 ||
        rpc_write(conn, &src, sizeof(const void*)) < 0 ||
        rpc_write(conn, &count, sizeof(size_t)) < 0 ||
        rpc_write(conn, &offset, sizeof(size_t)) < 0 ||
        rpc_write(conn, &kind, sizeof(enum cudaMemcpyKind)) < 0 ||
        rpc_write(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)symbol, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)src, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&offset, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&kind, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaMemset(void* devPtr, int value, size_t count)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)devPtr, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&value, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaMemset) < 0 ||
        rpc_write(conn, &devPtr, sizeof(void*)) < 0 ||
        rpc_write(conn, &value, sizeof(int)) < 0 ||
        rpc_write(conn, &count, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)devPtr, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&value, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaMemset2D(void* devPtr, size_t pitch, int value, size_t width, size_t height)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)devPtr, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&pitch, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&value, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&width, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&height, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaMemset2D) < 0 ||
        rpc_write(conn, &devPtr, sizeof(void*)) < 0 ||
        rpc_write(conn, &pitch, sizeof(size_t)) < 0 ||
        rpc_write(conn, &value, sizeof(int)) < 0 ||
        rpc_write(conn, &width, sizeof(size_t)) < 0 ||
        rpc_write(conn, &height, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)devPtr, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&pitch, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&value, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&width, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&height, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaMemset3D(struct cudaPitchedPtr pitchedDevPtr, int value, struct cudaExtent extent)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&pitchedDevPtr, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&value, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&extent, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaMemset3D) < 0 ||
        rpc_write(conn, &pitchedDevPtr, sizeof(struct cudaPitchedPtr)) < 0 ||
        rpc_write(conn, &value, sizeof(int)) < 0 ||
        rpc_write(conn, &extent, sizeof(struct cudaExtent)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&pitchedDevPtr, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&value, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&extent, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaMemsetAsync(void* devPtr, int value, size_t count, cudaStream_t stream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)devPtr, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&value, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaMemsetAsync) < 0 ||
        rpc_write(conn, &devPtr, sizeof(void*)) < 0 ||
        rpc_write(conn, &value, sizeof(int)) < 0 ||
        rpc_write(conn, &count, sizeof(size_t)) < 0 ||
        rpc_write(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)devPtr, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&value, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaMemset2DAsync(void* devPtr, size_t pitch, int value, size_t width, size_t height, cudaStream_t stream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)devPtr, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&pitch, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&value, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&width, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&height, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaMemset2DAsync) < 0 ||
        rpc_write(conn, &devPtr, sizeof(void*)) < 0 ||
        rpc_write(conn, &pitch, sizeof(size_t)) < 0 ||
        rpc_write(conn, &value, sizeof(int)) < 0 ||
        rpc_write(conn, &width, sizeof(size_t)) < 0 ||
        rpc_write(conn, &height, sizeof(size_t)) < 0 ||
        rpc_write(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)devPtr, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&pitch, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&value, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&width, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&height, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaMemset3DAsync(struct cudaPitchedPtr pitchedDevPtr, int value, struct cudaExtent extent, cudaStream_t stream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&pitchedDevPtr, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&value, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&extent, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaMemset3DAsync) < 0 ||
        rpc_write(conn, &pitchedDevPtr, sizeof(struct cudaPitchedPtr)) < 0 ||
        rpc_write(conn, &value, sizeof(int)) < 0 ||
        rpc_write(conn, &extent, sizeof(struct cudaExtent)) < 0 ||
        rpc_write(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&pitchedDevPtr, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&value, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&extent, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGetSymbolAddress(void** devPtr, const void* symbol)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)devPtr, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)symbol, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGetSymbolAddress) < 0 ||
        rpc_write(conn, devPtr, sizeof(void*)) < 0 ||
        rpc_write(conn, &symbol, sizeof(const void*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, devPtr, sizeof(void*)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)devPtr, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)symbol, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGetSymbolSize(size_t* size, const void* symbol)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)size, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)symbol, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGetSymbolSize) < 0 ||
        rpc_write(conn, size, sizeof(size_t)) < 0 ||
        rpc_write(conn, &symbol, sizeof(const void*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, size, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)size, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)symbol, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaMemPrefetchAsync(const void* devPtr, size_t count, int dstDevice, cudaStream_t stream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)devPtr, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaMemPrefetchAsync) < 0 ||
        rpc_write(conn, &devPtr, sizeof(const void*)) < 0 ||
        rpc_write(conn, &count, sizeof(size_t)) < 0 ||
        rpc_write(conn, &dstDevice, sizeof(int)) < 0 ||
        rpc_write(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)devPtr, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&dstDevice, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaMemPrefetchAsync_v2(const void* devPtr, size_t count, struct cudaMemLocation location, unsigned int flags, cudaStream_t stream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)devPtr, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&location, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaMemPrefetchAsync_v2) < 0 ||
        rpc_write(conn, &devPtr, sizeof(const void*)) < 0 ||
        rpc_write(conn, &count, sizeof(size_t)) < 0 ||
        rpc_write(conn, &location, sizeof(struct cudaMemLocation)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)devPtr, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&location, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaMemAdvise(const void* devPtr, size_t count, enum cudaMemoryAdvise advice, int device)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)devPtr, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&advice, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaMemAdvise) < 0 ||
        rpc_write(conn, &devPtr, sizeof(const void*)) < 0 ||
        rpc_write(conn, &count, sizeof(size_t)) < 0 ||
        rpc_write(conn, &advice, sizeof(enum cudaMemoryAdvise)) < 0 ||
        rpc_write(conn, &device, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)devPtr, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&advice, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaMemAdvise_v2(const void* devPtr, size_t count, enum cudaMemoryAdvise advice, struct cudaMemLocation location)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)devPtr, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&advice, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&location, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaMemAdvise_v2) < 0 ||
        rpc_write(conn, &devPtr, sizeof(const void*)) < 0 ||
        rpc_write(conn, &count, sizeof(size_t)) < 0 ||
        rpc_write(conn, &advice, sizeof(enum cudaMemoryAdvise)) < 0 ||
        rpc_write(conn, &location, sizeof(struct cudaMemLocation)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)devPtr, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&advice, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&location, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaMemRangeGetAttributes(void** data, size_t* dataSizes, enum cudaMemRangeAttribute* attributes, size_t numAttributes, const void* devPtr, size_t count)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)data, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)dataSizes, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)attributes, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&numAttributes, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)devPtr, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaMemRangeGetAttributes) < 0 ||
        rpc_write(conn, data, sizeof(void*)) < 0 ||
        rpc_write(conn, dataSizes, sizeof(size_t)) < 0 ||
        rpc_write(conn, attributes, sizeof(enum cudaMemRangeAttribute)) < 0 ||
        rpc_write(conn, &numAttributes, sizeof(size_t)) < 0 ||
        rpc_write(conn, &devPtr, sizeof(const void*)) < 0 ||
        rpc_write(conn, &count, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, data, sizeof(void*)) < 0 ||
        rpc_read(conn, dataSizes, sizeof(size_t)) < 0 ||
        rpc_read(conn, attributes, sizeof(enum cudaMemRangeAttribute)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)data, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)dataSizes, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)attributes, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&numAttributes, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)devPtr, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaMemcpyToArray(cudaArray_t dst, size_t wOffset, size_t hOffset, const void* src, size_t count, enum cudaMemcpyKind kind)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&dst, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&wOffset, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hOffset, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)src, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&kind, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaMemcpyToArray) < 0 ||
        rpc_write(conn, &dst, sizeof(cudaArray_t)) < 0 ||
        rpc_write(conn, &wOffset, sizeof(size_t)) < 0 ||
        rpc_write(conn, &hOffset, sizeof(size_t)) < 0 ||
        rpc_write(conn, &src, sizeof(const void*)) < 0 ||
        rpc_write(conn, &count, sizeof(size_t)) < 0 ||
        rpc_write(conn, &kind, sizeof(enum cudaMemcpyKind)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&dst, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&wOffset, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hOffset, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)src, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&kind, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaMemcpyArrayToArray(cudaArray_t dst, size_t wOffsetDst, size_t hOffsetDst, cudaArray_const_t src, size_t wOffsetSrc, size_t hOffsetSrc, size_t count, enum cudaMemcpyKind kind)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&dst, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&wOffsetDst, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hOffsetDst, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&src, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&wOffsetSrc, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hOffsetSrc, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&kind, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaMemcpyArrayToArray) < 0 ||
        rpc_write(conn, &dst, sizeof(cudaArray_t)) < 0 ||
        rpc_write(conn, &wOffsetDst, sizeof(size_t)) < 0 ||
        rpc_write(conn, &hOffsetDst, sizeof(size_t)) < 0 ||
        rpc_write(conn, &src, sizeof(cudaArray_const_t)) < 0 ||
        rpc_write(conn, &wOffsetSrc, sizeof(size_t)) < 0 ||
        rpc_write(conn, &hOffsetSrc, sizeof(size_t)) < 0 ||
        rpc_write(conn, &count, sizeof(size_t)) < 0 ||
        rpc_write(conn, &kind, sizeof(enum cudaMemcpyKind)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&dst, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&wOffsetDst, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hOffsetDst, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&src, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&wOffsetSrc, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hOffsetSrc, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&kind, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaMemcpyToArrayAsync(cudaArray_t dst, size_t wOffset, size_t hOffset, const void* src, size_t count, enum cudaMemcpyKind kind, cudaStream_t stream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&dst, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&wOffset, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hOffset, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)src, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&kind, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaMemcpyToArrayAsync) < 0 ||
        rpc_write(conn, &dst, sizeof(cudaArray_t)) < 0 ||
        rpc_write(conn, &wOffset, sizeof(size_t)) < 0 ||
        rpc_write(conn, &hOffset, sizeof(size_t)) < 0 ||
        rpc_write(conn, &src, sizeof(const void*)) < 0 ||
        rpc_write(conn, &count, sizeof(size_t)) < 0 ||
        rpc_write(conn, &kind, sizeof(enum cudaMemcpyKind)) < 0 ||
        rpc_write(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&dst, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&wOffset, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hOffset, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)src, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&kind, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaMallocAsync(void** devPtr, size_t size, cudaStream_t hStream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)devPtr, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&size, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaMallocAsync) < 0 ||
        rpc_write(conn, devPtr, sizeof(void*)) < 0 ||
        rpc_write(conn, &size, sizeof(size_t)) < 0 ||
        rpc_write(conn, &hStream, sizeof(cudaStream_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, devPtr, sizeof(void*)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)devPtr, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&size, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hStream, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaMemPoolTrimTo(cudaMemPool_t memPool, size_t minBytesToKeep)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&memPool, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&minBytesToKeep, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaMemPoolTrimTo) < 0 ||
        rpc_write(conn, &memPool, sizeof(cudaMemPool_t)) < 0 ||
        rpc_write(conn, &minBytesToKeep, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&memPool, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&minBytesToKeep, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaMemPoolSetAccess(cudaMemPool_t memPool, const struct cudaMemAccessDesc* descList, size_t count)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&memPool, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)descList, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaMemPoolSetAccess) < 0 ||
        rpc_write(conn, &memPool, sizeof(cudaMemPool_t)) < 0 ||
        rpc_write(conn, &descList, sizeof(const struct cudaMemAccessDesc*)) < 0 ||
        rpc_write(conn, &count, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&memPool, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)descList, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaMemPoolGetAccess(enum cudaMemAccessFlags* flags, cudaMemPool_t memPool, struct cudaMemLocation* location)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)flags, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&memPool, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)location, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaMemPoolGetAccess) < 0 ||
        rpc_write(conn, flags, sizeof(enum cudaMemAccessFlags)) < 0 ||
        rpc_write(conn, &memPool, sizeof(cudaMemPool_t)) < 0 ||
        rpc_write(conn, location, sizeof(struct cudaMemLocation)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, flags, sizeof(enum cudaMemAccessFlags)) < 0 ||
        rpc_read(conn, location, sizeof(struct cudaMemLocation)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)flags, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&memPool, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)location, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaMemPoolCreate(cudaMemPool_t* memPool, const struct cudaMemPoolProps* poolProps)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)memPool, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)poolProps, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaMemPoolCreate) < 0 ||
        rpc_write(conn, memPool, sizeof(cudaMemPool_t)) < 0 ||
        rpc_write(conn, &poolProps, sizeof(const struct cudaMemPoolProps*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, memPool, sizeof(cudaMemPool_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)memPool, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)poolProps, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaMemPoolDestroy(cudaMemPool_t memPool)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&memPool, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaMemPoolDestroy) < 0 ||
        rpc_write(conn, &memPool, sizeof(cudaMemPool_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&memPool, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaMallocFromPoolAsync(void** ptr, size_t size, cudaMemPool_t memPool, cudaStream_t stream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)ptr, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&size, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&memPool, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaMallocFromPoolAsync) < 0 ||
        rpc_write(conn, ptr, sizeof(void*)) < 0 ||
        rpc_write(conn, &size, sizeof(size_t)) < 0 ||
        rpc_write(conn, &memPool, sizeof(cudaMemPool_t)) < 0 ||
        rpc_write(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, ptr, sizeof(void*)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)ptr, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&size, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&memPool, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaMemPoolImportPointer(void** ptr, cudaMemPool_t memPool, struct cudaMemPoolPtrExportData* exportData)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)ptr, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&memPool, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)exportData, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaMemPoolImportPointer) < 0 ||
        rpc_write(conn, ptr, sizeof(void*)) < 0 ||
        rpc_write(conn, &memPool, sizeof(cudaMemPool_t)) < 0 ||
        rpc_write(conn, exportData, sizeof(struct cudaMemPoolPtrExportData)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, ptr, sizeof(void*)) < 0 ||
        rpc_read(conn, exportData, sizeof(struct cudaMemPoolPtrExportData)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)ptr, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&memPool, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)exportData, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaPointerGetAttributes(struct cudaPointerAttributes* attributes, const void* ptr)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)attributes, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)ptr, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaPointerGetAttributes) < 0 ||
        rpc_write(conn, attributes, sizeof(struct cudaPointerAttributes)) < 0 ||
        rpc_write(conn, &ptr, sizeof(const void*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, attributes, sizeof(struct cudaPointerAttributes)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)attributes, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)ptr, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaDeviceCanAccessPeer(int* canAccessPeer, int device, int peerDevice)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)canAccessPeer, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&peerDevice, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaDeviceCanAccessPeer) < 0 ||
        rpc_write(conn, canAccessPeer, sizeof(int)) < 0 ||
        rpc_write(conn, &device, sizeof(int)) < 0 ||
        rpc_write(conn, &peerDevice, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, canAccessPeer, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)canAccessPeer, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&peerDevice, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaDeviceEnablePeerAccess(int peerDevice, unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&peerDevice, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaDeviceEnablePeerAccess) < 0 ||
        rpc_write(conn, &peerDevice, sizeof(int)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&peerDevice, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaDeviceDisablePeerAccess(int peerDevice)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&peerDevice, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaDeviceDisablePeerAccess) < 0 ||
        rpc_write(conn, &peerDevice, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&peerDevice, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphicsUnregisterResource(cudaGraphicsResource_t resource)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&resource, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphicsUnregisterResource) < 0 ||
        rpc_write(conn, &resource, sizeof(cudaGraphicsResource_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&resource, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphicsResourceSetMapFlags(cudaGraphicsResource_t resource, unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&resource, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphicsResourceSetMapFlags) < 0 ||
        rpc_write(conn, &resource, sizeof(cudaGraphicsResource_t)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&resource, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphicsMapResources(int count, cudaGraphicsResource_t* resources, cudaStream_t stream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)resources, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphicsMapResources) < 0 ||
        rpc_write(conn, &count, sizeof(int)) < 0 ||
        rpc_write(conn, resources, sizeof(cudaGraphicsResource_t)) < 0 ||
        rpc_write(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, resources, sizeof(cudaGraphicsResource_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)resources, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphicsUnmapResources(int count, cudaGraphicsResource_t* resources, cudaStream_t stream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)resources, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphicsUnmapResources) < 0 ||
        rpc_write(conn, &count, sizeof(int)) < 0 ||
        rpc_write(conn, resources, sizeof(cudaGraphicsResource_t)) < 0 ||
        rpc_write(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, resources, sizeof(cudaGraphicsResource_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)resources, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphicsResourceGetMappedPointer(void** devPtr, size_t* size, cudaGraphicsResource_t resource)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)devPtr, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)size, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&resource, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphicsResourceGetMappedPointer) < 0 ||
        rpc_write(conn, devPtr, sizeof(void*)) < 0 ||
        rpc_write(conn, size, sizeof(size_t)) < 0 ||
        rpc_write(conn, &resource, sizeof(cudaGraphicsResource_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, devPtr, sizeof(void*)) < 0 ||
        rpc_read(conn, size, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)devPtr, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)size, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&resource, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphicsSubResourceGetMappedArray(cudaArray_t* array, cudaGraphicsResource_t resource, unsigned int arrayIndex, unsigned int mipLevel)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)array, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&resource, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&arrayIndex, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&mipLevel, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphicsSubResourceGetMappedArray) < 0 ||
        rpc_write(conn, array, sizeof(cudaArray_t)) < 0 ||
        rpc_write(conn, &resource, sizeof(cudaGraphicsResource_t)) < 0 ||
        rpc_write(conn, &arrayIndex, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &mipLevel, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, array, sizeof(cudaArray_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)array, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&resource, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&arrayIndex, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&mipLevel, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphicsResourceGetMappedMipmappedArray(cudaMipmappedArray_t* mipmappedArray, cudaGraphicsResource_t resource)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)mipmappedArray, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&resource, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphicsResourceGetMappedMipmappedArray) < 0 ||
        rpc_write(conn, mipmappedArray, sizeof(cudaMipmappedArray_t)) < 0 ||
        rpc_write(conn, &resource, sizeof(cudaGraphicsResource_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, mipmappedArray, sizeof(cudaMipmappedArray_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)mipmappedArray, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&resource, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGetChannelDesc(struct cudaChannelFormatDesc* desc, cudaArray_const_t array)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)desc, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&array, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGetChannelDesc) < 0 ||
        rpc_write(conn, desc, sizeof(struct cudaChannelFormatDesc)) < 0 ||
        rpc_write(conn, &array, sizeof(cudaArray_const_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, desc, sizeof(struct cudaChannelFormatDesc)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)desc, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&array, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaCreateTextureObject(cudaTextureObject_t* pTexObject, const struct cudaResourceDesc* pResDesc, const struct cudaTextureDesc* pTexDesc, const struct cudaResourceViewDesc* pResViewDesc)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pTexObject, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pResDesc, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pTexDesc, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pResViewDesc, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaCreateTextureObject) < 0 ||
        rpc_write(conn, pTexObject, sizeof(cudaTextureObject_t)) < 0 ||
        rpc_write(conn, &pResDesc, sizeof(const struct cudaResourceDesc*)) < 0 ||
        rpc_write(conn, &pTexDesc, sizeof(const struct cudaTextureDesc*)) < 0 ||
        rpc_write(conn, &pResViewDesc, sizeof(const struct cudaResourceViewDesc*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pTexObject, sizeof(cudaTextureObject_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pTexObject, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pResDesc, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pTexDesc, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pResViewDesc, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaDestroyTextureObject(cudaTextureObject_t texObject)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&texObject, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaDestroyTextureObject) < 0 ||
        rpc_write(conn, &texObject, sizeof(cudaTextureObject_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&texObject, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGetTextureObjectResourceDesc(struct cudaResourceDesc* pResDesc, cudaTextureObject_t texObject)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pResDesc, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&texObject, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGetTextureObjectResourceDesc) < 0 ||
        rpc_write(conn, pResDesc, sizeof(struct cudaResourceDesc)) < 0 ||
        rpc_write(conn, &texObject, sizeof(cudaTextureObject_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pResDesc, sizeof(struct cudaResourceDesc)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pResDesc, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&texObject, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGetTextureObjectTextureDesc(struct cudaTextureDesc* pTexDesc, cudaTextureObject_t texObject)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pTexDesc, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&texObject, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGetTextureObjectTextureDesc) < 0 ||
        rpc_write(conn, pTexDesc, sizeof(struct cudaTextureDesc)) < 0 ||
        rpc_write(conn, &texObject, sizeof(cudaTextureObject_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pTexDesc, sizeof(struct cudaTextureDesc)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pTexDesc, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&texObject, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGetTextureObjectResourceViewDesc(struct cudaResourceViewDesc* pResViewDesc, cudaTextureObject_t texObject)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pResViewDesc, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&texObject, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGetTextureObjectResourceViewDesc) < 0 ||
        rpc_write(conn, pResViewDesc, sizeof(struct cudaResourceViewDesc)) < 0 ||
        rpc_write(conn, &texObject, sizeof(cudaTextureObject_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pResViewDesc, sizeof(struct cudaResourceViewDesc)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pResViewDesc, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&texObject, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaCreateSurfaceObject(cudaSurfaceObject_t* pSurfObject, const struct cudaResourceDesc* pResDesc)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pSurfObject, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pResDesc, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaCreateSurfaceObject) < 0 ||
        rpc_write(conn, pSurfObject, sizeof(cudaSurfaceObject_t)) < 0 ||
        rpc_write(conn, &pResDesc, sizeof(const struct cudaResourceDesc*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pSurfObject, sizeof(cudaSurfaceObject_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pSurfObject, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pResDesc, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaDestroySurfaceObject(cudaSurfaceObject_t surfObject)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&surfObject, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaDestroySurfaceObject) < 0 ||
        rpc_write(conn, &surfObject, sizeof(cudaSurfaceObject_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&surfObject, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGetSurfaceObjectResourceDesc(struct cudaResourceDesc* pResDesc, cudaSurfaceObject_t surfObject)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pResDesc, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&surfObject, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGetSurfaceObjectResourceDesc) < 0 ||
        rpc_write(conn, pResDesc, sizeof(struct cudaResourceDesc)) < 0 ||
        rpc_write(conn, &surfObject, sizeof(cudaSurfaceObject_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pResDesc, sizeof(struct cudaResourceDesc)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pResDesc, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&surfObject, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaDriverGetVersion(int* driverVersion)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)driverVersion, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaDriverGetVersion) < 0 ||
        rpc_write(conn, driverVersion, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, driverVersion, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)driverVersion, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaRuntimeGetVersion(int* runtimeVersion)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)runtimeVersion, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaRuntimeGetVersion) < 0 ||
        rpc_write(conn, runtimeVersion, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, runtimeVersion, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)runtimeVersion, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphCreate(cudaGraph_t* pGraph, unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pGraph, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphCreate) < 0 ||
        rpc_write(conn, pGraph, sizeof(cudaGraph_t)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pGraph, sizeof(cudaGraph_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pGraph, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphKernelNodeGetParams(cudaGraphNode_t node, struct cudaKernelNodeParams* pNodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pNodeParams, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphKernelNodeGetParams) < 0 ||
        rpc_write(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, pNodeParams, sizeof(struct cudaKernelNodeParams)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pNodeParams, sizeof(struct cudaKernelNodeParams)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pNodeParams, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphKernelNodeSetParams(cudaGraphNode_t node, const struct cudaKernelNodeParams* pNodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pNodeParams, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphKernelNodeSetParams) < 0 ||
        rpc_write(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &pNodeParams, sizeof(const struct cudaKernelNodeParams*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pNodeParams, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphKernelNodeCopyAttributes(cudaGraphNode_t hSrc, cudaGraphNode_t hDst)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hSrc, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hDst, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphKernelNodeCopyAttributes) < 0 ||
        rpc_write(conn, &hSrc, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &hDst, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hSrc, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hDst, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphKernelNodeGetAttribute(cudaGraphNode_t hNode, cudaLaunchAttributeID attr, cudaLaunchAttributeValue* value_out)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&attr, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)value_out, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphKernelNodeGetAttribute) < 0 ||
        rpc_write(conn, &hNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &attr, sizeof(cudaLaunchAttributeID)) < 0 ||
        rpc_write(conn, value_out, sizeof(cudaLaunchAttributeValue)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, value_out, sizeof(cudaLaunchAttributeValue)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&attr, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)value_out, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphKernelNodeSetAttribute(cudaGraphNode_t hNode, cudaLaunchAttributeID attr, const cudaLaunchAttributeValue* value)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&attr, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)value, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphKernelNodeSetAttribute) < 0 ||
        rpc_write(conn, &hNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &attr, sizeof(cudaLaunchAttributeID)) < 0 ||
        rpc_write(conn, &value, sizeof(const cudaLaunchAttributeValue*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&attr, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)value, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphAddMemcpyNodeToSymbol(cudaGraphNode_t* pGraphNode, cudaGraph_t graph, const cudaGraphNode_t* pDependencies, size_t numDependencies, const void* symbol, const void* src, size_t count, size_t offset, enum cudaMemcpyKind kind)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pGraphNode, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pDependencies, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    for (int i = 0; i < static_cast<int>(numDependencies) && is_unified_pointer(conn, (void*)pDependencies); i++)
      if (maybe_copy_unified_arg(conn, (void*)&pDependencies[i], cudaMemcpyHostToDevice) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)symbol, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)src, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&offset, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&kind, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphAddMemcpyNodeToSymbol) < 0 ||
        rpc_write(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_write(conn, pGraphNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
[=]() -> bool {
                    for (size_t i = 0; i < numDependencies; ++i) {
                        if (rpc_write(0, &pDependencies[i], sizeof(const cudaGraphNode_t)) < 0) {
                            printf("Failed to write Dependency[%zu]\n", i);
                            return false;
                        }
                    }
                    return true;
                }() == false ||        rpc_write(conn, &symbol, sizeof(const void*)) < 0 ||
        rpc_write(conn, &src, sizeof(const void*)) < 0 ||
        rpc_write(conn, &count, sizeof(size_t)) < 0 ||
        rpc_write(conn, &offset, sizeof(size_t)) < 0 ||
        rpc_write(conn, &kind, sizeof(enum cudaMemcpyKind)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pGraphNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pGraphNode, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pDependencies, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    for (int i = 0; i < static_cast<int>(numDependencies) && is_unified_pointer(conn, (void*)pDependencies); i++)
      if (maybe_copy_unified_arg(conn, (void*)&pDependencies[i], cudaMemcpyDeviceToHost) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)symbol, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)src, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&offset, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&kind, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphMemcpyNodeGetParams(cudaGraphNode_t node, struct cudaMemcpy3DParms* pNodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pNodeParams, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphMemcpyNodeGetParams) < 0 ||
        rpc_write(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, pNodeParams, sizeof(struct cudaMemcpy3DParms)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pNodeParams, sizeof(struct cudaMemcpy3DParms)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pNodeParams, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphMemcpyNodeSetParams(cudaGraphNode_t node, const struct cudaMemcpy3DParms* pNodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pNodeParams, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphMemcpyNodeSetParams) < 0 ||
        rpc_write(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &pNodeParams, sizeof(const struct cudaMemcpy3DParms*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pNodeParams, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphMemcpyNodeSetParamsToSymbol(cudaGraphNode_t node, const void* symbol, const void* src, size_t count, size_t offset, enum cudaMemcpyKind kind)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)symbol, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)src, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&offset, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&kind, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphMemcpyNodeSetParamsToSymbol) < 0 ||
        rpc_write(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &symbol, sizeof(const void*)) < 0 ||
        rpc_write(conn, &src, sizeof(const void*)) < 0 ||
        rpc_write(conn, &count, sizeof(size_t)) < 0 ||
        rpc_write(conn, &offset, sizeof(size_t)) < 0 ||
        rpc_write(conn, &kind, sizeof(enum cudaMemcpyKind)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)symbol, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)src, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&offset, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&kind, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphAddMemsetNode(cudaGraphNode_t* pGraphNode, cudaGraph_t graph, const cudaGraphNode_t* pDependencies, size_t numDependencies, const struct cudaMemsetParams* pMemsetParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pGraphNode, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pDependencies, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    for (int i = 0; i < static_cast<int>(numDependencies) && is_unified_pointer(conn, (void*)pDependencies); i++)
      if (maybe_copy_unified_arg(conn, (void*)&pDependencies[i], cudaMemcpyHostToDevice) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pMemsetParams, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphAddMemsetNode) < 0 ||
        rpc_write(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_write(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
[=]() -> bool {
                    for (size_t i = 0; i < numDependencies; ++i) {
                        if (rpc_write(0, &pDependencies[i], sizeof(const cudaGraphNode_t)) < 0) {
                            printf("Failed to write Dependency[%zu]\n", i);
                            return false;
                        }
                    }
                    return true;
                }() == false ||        rpc_write(conn, &pMemsetParams, sizeof(const struct cudaMemsetParams*)) < 0 ||
        (pMemsetParams != nullptr && rpc_write(conn, pMemsetParams, sizeof(const struct cudaMemsetParams)) < 0) ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pGraphNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pGraphNode, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pDependencies, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    for (int i = 0; i < static_cast<int>(numDependencies) && is_unified_pointer(conn, (void*)pDependencies); i++)
      if (maybe_copy_unified_arg(conn, (void*)&pDependencies[i], cudaMemcpyDeviceToHost) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pMemsetParams, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphMemsetNodeGetParams(cudaGraphNode_t node, struct cudaMemsetParams* pNodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pNodeParams, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphMemsetNodeGetParams) < 0 ||
        rpc_write(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, pNodeParams, sizeof(struct cudaMemsetParams)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pNodeParams, sizeof(struct cudaMemsetParams)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pNodeParams, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphMemsetNodeSetParams(cudaGraphNode_t node, const struct cudaMemsetParams* pNodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pNodeParams, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphMemsetNodeSetParams) < 0 ||
        rpc_write(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &pNodeParams, sizeof(const struct cudaMemsetParams*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pNodeParams, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphHostNodeGetParams(cudaGraphNode_t node, struct cudaHostNodeParams* pNodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pNodeParams, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphHostNodeGetParams) < 0 ||
        rpc_write(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, pNodeParams, sizeof(struct cudaHostNodeParams)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pNodeParams, sizeof(struct cudaHostNodeParams)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pNodeParams, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphHostNodeSetParams(cudaGraphNode_t node, const struct cudaHostNodeParams* pNodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pNodeParams, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphHostNodeSetParams) < 0 ||
        rpc_write(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &pNodeParams, sizeof(const struct cudaHostNodeParams*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pNodeParams, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphAddChildGraphNode(cudaGraphNode_t* pGraphNode, cudaGraph_t graph, const cudaGraphNode_t* pDependencies, size_t numDependencies, cudaGraph_t childGraph)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pGraphNode, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pDependencies, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    for (int i = 0; i < static_cast<int>(numDependencies) && is_unified_pointer(conn, (void*)pDependencies); i++)
      if (maybe_copy_unified_arg(conn, (void*)&pDependencies[i], cudaMemcpyHostToDevice) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&childGraph, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphAddChildGraphNode) < 0 ||
        rpc_write(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_write(conn, pGraphNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
[=]() -> bool {
                    for (size_t i = 0; i < numDependencies; ++i) {
                        if (rpc_write(0, &pDependencies[i], sizeof(const cudaGraphNode_t)) < 0) {
                            printf("Failed to write Dependency[%zu]\n", i);
                            return false;
                        }
                    }
                    return true;
                }() == false ||        rpc_write(conn, &childGraph, sizeof(cudaGraph_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pGraphNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pGraphNode, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pDependencies, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    for (int i = 0; i < static_cast<int>(numDependencies) && is_unified_pointer(conn, (void*)pDependencies); i++)
      if (maybe_copy_unified_arg(conn, (void*)&pDependencies[i], cudaMemcpyDeviceToHost) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&childGraph, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphChildGraphNodeGetGraph(cudaGraphNode_t node, cudaGraph_t* pGraph)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pGraph, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphChildGraphNodeGetGraph) < 0 ||
        rpc_write(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, pGraph, sizeof(cudaGraph_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pGraph, sizeof(cudaGraph_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pGraph, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphAddEmptyNode(cudaGraphNode_t* pGraphNode, cudaGraph_t graph, const cudaGraphNode_t* pDependencies, size_t numDependencies)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pGraphNode, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pDependencies, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    for (int i = 0; i < static_cast<int>(numDependencies) && is_unified_pointer(conn, (void*)pDependencies); i++)
      if (maybe_copy_unified_arg(conn, (void*)&pDependencies[i], cudaMemcpyHostToDevice) < 0)
        return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphAddEmptyNode) < 0 ||
        rpc_write(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_write(conn, pGraphNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
[=]() -> bool {
                    for (size_t i = 0; i < numDependencies; ++i) {
                        if (rpc_write(0, &pDependencies[i], sizeof(const cudaGraphNode_t)) < 0) {
                            printf("Failed to write Dependency[%zu]\n", i);
                            return false;
                        }
                    }
                    return true;
                }() == false ||        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pGraphNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pGraphNode, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pDependencies, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    for (int i = 0; i < static_cast<int>(numDependencies) && is_unified_pointer(conn, (void*)pDependencies); i++)
      if (maybe_copy_unified_arg(conn, (void*)&pDependencies[i], cudaMemcpyDeviceToHost) < 0)
        return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphAddEventRecordNode(cudaGraphNode_t* pGraphNode, cudaGraph_t graph, const cudaGraphNode_t* pDependencies, size_t numDependencies, cudaEvent_t event)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pGraphNode, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pDependencies, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    for (int i = 0; i < static_cast<int>(numDependencies) && is_unified_pointer(conn, (void*)pDependencies); i++)
      if (maybe_copy_unified_arg(conn, (void*)&pDependencies[i], cudaMemcpyHostToDevice) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&event, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphAddEventRecordNode) < 0 ||
        rpc_write(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_write(conn, pGraphNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
[=]() -> bool {
                    for (size_t i = 0; i < numDependencies; ++i) {
                        if (rpc_write(0, &pDependencies[i], sizeof(const cudaGraphNode_t)) < 0) {
                            printf("Failed to write Dependency[%zu]\n", i);
                            return false;
                        }
                    }
                    return true;
                }() == false ||        rpc_write(conn, &event, sizeof(cudaEvent_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pGraphNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pGraphNode, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pDependencies, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    for (int i = 0; i < static_cast<int>(numDependencies) && is_unified_pointer(conn, (void*)pDependencies); i++)
      if (maybe_copy_unified_arg(conn, (void*)&pDependencies[i], cudaMemcpyDeviceToHost) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&event, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphEventRecordNodeGetEvent(cudaGraphNode_t node, cudaEvent_t* event_out)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)event_out, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphEventRecordNodeGetEvent) < 0 ||
        rpc_write(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, event_out, sizeof(cudaEvent_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, event_out, sizeof(cudaEvent_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)event_out, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphEventRecordNodeSetEvent(cudaGraphNode_t node, cudaEvent_t event)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&event, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphEventRecordNodeSetEvent) < 0 ||
        rpc_write(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &event, sizeof(cudaEvent_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&event, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphAddEventWaitNode(cudaGraphNode_t* pGraphNode, cudaGraph_t graph, const cudaGraphNode_t* pDependencies, size_t numDependencies, cudaEvent_t event)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pGraphNode, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pDependencies, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    for (int i = 0; i < static_cast<int>(numDependencies) && is_unified_pointer(conn, (void*)pDependencies); i++)
      if (maybe_copy_unified_arg(conn, (void*)&pDependencies[i], cudaMemcpyHostToDevice) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&event, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphAddEventWaitNode) < 0 ||
        rpc_write(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_write(conn, pGraphNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
[=]() -> bool {
                    for (size_t i = 0; i < numDependencies; ++i) {
                        if (rpc_write(0, &pDependencies[i], sizeof(const cudaGraphNode_t)) < 0) {
                            printf("Failed to write Dependency[%zu]\n", i);
                            return false;
                        }
                    }
                    return true;
                }() == false ||        rpc_write(conn, &event, sizeof(cudaEvent_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pGraphNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pGraphNode, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pDependencies, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    for (int i = 0; i < static_cast<int>(numDependencies) && is_unified_pointer(conn, (void*)pDependencies); i++)
      if (maybe_copy_unified_arg(conn, (void*)&pDependencies[i], cudaMemcpyDeviceToHost) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&event, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphEventWaitNodeGetEvent(cudaGraphNode_t node, cudaEvent_t* event_out)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)event_out, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphEventWaitNodeGetEvent) < 0 ||
        rpc_write(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, event_out, sizeof(cudaEvent_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, event_out, sizeof(cudaEvent_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)event_out, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphEventWaitNodeSetEvent(cudaGraphNode_t node, cudaEvent_t event)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&event, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphEventWaitNodeSetEvent) < 0 ||
        rpc_write(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &event, sizeof(cudaEvent_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&event, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphAddExternalSemaphoresSignalNode(cudaGraphNode_t* pGraphNode, cudaGraph_t graph, const cudaGraphNode_t* pDependencies, size_t numDependencies, const struct cudaExternalSemaphoreSignalNodeParams* nodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pGraphNode, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pDependencies, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    for (int i = 0; i < static_cast<int>(numDependencies) && is_unified_pointer(conn, (void*)pDependencies); i++)
      if (maybe_copy_unified_arg(conn, (void*)&pDependencies[i], cudaMemcpyHostToDevice) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphAddExternalSemaphoresSignalNode) < 0 ||
        rpc_write(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_write(conn, pGraphNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
[=]() -> bool {
                    for (size_t i = 0; i < numDependencies; ++i) {
                        if (rpc_write(0, &pDependencies[i], sizeof(const cudaGraphNode_t)) < 0) {
                            printf("Failed to write Dependency[%zu]\n", i);
                            return false;
                        }
                    }
                    return true;
                }() == false ||        rpc_write(conn, &nodeParams, sizeof(const struct cudaExternalSemaphoreSignalNodeParams*)) < 0 ||
        (nodeParams != nullptr && rpc_write(conn, nodeParams, sizeof(const struct cudaExternalSemaphoreSignalNodeParams)) < 0) ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pGraphNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pGraphNode, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pDependencies, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    for (int i = 0; i < static_cast<int>(numDependencies) && is_unified_pointer(conn, (void*)pDependencies); i++)
      if (maybe_copy_unified_arg(conn, (void*)&pDependencies[i], cudaMemcpyDeviceToHost) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphExternalSemaphoresSignalNodeGetParams(cudaGraphNode_t hNode, struct cudaExternalSemaphoreSignalNodeParams* params_out)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)params_out, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphExternalSemaphoresSignalNodeGetParams) < 0 ||
        rpc_write(conn, &hNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, params_out, sizeof(struct cudaExternalSemaphoreSignalNodeParams)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, params_out, sizeof(struct cudaExternalSemaphoreSignalNodeParams)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)params_out, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphExternalSemaphoresSignalNodeSetParams(cudaGraphNode_t hNode, const struct cudaExternalSemaphoreSignalNodeParams* nodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphExternalSemaphoresSignalNodeSetParams) < 0 ||
        rpc_write(conn, &hNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &nodeParams, sizeof(const struct cudaExternalSemaphoreSignalNodeParams*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphAddExternalSemaphoresWaitNode(cudaGraphNode_t* pGraphNode, cudaGraph_t graph, const cudaGraphNode_t* pDependencies, size_t numDependencies, const struct cudaExternalSemaphoreWaitNodeParams* nodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pGraphNode, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pDependencies, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    for (int i = 0; i < static_cast<int>(numDependencies) && is_unified_pointer(conn, (void*)pDependencies); i++)
      if (maybe_copy_unified_arg(conn, (void*)&pDependencies[i], cudaMemcpyHostToDevice) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphAddExternalSemaphoresWaitNode) < 0 ||
        rpc_write(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_write(conn, pGraphNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
[=]() -> bool {
                    for (size_t i = 0; i < numDependencies; ++i) {
                        if (rpc_write(0, &pDependencies[i], sizeof(const cudaGraphNode_t)) < 0) {
                            printf("Failed to write Dependency[%zu]\n", i);
                            return false;
                        }
                    }
                    return true;
                }() == false ||        rpc_write(conn, &nodeParams, sizeof(const struct cudaExternalSemaphoreWaitNodeParams*)) < 0 ||
        (nodeParams != nullptr && rpc_write(conn, nodeParams, sizeof(const struct cudaExternalSemaphoreWaitNodeParams)) < 0) ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pGraphNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pGraphNode, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pDependencies, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    for (int i = 0; i < static_cast<int>(numDependencies) && is_unified_pointer(conn, (void*)pDependencies); i++)
      if (maybe_copy_unified_arg(conn, (void*)&pDependencies[i], cudaMemcpyDeviceToHost) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphExternalSemaphoresWaitNodeGetParams(cudaGraphNode_t hNode, struct cudaExternalSemaphoreWaitNodeParams* params_out)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)params_out, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphExternalSemaphoresWaitNodeGetParams) < 0 ||
        rpc_write(conn, &hNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, params_out, sizeof(struct cudaExternalSemaphoreWaitNodeParams)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, params_out, sizeof(struct cudaExternalSemaphoreWaitNodeParams)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)params_out, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphExternalSemaphoresWaitNodeSetParams(cudaGraphNode_t hNode, const struct cudaExternalSemaphoreWaitNodeParams* nodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphExternalSemaphoresWaitNodeSetParams) < 0 ||
        rpc_write(conn, &hNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &nodeParams, sizeof(const struct cudaExternalSemaphoreWaitNodeParams*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphMemAllocNodeGetParams(cudaGraphNode_t node, struct cudaMemAllocNodeParams* params_out)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)params_out, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphMemAllocNodeGetParams) < 0 ||
        rpc_write(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, params_out, sizeof(struct cudaMemAllocNodeParams)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, params_out, sizeof(struct cudaMemAllocNodeParams)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)params_out, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaDeviceGraphMemTrim(int device)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaDeviceGraphMemTrim) < 0 ||
        rpc_write(conn, &device, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&device, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphClone(cudaGraph_t* pGraphClone, cudaGraph_t originalGraph)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pGraphClone, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&originalGraph, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphClone) < 0 ||
        rpc_write(conn, pGraphClone, sizeof(cudaGraph_t)) < 0 ||
        rpc_write(conn, &originalGraph, sizeof(cudaGraph_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pGraphClone, sizeof(cudaGraph_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pGraphClone, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&originalGraph, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphNodeFindInClone(cudaGraphNode_t* pNode, cudaGraphNode_t originalNode, cudaGraph_t clonedGraph)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pNode, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&originalNode, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&clonedGraph, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphNodeFindInClone) < 0 ||
        rpc_write(conn, pNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &originalNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &clonedGraph, sizeof(cudaGraph_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pNode, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&originalNode, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&clonedGraph, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphNodeGetType(cudaGraphNode_t node, enum cudaGraphNodeType* pType)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pType, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphNodeGetType) < 0 ||
        rpc_write(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, pType, sizeof(enum cudaGraphNodeType)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pType, sizeof(enum cudaGraphNodeType)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pType, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphGetRootNodes(cudaGraph_t graph, cudaGraphNode_t* pRootNodes, size_t* pNumRootNodes)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pRootNodes, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pNumRootNodes, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphGetRootNodes) < 0 ||
        rpc_write(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
        rpc_write(conn, pRootNodes, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, pNumRootNodes, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pRootNodes, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, pNumRootNodes, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pRootNodes, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pNumRootNodes, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphGetEdges(cudaGraph_t graph, cudaGraphNode_t* from, cudaGraphNode_t* to, size_t* numEdges)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)from, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)to, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)numEdges, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphGetEdges) < 0 ||
        rpc_write(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
        rpc_write(conn, from, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, to, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, numEdges, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, from, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, to, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, numEdges, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)from, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)to, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)numEdges, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphGetEdges_v2(cudaGraph_t graph, cudaGraphNode_t* from, cudaGraphNode_t* to, cudaGraphEdgeData* edgeData, size_t* numEdges)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)from, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)to, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)edgeData, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)numEdges, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphGetEdges_v2) < 0 ||
        rpc_write(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
        rpc_write(conn, from, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, to, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, edgeData, sizeof(cudaGraphEdgeData)) < 0 ||
        rpc_write(conn, numEdges, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, from, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, to, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, edgeData, sizeof(cudaGraphEdgeData)) < 0 ||
        rpc_read(conn, numEdges, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)from, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)to, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)edgeData, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)numEdges, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphNodeGetDependencies(cudaGraphNode_t node, cudaGraphNode_t* pDependencies, size_t* pNumDependencies)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pDependencies, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pNumDependencies, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphNodeGetDependencies) < 0 ||
        rpc_write(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, pDependencies, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, pNumDependencies, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pDependencies, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, pNumDependencies, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pDependencies, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pNumDependencies, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphNodeGetDependencies_v2(cudaGraphNode_t node, cudaGraphNode_t* pDependencies, cudaGraphEdgeData* edgeData, size_t* pNumDependencies)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pDependencies, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)edgeData, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pNumDependencies, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphNodeGetDependencies_v2) < 0 ||
        rpc_write(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, pDependencies, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, edgeData, sizeof(cudaGraphEdgeData)) < 0 ||
        rpc_write(conn, pNumDependencies, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pDependencies, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, edgeData, sizeof(cudaGraphEdgeData)) < 0 ||
        rpc_read(conn, pNumDependencies, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pDependencies, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)edgeData, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pNumDependencies, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphNodeGetDependentNodes(cudaGraphNode_t node, cudaGraphNode_t* pDependentNodes, size_t* pNumDependentNodes)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pDependentNodes, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pNumDependentNodes, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphNodeGetDependentNodes) < 0 ||
        rpc_write(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, pDependentNodes, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, pNumDependentNodes, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pDependentNodes, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, pNumDependentNodes, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pDependentNodes, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pNumDependentNodes, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphNodeGetDependentNodes_v2(cudaGraphNode_t node, cudaGraphNode_t* pDependentNodes, cudaGraphEdgeData* edgeData, size_t* pNumDependentNodes)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pDependentNodes, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)edgeData, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pNumDependentNodes, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphNodeGetDependentNodes_v2) < 0 ||
        rpc_write(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, pDependentNodes, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, edgeData, sizeof(cudaGraphEdgeData)) < 0 ||
        rpc_write(conn, pNumDependentNodes, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pDependentNodes, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_read(conn, edgeData, sizeof(cudaGraphEdgeData)) < 0 ||
        rpc_read(conn, pNumDependentNodes, sizeof(size_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pDependentNodes, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)edgeData, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pNumDependentNodes, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphAddDependencies(cudaGraph_t graph, const cudaGraphNode_t* from, const cudaGraphNode_t* to, size_t numDependencies)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)from, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)to, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphAddDependencies) < 0 ||
        rpc_write(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
        rpc_write(conn, &from, sizeof(const cudaGraphNode_t*)) < 0 ||
        rpc_write(conn, &to, sizeof(const cudaGraphNode_t*)) < 0 ||
        rpc_write(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)from, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)to, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphAddDependencies_v2(cudaGraph_t graph, const cudaGraphNode_t* from, const cudaGraphNode_t* to, const cudaGraphEdgeData* edgeData, size_t numDependencies)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)from, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)to, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)edgeData, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphAddDependencies_v2) < 0 ||
        rpc_write(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
        rpc_write(conn, &from, sizeof(const cudaGraphNode_t*)) < 0 ||
        rpc_write(conn, &to, sizeof(const cudaGraphNode_t*)) < 0 ||
        rpc_write(conn, &edgeData, sizeof(const cudaGraphEdgeData*)) < 0 ||
        rpc_write(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)from, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)to, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)edgeData, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphRemoveDependencies(cudaGraph_t graph, const cudaGraphNode_t* from, const cudaGraphNode_t* to, size_t numDependencies)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)from, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)to, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphRemoveDependencies) < 0 ||
        rpc_write(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
        rpc_write(conn, &from, sizeof(const cudaGraphNode_t*)) < 0 ||
        rpc_write(conn, &to, sizeof(const cudaGraphNode_t*)) < 0 ||
        rpc_write(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)from, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)to, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphRemoveDependencies_v2(cudaGraph_t graph, const cudaGraphNode_t* from, const cudaGraphNode_t* to, const cudaGraphEdgeData* edgeData, size_t numDependencies)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)from, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)to, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)edgeData, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphRemoveDependencies_v2) < 0 ||
        rpc_write(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
        rpc_write(conn, &from, sizeof(const cudaGraphNode_t*)) < 0 ||
        rpc_write(conn, &to, sizeof(const cudaGraphNode_t*)) < 0 ||
        rpc_write(conn, &edgeData, sizeof(const cudaGraphEdgeData*)) < 0 ||
        rpc_write(conn, &numDependencies, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)from, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)to, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)edgeData, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&numDependencies, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphDestroyNode(cudaGraphNode_t node)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphDestroyNode) < 0 ||
        rpc_write(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphInstantiate(cudaGraphExec_t* pGraphExec, cudaGraph_t graph, unsigned long long flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pGraphExec, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphInstantiate) < 0 ||
        rpc_write(conn, pGraphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_write(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned long long)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pGraphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pGraphExec, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphInstantiateWithFlags(cudaGraphExec_t* pGraphExec, cudaGraph_t graph, unsigned long long flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pGraphExec, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphInstantiateWithFlags) < 0 ||
        rpc_write(conn, pGraphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_write(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned long long)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pGraphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pGraphExec, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphInstantiateWithParams(cudaGraphExec_t* pGraphExec, cudaGraph_t graph, cudaGraphInstantiateParams* instantiateParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pGraphExec, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)instantiateParams, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphInstantiateWithParams) < 0 ||
        rpc_write(conn, pGraphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_write(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
        rpc_write(conn, instantiateParams, sizeof(cudaGraphInstantiateParams)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pGraphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_read(conn, instantiateParams, sizeof(cudaGraphInstantiateParams)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pGraphExec, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)instantiateParams, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphExecGetFlags(cudaGraphExec_t graphExec, unsigned long long* flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&graphExec, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)flags, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphExecGetFlags) < 0 ||
        rpc_write(conn, &graphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_write(conn, flags, sizeof(unsigned long long)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, flags, sizeof(unsigned long long)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&graphExec, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)flags, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphExecKernelNodeSetParams(cudaGraphExec_t hGraphExec, cudaGraphNode_t node, const struct cudaKernelNodeParams* pNodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pNodeParams, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphExecKernelNodeSetParams) < 0 ||
        rpc_write(conn, &hGraphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_write(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &pNodeParams, sizeof(const struct cudaKernelNodeParams*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pNodeParams, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphExecMemcpyNodeSetParams(cudaGraphExec_t hGraphExec, cudaGraphNode_t node, const struct cudaMemcpy3DParms* pNodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pNodeParams, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphExecMemcpyNodeSetParams) < 0 ||
        rpc_write(conn, &hGraphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_write(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &pNodeParams, sizeof(const struct cudaMemcpy3DParms*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pNodeParams, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphExecMemcpyNodeSetParamsToSymbol(cudaGraphExec_t hGraphExec, cudaGraphNode_t node, const void* symbol, const void* src, size_t count, size_t offset, enum cudaMemcpyKind kind)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)symbol, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)src, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&offset, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&kind, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphExecMemcpyNodeSetParamsToSymbol) < 0 ||
        rpc_write(conn, &hGraphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_write(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &symbol, sizeof(const void*)) < 0 ||
        rpc_write(conn, &src, sizeof(const void*)) < 0 ||
        rpc_write(conn, &count, sizeof(size_t)) < 0 ||
        rpc_write(conn, &offset, sizeof(size_t)) < 0 ||
        rpc_write(conn, &kind, sizeof(enum cudaMemcpyKind)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)symbol, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)src, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&offset, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&kind, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphExecMemsetNodeSetParams(cudaGraphExec_t hGraphExec, cudaGraphNode_t node, const struct cudaMemsetParams* pNodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pNodeParams, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphExecMemsetNodeSetParams) < 0 ||
        rpc_write(conn, &hGraphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_write(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &pNodeParams, sizeof(const struct cudaMemsetParams*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pNodeParams, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphExecHostNodeSetParams(cudaGraphExec_t hGraphExec, cudaGraphNode_t node, const struct cudaHostNodeParams* pNodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pNodeParams, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphExecHostNodeSetParams) < 0 ||
        rpc_write(conn, &hGraphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_write(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &pNodeParams, sizeof(const struct cudaHostNodeParams*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pNodeParams, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphExecChildGraphNodeSetParams(cudaGraphExec_t hGraphExec, cudaGraphNode_t node, cudaGraph_t childGraph)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&childGraph, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphExecChildGraphNodeSetParams) < 0 ||
        rpc_write(conn, &hGraphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_write(conn, &node, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &childGraph, sizeof(cudaGraph_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&node, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&childGraph, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphExecEventRecordNodeSetEvent(cudaGraphExec_t hGraphExec, cudaGraphNode_t hNode, cudaEvent_t event)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&event, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphExecEventRecordNodeSetEvent) < 0 ||
        rpc_write(conn, &hGraphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_write(conn, &hNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &event, sizeof(cudaEvent_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&event, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphExecEventWaitNodeSetEvent(cudaGraphExec_t hGraphExec, cudaGraphNode_t hNode, cudaEvent_t event)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&event, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphExecEventWaitNodeSetEvent) < 0 ||
        rpc_write(conn, &hGraphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_write(conn, &hNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &event, sizeof(cudaEvent_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&event, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphExecExternalSemaphoresSignalNodeSetParams(cudaGraphExec_t hGraphExec, cudaGraphNode_t hNode, const struct cudaExternalSemaphoreSignalNodeParams* nodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphExecExternalSemaphoresSignalNodeSetParams) < 0 ||
        rpc_write(conn, &hGraphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_write(conn, &hNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &nodeParams, sizeof(const struct cudaExternalSemaphoreSignalNodeParams*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphExecExternalSemaphoresWaitNodeSetParams(cudaGraphExec_t hGraphExec, cudaGraphNode_t hNode, const struct cudaExternalSemaphoreWaitNodeParams* nodeParams)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphExecExternalSemaphoresWaitNodeSetParams) < 0 ||
        rpc_write(conn, &hGraphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_write(conn, &hNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &nodeParams, sizeof(const struct cudaExternalSemaphoreWaitNodeParams*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)nodeParams, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphNodeSetEnabled(cudaGraphExec_t hGraphExec, cudaGraphNode_t hNode, unsigned int isEnabled)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&isEnabled, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphNodeSetEnabled) < 0 ||
        rpc_write(conn, &hGraphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_write(conn, &hNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, &isEnabled, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&isEnabled, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphNodeGetEnabled(cudaGraphExec_t hGraphExec, cudaGraphNode_t hNode, unsigned int* isEnabled)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)isEnabled, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphNodeGetEnabled) < 0 ||
        rpc_write(conn, &hGraphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_write(conn, &hNode, sizeof(cudaGraphNode_t)) < 0 ||
        rpc_write(conn, isEnabled, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, isEnabled, sizeof(unsigned int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hNode, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)isEnabled, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphExecUpdate(cudaGraphExec_t hGraphExec, cudaGraph_t hGraph, cudaGraphExecUpdateResultInfo* resultInfo)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)resultInfo, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphExecUpdate) < 0 ||
        rpc_write(conn, &hGraphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_write(conn, &hGraph, sizeof(cudaGraph_t)) < 0 ||
        rpc_write(conn, resultInfo, sizeof(cudaGraphExecUpdateResultInfo)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, resultInfo, sizeof(cudaGraphExecUpdateResultInfo)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hGraphExec, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&hGraph, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)resultInfo, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphUpload(cudaGraphExec_t graphExec, cudaStream_t stream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&graphExec, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphUpload) < 0 ||
        rpc_write(conn, &graphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_write(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&graphExec, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphLaunch(cudaGraphExec_t graphExec, cudaStream_t stream)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&graphExec, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphLaunch) < 0 ||
        rpc_write(conn, &graphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_write(conn, &stream, sizeof(cudaStream_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&graphExec, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&stream, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphExecDestroy(cudaGraphExec_t graphExec)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&graphExec, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphExecDestroy) < 0 ||
        rpc_write(conn, &graphExec, sizeof(cudaGraphExec_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&graphExec, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphDebugDotPrint(cudaGraph_t graph, const char* path, unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)path, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphDebugDotPrint) < 0 ||
        rpc_write(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
        rpc_write(conn, &path, sizeof(const char*)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)path, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaUserObjectRetain(cudaUserObject_t object, unsigned int count)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&object, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaUserObjectRetain) < 0 ||
        rpc_write(conn, &object, sizeof(cudaUserObject_t)) < 0 ||
        rpc_write(conn, &count, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&object, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaUserObjectRelease(cudaUserObject_t object, unsigned int count)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&object, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaUserObjectRelease) < 0 ||
        rpc_write(conn, &object, sizeof(cudaUserObject_t)) < 0 ||
        rpc_write(conn, &count, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&object, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphRetainUserObject(cudaGraph_t graph, cudaUserObject_t object, unsigned int count, unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&object, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphRetainUserObject) < 0 ||
        rpc_write(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
        rpc_write(conn, &object, sizeof(cudaUserObject_t)) < 0 ||
        rpc_write(conn, &count, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&object, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphReleaseUserObject(cudaGraph_t graph, cudaUserObject_t object, unsigned int count)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&object, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphReleaseUserObject) < 0 ||
        rpc_write(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
        rpc_write(conn, &object, sizeof(cudaUserObject_t)) < 0 ||
        rpc_write(conn, &count, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&object, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&count, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGraphConditionalHandleCreate(cudaGraphConditionalHandle* pHandle_out, cudaGraph_t graph, unsigned int defaultLaunchValue, unsigned int flags)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)pHandle_out, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&defaultLaunchValue, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGraphConditionalHandleCreate) < 0 ||
        rpc_write(conn, pHandle_out, sizeof(cudaGraphConditionalHandle)) < 0 ||
        rpc_write(conn, &graph, sizeof(cudaGraph_t)) < 0 ||
        rpc_write(conn, &defaultLaunchValue, sizeof(unsigned int)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, pHandle_out, sizeof(cudaGraphConditionalHandle)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pHandle_out, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&graph, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&defaultLaunchValue, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGetDriverEntryPoint(const char* symbol, void** funcPtr, unsigned long long flags, enum cudaDriverEntryPointQueryResult* driverStatus)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)symbol, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)funcPtr, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)driverStatus, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGetDriverEntryPoint) < 0 ||
        rpc_write(conn, &symbol, sizeof(const char*)) < 0 ||
        rpc_write(conn, funcPtr, sizeof(void*)) < 0 ||
        rpc_write(conn, &flags, sizeof(unsigned long long)) < 0 ||
        rpc_write(conn, driverStatus, sizeof(enum cudaDriverEntryPointQueryResult)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, funcPtr, sizeof(void*)) < 0 ||
        rpc_read(conn, driverStatus, sizeof(enum cudaDriverEntryPointQueryResult)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)symbol, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)funcPtr, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)&flags, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)driverStatus, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGetExportTable(const void** ppExportTable, const cudaUUID_t* pExportTableId)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)ppExportTable, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pExportTableId, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGetExportTable) < 0 ||
        rpc_write(conn, ppExportTable, sizeof(const void*)) < 0 ||
        rpc_write(conn, &pExportTableId, sizeof(const cudaUUID_t*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, ppExportTable, sizeof(const void*)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)ppExportTable, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)pExportTableId, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGetFuncBySymbol(cudaFunction_t* functionPtr, const void* symbolPtr)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)functionPtr, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)symbolPtr, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGetFuncBySymbol) < 0 ||
        rpc_write(conn, functionPtr, sizeof(cudaFunction_t)) < 0 ||
        rpc_write(conn, &symbolPtr, sizeof(const void*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, functionPtr, sizeof(cudaFunction_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)functionPtr, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)symbolPtr, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cudaError_t cudaGetKernel(cudaKernel_t* kernelPtr, const void* entryFuncAddr)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)kernelPtr, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)entryFuncAddr, cudaMemcpyHostToDevice) < 0)
      return cudaErrorDevicesUnavailable;
    cudaError_t return_value;
    if (rpc_write_start_request(conn, RPC_cudaGetKernel) < 0 ||
        rpc_write(conn, kernelPtr, sizeof(cudaKernel_t)) < 0 ||
        rpc_write(conn, &entryFuncAddr, sizeof(const void*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, kernelPtr, sizeof(cudaKernel_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cudaError_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)kernelPtr, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    if (maybe_copy_unified_arg(conn, (void*)entryFuncAddr, cudaMemcpyDeviceToHost) < 0)
      return cudaErrorDevicesUnavailable;
    return return_value;
}

cublasStatus_t cublasCreate_v2(cublasHandle_t* handle)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)handle, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    cublasStatus_t return_value;
    if (rpc_write_start_request(conn, RPC_cublasCreate_v2) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, handle, sizeof(cublasHandle_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cublasStatus_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)handle, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    return return_value;
}

cublasStatus_t cublasDestroy_v2(cublasHandle_t handle)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&handle, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    cublasStatus_t return_value;
    if (rpc_write_start_request(conn, RPC_cublasDestroy_v2) < 0 ||
        rpc_write(conn, &handle, sizeof(cublasHandle_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cublasStatus_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&handle, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    return return_value;
}

cublasStatus_t cublasGetVersion_v2(cublasHandle_t handle, int* version)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&handle, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)version, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    cublasStatus_t return_value;
    if (rpc_write_start_request(conn, RPC_cublasGetVersion_v2) < 0 ||
        rpc_write(conn, &handle, sizeof(cublasHandle_t)) < 0 ||
        rpc_write(conn, version, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, version, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cublasStatus_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&handle, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)version, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    return return_value;
}

cublasStatus_t cublasGetProperty(libraryPropertyType type, int* value)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&type, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)value, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    cublasStatus_t return_value;
    if (rpc_write_start_request(conn, RPC_cublasGetProperty) < 0 ||
        rpc_write(conn, &type, sizeof(libraryPropertyType)) < 0 ||
        rpc_write(conn, value, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, value, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cublasStatus_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&type, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)value, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    return return_value;
}

cublasStatus_t cublasSetWorkspace_v2(cublasHandle_t handle, void* workspace, size_t workspaceSizeInBytes)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&handle, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)workspace, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&workspaceSizeInBytes, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    cublasStatus_t return_value;
    if (rpc_write_start_request(conn, RPC_cublasSetWorkspace_v2) < 0 ||
        rpc_write(conn, &handle, sizeof(cublasHandle_t)) < 0 ||
        rpc_write(conn, &workspace, sizeof(void*)) < 0 ||
        rpc_write(conn, &workspaceSizeInBytes, sizeof(size_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cublasStatus_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&handle, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)workspace, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&workspaceSizeInBytes, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    return return_value;
}

cublasStatus_t cublasSetStream_v2(cublasHandle_t handle, cudaStream_t streamId)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&handle, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&streamId, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    cublasStatus_t return_value;
    if (rpc_write_start_request(conn, RPC_cublasSetStream_v2) < 0 ||
        rpc_write(conn, &handle, sizeof(cublasHandle_t)) < 0 ||
        rpc_write(conn, &streamId, sizeof(cudaStream_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cublasStatus_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&handle, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&streamId, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    return return_value;
}

cublasStatus_t cublasGetStream_v2(cublasHandle_t handle, cudaStream_t* streamId)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&handle, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)streamId, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    cublasStatus_t return_value;
    if (rpc_write_start_request(conn, RPC_cublasGetStream_v2) < 0 ||
        rpc_write(conn, &handle, sizeof(cublasHandle_t)) < 0 ||
        rpc_write(conn, streamId, sizeof(cudaStream_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, streamId, sizeof(cudaStream_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cublasStatus_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&handle, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)streamId, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    return return_value;
}

cublasStatus_t cublasGetPointerMode_v2(cublasHandle_t handle, cublasPointerMode_t* mode)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&handle, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)mode, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    cublasStatus_t return_value;
    if (rpc_write_start_request(conn, RPC_cublasGetPointerMode_v2) < 0 ||
        rpc_write(conn, &handle, sizeof(cublasHandle_t)) < 0 ||
        rpc_write(conn, mode, sizeof(cublasPointerMode_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, mode, sizeof(cublasPointerMode_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cublasStatus_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&handle, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)mode, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    return return_value;
}

cublasStatus_t cublasSetPointerMode_v2(cublasHandle_t handle, cublasPointerMode_t mode)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&handle, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&mode, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    cublasStatus_t return_value;
    if (rpc_write_start_request(conn, RPC_cublasSetPointerMode_v2) < 0 ||
        rpc_write(conn, &handle, sizeof(cublasHandle_t)) < 0 ||
        rpc_write(conn, &mode, sizeof(cublasPointerMode_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cublasStatus_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&handle, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&mode, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    return return_value;
}

cublasStatus_t cublasGetAtomicsMode(cublasHandle_t handle, cublasAtomicsMode_t* mode)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&handle, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)mode, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    cublasStatus_t return_value;
    if (rpc_write_start_request(conn, RPC_cublasGetAtomicsMode) < 0 ||
        rpc_write(conn, &handle, sizeof(cublasHandle_t)) < 0 ||
        rpc_write(conn, mode, sizeof(cublasAtomicsMode_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, mode, sizeof(cublasAtomicsMode_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cublasStatus_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&handle, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)mode, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    return return_value;
}

cublasStatus_t cublasSetAtomicsMode(cublasHandle_t handle, cublasAtomicsMode_t mode)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&handle, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&mode, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    cublasStatus_t return_value;
    if (rpc_write_start_request(conn, RPC_cublasSetAtomicsMode) < 0 ||
        rpc_write(conn, &handle, sizeof(cublasHandle_t)) < 0 ||
        rpc_write(conn, &mode, sizeof(cublasAtomicsMode_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cublasStatus_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&handle, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&mode, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    return return_value;
}

cublasStatus_t cublasGetMathMode(cublasHandle_t handle, cublasMath_t* mode)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&handle, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)mode, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    cublasStatus_t return_value;
    if (rpc_write_start_request(conn, RPC_cublasGetMathMode) < 0 ||
        rpc_write(conn, &handle, sizeof(cublasHandle_t)) < 0 ||
        rpc_write(conn, mode, sizeof(cublasMath_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, mode, sizeof(cublasMath_t)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cublasStatus_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&handle, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)mode, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    return return_value;
}

cublasStatus_t cublasSetMathMode(cublasHandle_t handle, cublasMath_t mode)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&handle, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&mode, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    cublasStatus_t return_value;
    if (rpc_write_start_request(conn, RPC_cublasSetMathMode) < 0 ||
        rpc_write(conn, &handle, sizeof(cublasHandle_t)) < 0 ||
        rpc_write(conn, &mode, sizeof(cublasMath_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cublasStatus_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&handle, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&mode, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    return return_value;
}

cublasStatus_t cublasGetSmCountTarget(cublasHandle_t handle, int* smCountTarget)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&handle, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)smCountTarget, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    cublasStatus_t return_value;
    if (rpc_write_start_request(conn, RPC_cublasGetSmCountTarget) < 0 ||
        rpc_write(conn, &handle, sizeof(cublasHandle_t)) < 0 ||
        rpc_write(conn, smCountTarget, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, smCountTarget, sizeof(int)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cublasStatus_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&handle, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)smCountTarget, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    return return_value;
}

cublasStatus_t cublasSetSmCountTarget(cublasHandle_t handle, int smCountTarget)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&handle, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&smCountTarget, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    cublasStatus_t return_value;
    if (rpc_write_start_request(conn, RPC_cublasSetSmCountTarget) < 0 ||
        rpc_write(conn, &handle, sizeof(cublasHandle_t)) < 0 ||
        rpc_write(conn, &smCountTarget, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cublasStatus_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&handle, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&smCountTarget, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    return return_value;
}

cublasStatus_t cublasLoggerConfigure(int logIsOn, int logToStdOut, int logToStdErr, const char* logFileName)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&logIsOn, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&logToStdOut, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&logToStdErr, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)logFileName, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    cublasStatus_t return_value;
    if (rpc_write_start_request(conn, RPC_cublasLoggerConfigure) < 0 ||
        rpc_write(conn, &logIsOn, sizeof(int)) < 0 ||
        rpc_write(conn, &logToStdOut, sizeof(int)) < 0 ||
        rpc_write(conn, &logToStdErr, sizeof(int)) < 0 ||
        rpc_write(conn, &logFileName, sizeof(const char*)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cublasStatus_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&logIsOn, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&logToStdOut, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&logToStdErr, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)logFileName, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    return return_value;
}

cublasStatus_t cublasSetLoggerCallback(cublasLogCallback userCallback)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&userCallback, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    cublasStatus_t return_value;
    if (rpc_write_start_request(conn, RPC_cublasSetLoggerCallback) < 0 ||
        rpc_write(conn, &userCallback, sizeof(cublasLogCallback)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cublasStatus_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&userCallback, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    return return_value;
}

cublasStatus_t cublasGetLoggerCallback(cublasLogCallback* userCallback)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)userCallback, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    cublasStatus_t return_value;
    if (rpc_write_start_request(conn, RPC_cublasGetLoggerCallback) < 0 ||
        rpc_write(conn, userCallback, sizeof(cublasLogCallback)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, userCallback, sizeof(cublasLogCallback)) < 0 ||
        rpc_read(conn, &return_value, sizeof(cublasStatus_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)userCallback, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    return return_value;
}

cublasStatus_t cublasSetVector(int n, int elemSize, const void* x, int incx, void* devicePtr, int incy)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&n, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&elemSize, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)x, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&incx, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)devicePtr, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&incy, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    cublasStatus_t return_value;
    if (rpc_write_start_request(conn, RPC_cublasSetVector) < 0 ||
        rpc_write(conn, &n, sizeof(int)) < 0 ||
        rpc_write(conn, &elemSize, sizeof(int)) < 0 ||
        rpc_write(conn, &x, sizeof(const void*)) < 0 ||
        rpc_write(conn, &incx, sizeof(int)) < 0 ||
        rpc_write(conn, &devicePtr, sizeof(void*)) < 0 ||
        rpc_write(conn, &incy, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cublasStatus_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&n, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&elemSize, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)x, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&incx, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)devicePtr, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&incy, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    return return_value;
}

cublasStatus_t cublasSetVector_64(int64_t n, int64_t elemSize, const void* x, int64_t incx, void* devicePtr, int64_t incy)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&n, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&elemSize, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)x, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&incx, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)devicePtr, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&incy, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    cublasStatus_t return_value;
    if (rpc_write_start_request(conn, RPC_cublasSetVector_64) < 0 ||
        rpc_write(conn, &n, sizeof(int64_t)) < 0 ||
        rpc_write(conn, &elemSize, sizeof(int64_t)) < 0 ||
        rpc_write(conn, &x, sizeof(const void*)) < 0 ||
        rpc_write(conn, &incx, sizeof(int64_t)) < 0 ||
        rpc_write(conn, &devicePtr, sizeof(void*)) < 0 ||
        rpc_write(conn, &incy, sizeof(int64_t)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
        rpc_read(conn, &return_value, sizeof(cublasStatus_t)) < 0 ||
        rpc_read_end(conn) < 0)
        return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&n, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&elemSize, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)x, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&incx, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)devicePtr, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&incy, cudaMemcpyDeviceToHost) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    return return_value;
}

cublasStatus_t cublasGetVector(int n, int elemSize, const void* x, int incx, void* y, int incy)
{
    conn_t *conn = rpc_client_get_connection(0);
    if (maybe_copy_unified_arg(conn, (void*)&n, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&elemSize, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)x, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&incx, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)y, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    for (int i = 0; i <  * n1 * elemSize && is_unified_pointer(conn, (void*)y); i++)
      if (maybe_copy_unified_arg(conn, (void*)&y[i], cudaMemcpyHostToDevice) < 0 )
        return CUBLAS_STATUS_NOT_INITIALIZED;
    if (maybe_copy_unified_arg(conn, (void*)&incy, cudaMemcpyHostToDevice) < 0)
      return CUBLAS_STATUS_NOT_INITIALIZED;
    cublasStatus_t return_value;
    if (rpc_write_start_request(conn, RPC_cublasGetVector) < 0 ||
        rpc_write(conn, &n, sizeof(int)) < 0 ||
        rpc_write(conn, &elemSize, sizeof(int)) < 0 ||
        rpc_write(conn, &x, sizeof(const void*)) < 0 ||
        rpc_write(conn, &incx, sizeof(int)) < 0 ||
        rpc_write(conn, &incy, sizeof(int)) < 0 ||
        rpc_wait_for_response(conn) < 0 ||
